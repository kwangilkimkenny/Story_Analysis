{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kimkwangil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/kimkwangil/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kimkwangil/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#conflict\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "from mpld3 import plugins, fig_to_html, save_html, fig_to_dict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "#character, setting\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import multiprocessing\n",
    "import os\n",
    "from pathlib import Path\n",
    "import io\n",
    "from gensim.models import Phrases\n",
    "from textblob import TextBlob\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Tom was in love with Cyndi. He really loved her dearly. She loved Tom too because he was so nice and kind. They would walk in the peaceful park gazing at the bright pink cherry blossom that colored the crisp spring air. Then they got into a big fight. Tom began to hate Cyndi's guts. Cyndi cheated on Tom and she began to ignore Tom. He eventually became furious. Back at their house, Tom began to slap Cyndi and she was crying miserably. However, the sun shined upon them when they won the lotto. Both of them were ecstatic and overjoyed with happiness. They where so happy that they jumped around in celebration. Nonetheless, the happiness never lasted too long. Cyndi began to cheat on Tom again and they went bankrupt very quickly. Fights and quarrels continued in the old torn smelly apartment. It really sucked for both of them. Extreme sadness came over them. But then Cyndi bought another lotto and she won again. This time, it was even more money and they were extremely excited. To celebrate the second winning, then went to a casino and began gambling. They were overly excited because they won money at the fancy casino too. Nevertheless, Tom had too much drink and died. Cyndi was so sad. Didn't know what to do and weeped in despair.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ai_plot_conf(essay_input_):\n",
    "    #1.input essay\n",
    "    input_text = essay_input_\n",
    "\n",
    "    #########################################################################\n",
    "\n",
    "    #2.유사단어를 추출하여 리스트로 반환\n",
    "    def conflict_sim_words(text):\n",
    "\n",
    "        essay_input_corpus = str(text) #문장입력\n",
    "        essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "        sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "        total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "        total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "        \n",
    "        split_sentences = []\n",
    "        for sentence in sentences:\n",
    "            processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "            words = processed.split()\n",
    "            split_sentences.append(words)\n",
    "\n",
    "        skip_gram = 1\n",
    "        workers = multiprocessing.cpu_count()\n",
    "        bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "        model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "        model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "        \n",
    "        #모델 설계 완료\n",
    "\n",
    "        #표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "        confict_words_list = ['clash', 'incompatible', 'inconsistent', 'incongruous', 'opposition', 'variance','vary', 'odds', \n",
    "                                'differ', 'diverge', 'disagree', 'contrast', 'collide', 'contradictory', 'incompatible', 'conflict',\n",
    "                                'inconsistent','irreconcilable','incongruous','contrary','opposite','opposing','opposed',\n",
    "                                'antithetical','clashing','discordant','differing','different','divergent','discrepant',\n",
    "                                'varying','disagreeing','contrasting','at odds','in opposition','at variance' ]\n",
    "        \n",
    "        ####문장에 list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "        \n",
    "        #우선 토큰화한다.\n",
    "        retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "        token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "        #print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "        #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "        filtered_chr_text = []\n",
    "        for k in token_input_text:\n",
    "            for j in confict_words_list:\n",
    "                if k == j:\n",
    "                    filtered_chr_text.append(j)\n",
    "        \n",
    "        #print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "        \n",
    "        filtered_chr_text_ = set(filtered_chr_text) #중복제거\n",
    "        filtered_chr_text__ = list(filtered_chr_text_) #다시 리스트로 변환\n",
    "        #print (filtered_chr_text__) # 중복값 제거 확인\n",
    "        \n",
    "#         for i in filtered_chr_text__:\n",
    "#             ext_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "        \n",
    "#         char_total_count = len(filtered_chr_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 표현 수\n",
    "#         char_count_ = len(filtered_chr_text__) #중복제거된  표현 총 수\n",
    "            \n",
    "#         result_char_ratio = round(char_total_count/total_words * 100, 2)\n",
    "\n",
    "#         import pandas as pd\n",
    "\n",
    "#         df_conf_words = pd.DataFrame(ext_sim_words_key, columns=['words','values']) #데이터프레임으로 변환\n",
    "#         df_r = df_conf_words['words'] #words 컬럼 값 추출\n",
    "#         ext_sim_words_key = df_r.values.tolist() # 유사단어 추출\n",
    "        ext_sim_words_key = filtered_chr_text__\n",
    "\n",
    "        #return result_char_ratio, total_sentences, total_words, char_total_count, char_count_, ext_sim_words_key\n",
    "        return ext_sim_words_key\n",
    "\n",
    "\n",
    "\n",
    "    #########################################################################\n",
    "    # 3.유사단어를 문장에서 추출하여 반환한다.\n",
    "    conflict_sim_words_ratio_result = conflict_sim_words(input_text)\n",
    "\n",
    "\n",
    "\n",
    "    #########################################################################\n",
    "    # 4.CONFLICT GRAPH EXPRESSION Analysis  -- 그래프로 그리기\n",
    "    # conflict(input_text):\n",
    "    contents = str(input_text)\n",
    "    token_list_str = text_to_word_sequence(contents) #tokenize\n",
    "\n",
    "    confict_words_list_basic = ['clash', 'incompatible', 'inconsistent', 'incongruous', 'opposition', 'variance','vary', 'odds', \n",
    "                            'differ', 'diverge', 'disagree', 'contrast', 'collide', 'contradictory', 'incompatible', 'conflict',\n",
    "                            'inconsistent','irreconcilable','incongruous','contrary','opposite','opposing','opposed',\n",
    "                            'antithetical','clashing','discordant','differing','different','divergent','discrepant',\n",
    "                            'varying','disagreeing','contrasting','at odds','in opposition','at variance' ]\n",
    "\n",
    "    confict_words_list = confict_words_list_basic + conflict_sim_words_ratio_result #유사단어를 계산결과 반영!\n",
    "\n",
    "    count_conflict_list = []\n",
    "    for i in token_list_str:\n",
    "        for j in confict_words_list:\n",
    "            if i == j:\n",
    "                count_conflict_list.append(j)\n",
    "\n",
    "    len(count_conflict_list) # 한 문장에 들어있는 conflict 단어 수\n",
    "\n",
    "    list_str = contents.split(\".\")  # 문장별로 분리한다. 분리는 .를 기준으로 한다.   \n",
    "\n",
    "    listSentiment = []\n",
    "\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    i=0\n",
    "    for sentence in tqdm(list_str): #한문장식 가져와서 처리한다.\n",
    "        ss = sid.polarity_scores(sentence) #긍정, 부정, 중립, 혼합점수 계산\n",
    "        #print(ss.keys())\n",
    "        #print('{}: neg:{},neu:{},pos:{},compound:{}'.format(i,ss['neg'],ss['neu'],ss['pos'],ss['compound']))\n",
    "        #print('{}: neg:{}'.format(i,ss['neg']))\n",
    "        i +=1\n",
    "        listSentiment.append([ss['neg'],ss['neu'],ss['pos'],ss['compound']])\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    df_sent = pd.DataFrame(listSentiment)\n",
    "    df_sent.columns = ['neg', 'neu', 'pos','compound']\n",
    "    reslult_df = df_sent.columns\n",
    "\n",
    "\n",
    "    df_sent['comp_score'] = df_sent['compound'].apply(lambda c: 'pos' if c >=0  else 'neg')\n",
    "\n",
    "    df_sent['comp_score'].value_counts()\n",
    "\n",
    "    conflict_ratio = df_sent['comp_score'].value_counts(normalize=True) #상대적 비율 계산\n",
    "\n",
    "    # df_sent 의 값은 아래와 같다.\n",
    "\n",
    "    # neg   neu   pos   compound   comp_score\n",
    "    # 0   0.000   0.808   0.192   0.2280   pos\n",
    "    # 1   0.000   1.000   0.000   0.0000   pos\n",
    "    # 2   0.041   0.778   0.181   0.7269   pos\n",
    "    # 3   0.044   0.787   0.169   0.6486   pos\n",
    "    # 4   0.190   0.678   0.132   -0.2144   neg\n",
    "    # 5   0.000   1.000   0.000   0.0000   pos\n",
    "\n",
    "    #comp_score를 1 -1 변환\n",
    "    df_sent.loc[df_sent[\"comp_score\"] == \"pos\",\"comp_score\"] = 1\n",
    "    df_sent.loc[df_sent[\"comp_score\"] == \"neg\",\"comp_score\"] = -1\n",
    "\n",
    "    # df_sent 의 변환된 값은 아래와 같다.\n",
    "    # neg   neu   pos   compound   comp_score\n",
    "    # 0   0.000   0.808   0.192   0.2280   1\n",
    "    # 1   0.000   1.000   0.000   0.0000   1\n",
    "    # 2   0.041   0.778   0.181   0.7269   1\n",
    "    # 3   0.044   0.787   0.169   0.6486   1\n",
    "    # 4   0.190   0.678   0.132   -0.2144   -1\n",
    "    # 5   0.000   1.000   0.000   0.0000   \n",
    "\n",
    "    #########################################################################\n",
    "    # 5. 그래프로 그려보자. 이 코드는 matplotlib 로 그린것임. 종필은 highcharts로 표현할 것\n",
    "    #########################################################################\n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    # from matplotlib import pyplot as plt\n",
    "\n",
    "    # plt.plot(df_sent)\n",
    "    # plt.xlabel('STORY')\n",
    "    # plt.ylabel('CONFLICT')\n",
    "    # plt.title('FLOW ANALYSIS')\n",
    "    # plt.legend(['neg','neu','pos','compound','reslult'])\n",
    "    # plt.show()\n",
    "\n",
    "    #########################################################################\n",
    "    # 6.ACTION VERB로 그래프 그리기\n",
    "\n",
    "\n",
    "    #입력한 글을 모두 단어로 쪼개로 리스트로 만들기 - \n",
    "    essay_input_corpus_ = str(input_text) #문장입력\n",
    "    essay_input_corpus_ = essay_input_corpus_.lower()#소문자 변환\n",
    "\n",
    "    sentences_  = sent_tokenize(essay_input_corpus_) #문장단위로 토큰화(구분)되어 리스에 담김\n",
    "\n",
    "    # 문장을 토크큰화하여 해당 문장에 Action Verbs가 있는지 분석 부분 코드임 ---> 뒤에서 나옴 아래 777로 표시된 코드부분에서 sentences_ 값 재활용\n",
    "\n",
    "    split_sentences_ = []\n",
    "    for sentence in sentences_:\n",
    "        processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "        words = processed.split()\n",
    "        split_sentences_.append(words)\n",
    "        \n",
    "    # 입력한 문장을 모두 리스트로 변환\n",
    "    input_text_list = [y for x in split_sentences_ for y in x] # 이중 리스트 Flatten\n",
    "\n",
    "    #리스로 변환된 값 확인\n",
    "    #input_text_list \n",
    "\n",
    "    #csv 파일에서 Action Verbs 단어 사전 불러오기\n",
    "    import pandas as pd\n",
    "\n",
    "    #Awards 데이터 불러오기\n",
    "    data_action_verbs = pd.read_csv('actionverbs.csv')\n",
    "    data_ac_verbs_list = data_action_verbs.values.tolist()\n",
    "    verbs_list = [y for x in data_ac_verbs_list for y in x]\n",
    "\n",
    "    #########################################################################\n",
    "    # 7.Action Verbs 유사단어를 추출하여 리스트로 반환\n",
    "\n",
    "    def actionverb_sim_words(text):\n",
    "\n",
    "        essay_input_corpus = str(text) #문장입력\n",
    "        essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "        sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "        total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "        total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "        \n",
    "        split_sentences = []\n",
    "        for sentence in sentences:\n",
    "            processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "            words = processed.split()\n",
    "            split_sentences.append(words)\n",
    "\n",
    "        skip_gram = 1\n",
    "        workers = multiprocessing.cpu_count()\n",
    "        bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "        model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "        model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "        \n",
    "        #모델 설계 완료\n",
    "\n",
    "        # ACTION VERBS 표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "        ##################################################\n",
    "        # verbs_list\n",
    "\n",
    "        ####문장에 list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "        \n",
    "        #우선 토큰화한다.\n",
    "        retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "        token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "        #print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "        #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "        filtered_chr_text = []\n",
    "        for k in token_input_text:\n",
    "            for j in verbs_list:\n",
    "                if k == j:\n",
    "                    filtered_chr_text.append(j)\n",
    "        \n",
    "        #print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "        \n",
    "        filtered_chr_text_ = set(filtered_chr_text) #중복제거\n",
    "        filtered_chr_text__ = list(filtered_chr_text_) #다시 리스트로 변환\n",
    "        #print (filtered_chr_text__) # 중복값 제거 확인\n",
    "        \n",
    "#         for i in filtered_chr_text__:\n",
    "#             ext_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "        \n",
    "#         char_total_count = len(filtered_chr_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 표현 수\n",
    "#         char_count_ = len(filtered_chr_text__) #중복제거된  표현 총 수\n",
    "            \n",
    "#         result_char_ratio = round(char_total_count/total_words * 100, 2)\n",
    "        \n",
    "#         df_conf_words = pd.DataFrame(ext_sim_words_key, columns=['words','values']) #데이터프레임으로 변환\n",
    "#         df_r = df_conf_words['words'] #words 컬럼 값 추출\n",
    "#         ext_sim_words_key = df_r.values.tolist() # 유사단어 추출\n",
    "\n",
    "        #return result_char_ratio, total_sentences, total_words, char_total_count, char_count_, ext_sim_words_key\n",
    "        ext_sim_words_key = filtered_chr_text__\n",
    "        return ext_sim_words_key\n",
    "\n",
    "\n",
    "    # 입력문장에서 맥락상 Aciton Verbs와 유사한 의미의 단어를 추출\n",
    "    ext_action_verbs = actionverb_sim_words(input_text)\n",
    "\n",
    "    #########################################################################\n",
    "    # 8.이제 입력문장에서 사용용된 Action Verbs 단어를 비교하여 추출해보자.\n",
    "\n",
    "    # Action Verbs를 모두 모음(직접적인 단어, 문맥상 유사어 포함)\n",
    "    all_ac_verbs_list = verbs_list + ext_action_verbs\n",
    "\n",
    "    #입력한 리스트 값을 하나씩 불러와서 데이터프레이에 있는지 비교 찾아내서 해당 점수를 가져오기\n",
    "    graph_calculation_list =[0]\n",
    "    get_words__ = []\n",
    "    counter= 0\n",
    "    for h in input_text_list: #데이터프레임에서 인덱스의 값과 비교하여\n",
    "        if h in all_ac_verbs_list: #df에 특정 단어가 있다면, 해당하는 컬럼의 값을 가져오기\n",
    "            get_words__.append(h) # 동일하면 저장하기\n",
    "            #print('counter :', counter)\n",
    "            graph_calculation_list.append(round(graph_calculation_list[counter]+2,2))\n",
    "            #print ('graph_calculation_list[counter]:', graph_calculation_list[counter])\n",
    "            #graph_calculation_list.append(random.randrange(1,10))\n",
    "            counter += 1\n",
    "        else: #없다면\n",
    "            #print('counter :', counter)\n",
    "            graph_calculation_list.append(round(graph_calculation_list[counter]-0.1,2)) \n",
    "            counter += 1\n",
    "    #문장에 Action Verbs 추출확인\n",
    "    #get_words__ \n",
    "\n",
    "\n",
    "    def divide_list(l, n): \n",
    "        # 리스트 l의 길이가 n이면 계속 반복\n",
    "        for i in range(0, int(len(l)), int(n)): \n",
    "            yield l[i:i + int(n)] \n",
    "        \n",
    "    # 한 리스트에 몇개씩 담을지 결정 = 20개씩\n",
    "\n",
    "    n = len(graph_calculation_list)/20\n",
    "\n",
    "    result_gr = list(divide_list(graph_calculation_list, n))\n",
    "\n",
    "    gr_cal = []\n",
    "    for regr in result_gr:\n",
    "        avg_gr = sum(regr,0.0)/len(regr) #묶어서 평균을 내고 \n",
    "        gr_cal.append(abs(round(avg_gr,2))) #절대값을 전환해서\n",
    "\n",
    "\n",
    "    graph_calculation_list = gr_cal  ## 그래프를 위한 최종결과 계산 후, 이것을 딕셔너리로 반환하여 > 그래프로 표현하기\n",
    "    #########################################################################\n",
    "    # 9. 그래프 출력 : 문장 전체를 단어로 분리하고, Action verbs가 사용된 부분을 그래프로 표시\n",
    "\n",
    "    # 전체 글에서 Action verbs가 언급된 부분을 리스트로 계산\n",
    "    # graph_calculation_list \n",
    "\n",
    "    #그래프로 표시됨\n",
    "    # plt.plot(graph_calculation_list)\n",
    "    # plt.xlabel('STORY')\n",
    "    # plt.ylabel('ACTON VERBS')\n",
    "    # plt.title('USAGE OF ACTION VERBS ANALYSIS')\n",
    "    # plt.legend(['action verbs'])\n",
    "    # plt.show()\n",
    "\n",
    "    #########################################################################\n",
    "    # 10.입력한 에세이 문장에서 Action Verbs가 얼마나 포함되어 있는지 포함비율 분석\n",
    "    action_verbs_ratio = round(len(get_words__)/len(input_text_list) *100, 3)\n",
    "\n",
    "    print (\"ACTION VERBS RATIO :\", action_verbs_ratio )\n",
    "\n",
    "\n",
    "    #########################################################################\n",
    "    # 11. 글속에 감정이 얼마나 표현되어 있는지 분석 - origin (Bert pre trained model 활용)\n",
    "    from transformers import BertTokenizer\n",
    "    from model import BertForMultiLabelClassification\n",
    "    #from essayai.ai_plot.model import BertForMultiLabelClassification\n",
    "    from multilabel_pipeline import MultiLabelPipeline\n",
    "    #from essayai.ai_plot.multilabel_pipeline import MultiLabelPipeline\n",
    "    from pprint import pprint\n",
    "\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "    model = BertForMultiLabelClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "\n",
    "    goemotions = MultiLabelPipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        threshold=0.3\n",
    "    )\n",
    "    #결과확인\n",
    "    #print(goemotions(texts))\n",
    "    ########## 여기서는 최초 입력 에세이를 적용한다. input_text !!!!!!!!\n",
    "    re_text = input_text.split(\".\")\n",
    "\n",
    "    #데이터 전처리 \n",
    "    def cleaning(datas):\n",
    "\n",
    "        fin_datas = []\n",
    "\n",
    "        for data in datas:\n",
    "            # 영문자 이외 문자는 공백으로 변환\n",
    "            only_english = re.sub('[^a-zA-Z]', ' ', data)\n",
    "        \n",
    "            # 데이터를 리스트에 추가 \n",
    "            fin_datas.append(only_english)\n",
    "\n",
    "        return fin_datas\n",
    "\n",
    "    texts = cleaning(re_text)\n",
    "\n",
    "    #분석된 감정만 추출\n",
    "    emo_re = goemotions(texts)\n",
    "\n",
    "    emo_all = []\n",
    "    for list_val in range(0, len(emo_re)):\n",
    "        #print(emo_re[list_val]['labels'],emo_re[list_val]['scores'])\n",
    "        #mo_all.append((emo_re[list_val]['labels'],emo_re[list_val]['scores'])) #KEY, VALUE만 추출하여 리스트로 저장\n",
    "        #emo_all.append(emo_re[list_val]['scores'])\n",
    "        emo_all.append((emo_re[list_val]['labels']))\n",
    "        \n",
    "    #추출결과 확인 \n",
    "    # emo_all\n",
    "\n",
    "    # ['sadness'],\n",
    "    #  ['anger'],\n",
    "    #  ['admiration', 'realization'],\n",
    "    #  ['admiration', 'disappointment'],\n",
    "    #  ['love'],\n",
    "    #  ['sadness', 'neutral'],\n",
    "    #  ['realization', 'neutral'],\n",
    "    #  ['neutral'],\n",
    "    #  ['optimism'],\n",
    "    #  ['neutral'],\n",
    "    #  ['excitement'],\n",
    "    #  ['neutral'],\n",
    "    #  ['neutral'],\n",
    "    #  ['caring'],\n",
    "    #  ['gratitude'],\n",
    "    #  ['admiration', 'approval'], ...\n",
    "\n",
    "    from pandas.core.common import flatten #이중리스틀 FLATTEN하게 변환\n",
    "    flat_list = list(flatten(emo_all))\n",
    "\n",
    "    # ['neutral',\n",
    "    #  'neutral',\n",
    "    #  'sadness',\n",
    "    #  'anger',\n",
    "    #  'admiration',\n",
    "    #  'realization',\n",
    "    #  'admiration',\n",
    "    #  'disappointment',\n",
    "\n",
    "\n",
    "    #중립적인 감정을 제외하고, 입력한 문장에서 다양한 감정을 모두 추출하고 어떤 감정이 있는지 계산해보자\n",
    "    unique = []\n",
    "    for r in flat_list:\n",
    "        if r == 'neutral':\n",
    "            pass\n",
    "        else:\n",
    "            unique.append(r)\n",
    "\n",
    "    #중립감정 제거 및 유일한 감정값 확인\n",
    "    #unique\n",
    "    unique_re = set(unique) #중복제거\n",
    "\n",
    "    ############################################################################\n",
    "    # 글에 표현된 감정이 얼마나 다양한지 분석 결과!!!¶\n",
    "    print(\"====================================================================\")\n",
    "    print(\"에세이에 표현된 다양한 감정 수:\", len(unique_re))\n",
    "    print(\"====================================================================\")\n",
    "\n",
    "    #분석가능한 감정 총 감정 수 - Bert origin model 적용시 28개 감정 추출돰\n",
    "    total_num_emotion_analyzed = 28\n",
    "\n",
    "    # 감정기복 비율 계산 !!!\n",
    "    result_emo_swings =round(len(unique_re)/total_num_emotion_analyzed *100,1) #소숫점 첫째자리만 표현\n",
    "    result_emo_swings\n",
    "    print(\"문장에 표현된 감정 비율 : \", result_emo_swings)\n",
    "    print(\"====================================================================\")\n",
    "\n",
    "\n",
    "    #########################################################################\n",
    "    # 12. SETTING RATIO 계산\n",
    "\n",
    "    def setting_anaysis(text):\n",
    "\n",
    "        essay_input_corpus = str(text) #문장입력\n",
    "        essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "        sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "        total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "        total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "        split_sentences = []\n",
    "        for sentence in sentences:\n",
    "            processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "            words = processed.split()\n",
    "            split_sentences.append(words)\n",
    "\n",
    "        skip_gram = 1\n",
    "        workers = multiprocessing.cpu_count()\n",
    "        bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "        model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "        model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "        \n",
    "        #모델 설계 완료\n",
    "\n",
    "        #setting을 표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "        location_list = ['above', 'behind','below','beside','betweed','by','in','inside','near',\n",
    "                        'on','over','through']\n",
    "        time_list = ['after', 'before','by','during','from','on','past','since','through','to','until','upon']\n",
    "        \n",
    "        movement_list = ['against','along','down','from','into','off','on','onto','out of','toward','up','upon']\n",
    "        \n",
    "        palce_terrain_type_list = ['wood', 'forest', 'copse', 'bush', 'trees', 'stand',\n",
    "                                    'swamp', 'marsh', 'wetland', 'fen', 'bog', 'moor', 'heath', 'fells', 'morass',\n",
    "                                    'jungle', 'rainforest', 'cloud forest','plains', 'fields', 'grass', 'grassland', \n",
    "                                    'savannah', 'flood plain', 'flats', 'prairie','tundra', 'iceberg', 'glacier', \n",
    "                                    'snowfields','hills', 'highland,' 'heights', 'plateau', 'badland', 'kame', 'shield',\n",
    "                                    'downs', 'downland', 'ridge', 'ridgeline','hollow,' 'valley',' vale','glen', 'dell',\n",
    "                                    'mountain', 'peak', 'summit', 'rise', 'pass', 'notch', 'crown', 'mount', 'switchback',\n",
    "                                    'furth','canyon', 'cliff', 'bluff,' 'ravine', 'gully', 'gulch', 'gorge',\n",
    "                                    'desert', 'scrub', 'waste', 'wasteland', 'sands', 'dunes',\n",
    "                                    'volcano', 'crater', 'cone', 'geyser', 'lava fields']\n",
    "        \n",
    "        water_list = ['ocean', 'sea', 'coast', 'beach', 'shore', 'strand','bay', 'port', 'harbour', 'fjord', 'vike',\n",
    "                    'cove', 'shoals', 'lagoon', 'firth', 'bight', 'sound', 'strait', 'gulf', 'inlet', 'loch', \n",
    "                    'bayou','dock', 'pier', 'anchorage', 'jetty', 'wharf', 'marina', 'landing', 'mooring', 'berth', \n",
    "                    'quay', 'staith','river', 'stream', 'creek', 'brook', 'waterway', 'rill','delta', 'bank', 'runoff',\n",
    "                    'bend', 'meander', 'backwater','lake', 'pool', 'pond', 'dugout', 'fountain', 'spring', \n",
    "                    'watering-hole', 'oasis','well', 'cistern', 'reservoir','waterfall', 'falls', 'rapids', 'cataract', \n",
    "                    'cascade','bridge', 'crossing', 'causeway', 'viaduct', 'aquaduct', 'ford', 'ferry','dam', 'dike', \n",
    "                    'bar', 'canal', 'ditch','peninsula', 'isthmus', 'island', 'isle', 'sandbar', 'reef', 'atoll', \n",
    "                    'archipelago', 'cay','shipwreck', 'derelict']\n",
    "        \n",
    "        \n",
    "        outdoor_places_list = ['clearing', 'meadow', 'grove', 'glade', 'fairy ring','earldom', 'fief', 'shire',\n",
    "                                'ruin', 'acropolis', 'desolation', 'remnant', 'remains',\n",
    "                                'henge', 'cairn', 'circle', 'mound', 'barrow', 'earthworks', 'petroglyphs',\n",
    "                                'lookout', 'aerie', 'promontory', 'outcropping', 'ledge', 'overhang', 'mesa', 'butte',\n",
    "                                'outland', 'outback', 'territory', 'reaches', 'wild', 'wilderness', 'expanse',\n",
    "                                'view', 'vista', 'tableau', 'spectacle', 'landscape', 'seascape', 'aurora', 'landmark',\n",
    "                                'battlefield', 'trenches', 'gambit', 'folly', 'conquest', 'claim', 'muster', 'post',\n",
    "                                'path', 'road', 'track', 'route', 'highway', 'way', 'trail', 'lane', 'thoroughfare', 'pike',\n",
    "                                'alley', 'street', 'avenue', 'boulevard', 'promenade', 'esplande', 'boardwalk',\n",
    "                                'crossroad', 'junction', 'intersection', 'turn', 'corner','plaza', 'terrace', 'square', \n",
    "                                'courtyard', 'court', 'park', 'marketplace', 'bazaar', 'fairground','realm', 'land', 'country',\n",
    "                                'nation', 'state', 'protectorate', 'empire', 'kingdom', 'principality','domain', 'dominion',\n",
    "                                'demesne', 'province', 'county', 'duchy', 'barony', 'baronetcy', 'march', 'canton']\n",
    "\n",
    "        \n",
    "        underground_list = ['pit', 'hole', 'abyss', 'sinkhole', 'crack', 'chasm', 'scar', 'rift', 'trench', 'fissure',\n",
    "                            'cavern', 'cave', 'gallery', 'grotto', 'karst',\n",
    "                            'mine', 'quarry', 'shaft', 'vein','graveyard', 'cemetery',\n",
    "                            'darkness', 'shadow', 'depths', 'void','maze', 'labyrinth'\n",
    "                            'tomb', 'grave', 'crypt', 'sepulchre', 'mausoleum', 'ossuary', 'boneyard']\n",
    "                            \n",
    "        living_places_list = ['nest', 'burrow', 'lair', 'den', 'bolt-hole', 'warren', 'roost', 'rookery', 'hibernaculum',\n",
    "                            'home', 'rest', 'hideout', 'hideaway', 'retreat', 'resting-place', 'safehouse', 'sanctuary',\n",
    "                            'respite', 'lodge','slum', 'shantytown', 'ghetto','camp', 'meeting place,' 'bivouac', 'campsite', \n",
    "                            'encampment','tepee', 'tent', 'wigwam', 'shelter', 'lean-to', 'yurt','house', 'mansion', 'estate',\n",
    "                            'villa','hut', 'palace', 'outbuilding', 'shack tenement', 'hovel', 'manse', 'manor', 'longhouse',\n",
    "                            'cottage', 'cabin','parsonage', 'rectory', 'vicarge', 'friary', 'priory','abbey', 'monastery', \n",
    "                            'nunnery', 'cloister', 'convent', 'hermitage','castle', 'keep', 'fort', 'fortress', 'citadel', \n",
    "                            'bailey', 'motte', 'stronghold', 'hold', 'chateau', 'outpost', 'redoubt',\n",
    "                            'town', 'village', 'hamlet', 'city', 'metropolis','settlement', 'commune']\n",
    "\n",
    "        building_facilities_list = ['temple', 'shrine', 'church', 'cathedral', 'tabernacle', 'ark', 'sanctum', 'parish', 'university',\n",
    "                                    'chapel', 'synagogue', 'mosque','pyramid', 'ziggurat', 'prison', 'jail', 'dungeon',\n",
    "                                    'oubliette', 'hospital', 'hospice', 'stocks', 'gallows','asylum', 'madhouse', 'bedlam',\n",
    "                                    'vault', 'treasury', 'warehouse', 'cellar', 'relicry', 'repository',\n",
    "                                    'barracks', 'armoury','sewer', 'gutter', 'catacombs', 'dump', 'middens', 'pipes', 'baths', 'heap',\n",
    "                                    'mill', 'windmill', 'sawmill', 'smithy', 'forge', 'workshop', 'brickyard', 'shipyard', 'forgeworks',\n",
    "                                    'foundry','bakery', 'brewery', 'almshouse', 'counting house', 'courthouse', 'apothecary', 'haberdashery', 'cobbler',\n",
    "                                    'garden', 'menagerie', 'zoo', 'aquarium', 'terrarium', 'conservatory', 'lawn', 'greenhouse',\n",
    "                                    'farm', 'orchard', 'vineyard', 'ranch', 'apiary', 'farmstead', 'homestead',\n",
    "                                    'pasture', 'commons', 'granary', 'silo', 'crop','barn', 'stable', 'pen', 'kennel', 'mews', 'hutch', \n",
    "                                    'pound', 'coop', 'stockade', 'yard', 'lumber yard','tavern', 'inn', 'pub', 'brothel', 'whorehouse',\n",
    "                                    'cathouse', 'discotheque','lighthouse', 'beacon','amphitheatre', 'colosseum', 'stadium', 'arena', \n",
    "                                    'circus','academy', 'university', 'campus', 'college', 'library', 'scriptorium', 'laboratory', \n",
    "                                    'observatory', 'museum']\n",
    "        \n",
    "        \n",
    "        architecture_list = ['hall', 'chamber', 'room','nave', 'aisle', 'vestibule',\n",
    "                            'antechamber', 'chantry', 'pulpit','dome', 'arch', 'colonnade',\n",
    "                            'stair', 'ladder', 'climb', 'ramp', 'steps',\n",
    "                            'portal', 'mouth', 'opening', 'door', 'gate', 'entrance', 'maw',\n",
    "                            'tunnel', 'passage', 'corridor', 'hallway', 'chute', 'slide', 'tube', 'trapdoor',\n",
    "                            'tower', 'turret', 'belfry','wall', 'fortifications', 'ramparts', 'pallisade', 'battlements',\n",
    "                            'portcullis', 'barbican','throne room', 'ballroom','roof', 'rooftops', 'chimney', 'attic',\n",
    "                            'loft', 'gable', 'eaves', 'belvedere','balcony', 'balustrade', 'parapet', 'walkway', 'catwalk',\n",
    "                            'pavillion', 'pagoda', 'gazebo','mirror', 'glass', 'mere','throne', 'seat', 'dais',\n",
    "                            'pillar', 'column', 'stone', 'spike', 'rock', 'megalith', 'menhir', 'dolmen', 'obelisk',\n",
    "                            'statue', 'giant', 'head', 'arm', 'leg', 'body', 'chest', 'body', 'face', 'visage', 'gargoyle', 'grotesque',\n",
    "                            'fire', 'flame', 'bonfire', 'hearth', 'fireplace', 'furnace', 'stove','window', 'grate', 'peephole', \n",
    "                            'arrowslit', 'slit', 'balistraria', 'lancet', 'aperture', 'dormerl']\n",
    "        \n",
    "        \n",
    "        setting_words_filter_list = location_list + time_list + movement_list + palce_terrain_type_list + water_list + outdoor_places_list + underground_list + underground_list + living_places_list + building_facilities_list + architecture_list\n",
    "\n",
    "        \n",
    "        ####문장에 setting_words_filter_list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "        #우선 토큰화한다.\n",
    "        retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "        token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "        #print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "        #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "        filtered_setting_text = []\n",
    "        for k in token_input_text:\n",
    "            for j in setting_words_filter_list:\n",
    "                if k == j:\n",
    "                    filtered_setting_text.append(j)\n",
    "        \n",
    "        #print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "        \n",
    "        filtered_setting_text_ = set(filtered_setting_text) #중복제거\n",
    "        filtered_setting_text__ = list(filtered_setting_text_) #다시 리스트로 변환\n",
    "        print (filtered_setting_text__) # 중복값 제거 확인\n",
    "        \n",
    "#         for i in filtered_setting_text__:\n",
    "#             ext_setting_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "        \n",
    "        setting_total_count = len(filtered_setting_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 setting 표현 수\n",
    "        setting_count_ = len(filtered_setting_text__) #중복제거된 setting표현 총 수\n",
    "            \n",
    "        result_setting_words_ratio = round(setting_total_count/total_words * 100, 2)\n",
    "        #return result_setting_words_ratio, total_sentences, total_words, setting_total_count, setting_count_, ext_setting_sim_words_key\n",
    "        return result_setting_words_ratio\n",
    "\n",
    "\n",
    "    # 셋팅 비율 계산\n",
    "    settig_ratio_re = setting_anaysis(input_text)\n",
    "    print(\"====================================================================\")\n",
    "    print(\"SETTING RATIO : \", settig_ratio_re)\n",
    "    print(\"====================================================================\")\n",
    "\n",
    "\n",
    "    ###################################################################################\n",
    "    # 13. PLOT COMPLEXITY 계산¶ - 캐릭터 20% + conflict 40% + 감정기복 30% + setting 10%\n",
    "    ###################################################################################\n",
    "\n",
    "    def character(text):\n",
    "\n",
    "        essay_input_corpus = str(text) #문장입력\n",
    "        essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "        sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "        total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "        total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "        \n",
    "        split_sentences = []\n",
    "        for sentence in sentences:\n",
    "            processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "            words = processed.split()\n",
    "            split_sentences.append(words)\n",
    "\n",
    "        skip_gram = 1\n",
    "        workers = multiprocessing.cpu_count()\n",
    "        bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "        model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "        model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "        \n",
    "        #모델 설계 완료\n",
    "\n",
    "        #캐릭터 표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "        character_list = ['i', 'my', 'me', 'mine', 'you', 'your', 'they','them',\n",
    "                        'yours', 'he','him','his' 'she','her','it','someone','their', 'myself', 'aunt',\n",
    "                        'brother','cousin','daughter','father','grandchild','granddaughter','granddson','grandfather',\n",
    "                        'grandmother','great-grandchild','husband','ex-husband','son-in-law', 'daughter-in-law','mother',\n",
    "                        'niece','nephew','parents','sister','son','stepfather','stepmother','stepdaughter', 'stepson',\n",
    "                        'twin','uncle','widow','widower','wife','ex-wife']\n",
    "        \n",
    "        ####문장에 char_list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "        #우선 토큰화한다.\n",
    "        retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "        token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "        #print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "        #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "        filtered_chr_text = []\n",
    "        for k in token_input_text:\n",
    "            for j in character_list:\n",
    "                if k == j:\n",
    "                    filtered_chr_text.append(j)\n",
    "        \n",
    "        #print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "        \n",
    "        filtered_chr_text_ = set(filtered_chr_text) #중복제거\n",
    "        filtered_chr_text__ = list(filtered_chr_text_) #다시 리스트로 변환\n",
    "        #print (filtered_chr_text__) # 중복값 제거 확인\n",
    "        \n",
    "        for i in filtered_chr_text__:\n",
    "            ext_sim_words_key = model.wv.most_similar_cosmul(i) #모델적용\n",
    "        \n",
    "        char_total_count = len(filtered_chr_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 캐릭터 표현 수\n",
    "        char_count_ = len(filtered_chr_text__) #중복제거된 캐릭터 표현 총 수\n",
    "            \n",
    "        result_char_ratio = round(char_total_count/total_words * 100, 2)\n",
    "\n",
    "        #return result_char_ratio, total_sentences, total_words, char_total_count, char_count_, ext_sim_words_key\n",
    "        return result_char_ratio\n",
    "\n",
    "\n",
    "    #######################################################################\n",
    "    ##########################   Plot complexity  ######################\n",
    "    #######################################################################\n",
    "    # 이제 최종 계산을 해보자.\n",
    "    # character_ratio_result #캐릭터 비율 20%\n",
    "    # result_emo_swings # 감정기복 비율 30%\n",
    "    # conflict_word_ratio #CONFLICT 비율 계산 40%\n",
    "    # settig_ratio_re #Setting 비율 계산 10%\n",
    "    \n",
    "    # 전체 문장에서 캐릭터를 의미하는 단어나 유사어 비율 \n",
    "\n",
    "    character_ratio_result = character(input_text)\n",
    "    character_ratio_result\n",
    "    print(\"전체 문장에서 캐릭터를 의미하는 단어나 유사어 비율 :\", character_ratio_result)\n",
    "\n",
    "    ###########################################################\n",
    "    ############# Degree of Conflict  비율 계산 #################\n",
    "    conflict_word_ratio = round(len(count_conflict_list) / len(input_text_list) * 1000, 1)  \n",
    "    print(\"Degree of conflict  단어가 전체 문장(단어)에서 차지하는 비율 계산 :\", conflict_word_ratio)\n",
    "\n",
    "    global coflict_ratio\n",
    "    coflict_ratio = [conflict_word_ratio] #그래프로 표현하는 값\n",
    "\n",
    "\n",
    "\n",
    "    ###########################################################\n",
    "    ############# Emotional Rollercoaster  비율 계산 #################\n",
    "    print(\"감정기복비율 :\", result_emo_swings) \n",
    "\n",
    "    # 셋팅비율 계산\n",
    "    print(\"셋팅비율 계산 : \", settig_ratio_re)\n",
    "\n",
    "    # 4개의 값을 리스트로 담는다.\n",
    "    de_flt_list = [character_ratio_result, result_emo_swings, conflict_word_ratio, settig_ratio_re]\n",
    "\n",
    "\n",
    "    import numpy\n",
    "    numpy.mean(de_flt_list) #평균\n",
    "    numpy.var(de_flt_list) #분산\n",
    "    numpy.std(de_flt_list) #표준편차\n",
    "\n",
    "    #######################################################################\n",
    "    ############# Plot complexity  st_input 표준편차 비율 계산 #################\n",
    "    de_flt_list_ = [character_ratio_result*2, result_emo_swings*3, conflict_word_ratio*4, settig_ratio_re]\n",
    "    numpy.mean(de_flt_list_) # 평균\n",
    "    numpy.var(de_flt_list_) # 분산\n",
    "    st_input = numpy.std(de_flt_list_) # 표준편차 ----> 이 값으로 계산\n",
    "    print(\"Plot Complxity :\", st_input )\n",
    "\n",
    "    global plot_comp_ratio\n",
    "\n",
    "    plot_comp_ratio = [round(st_input, 2)]\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"===============================================================================\")\n",
    "    print(\"======================      Degree of Conflict   ==============================\")\n",
    "    print(\"===============================================================================\")\n",
    "\n",
    "    \n",
    "    # return 값 설명  ====  plot complexity :st_input , emotion rollercoster: result_emo_swings, degree of conflict: conflict_word_ratio\n",
    "    return st_input, result_emo_swings, conflict_word_ratio,df_sent, graph_calculation_list\n",
    "\n",
    "\n",
    "#     return { \n",
    "\n",
    "#             \"result_all_plot\":result_all_avg, \n",
    "#             \"emotional_rollercoaster\":result_emo_swings, \n",
    "#             \"plot_complexity\":st_input, \n",
    "#             \"degree_conflict\": conflict_word_ratio, \n",
    "#             \"result_emotional_rollercoaster\": result_emotional_rollwercoaster,\n",
    "#             \"result_plot_complexity\" : result_plot_complexity,\n",
    "#             \"result_degree_conflict\" : result_degree_conflict,\n",
    "            \n",
    "#             \"neg\" : df_sent[\"neg\"],\n",
    "#             \"neu\" : df_sent[\"neu\"],\n",
    "#             \"pos\" : df_sent[\"pos\"],\n",
    "#             \"compound\" : df_sent[\"compound\"],\n",
    "#             \"graph_calculation_list\" : graph_calculation_list\n",
    "            \n",
    "#             }\n",
    "\n",
    "\n",
    "\n",
    "def ai_plot_coflict_total_analysis(input_text):\n",
    "\n",
    "    plot_conf_re = ai_plot_conf(input_text)\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"1명의 에세이 결과 계산점수 :\", plot_conf_re)\n",
    "    #1명의 에세이 결과 계산점수 : (28.602484157848945, 25.0, 0.3,df_sent)\n",
    "\n",
    "    # 위에서 계산한 총 4개의 값을 개인, 그룹의 값과 비교하여 lacking, ideal, overboard 계산\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 개인에세이 값 계산 4가지 결과 추출 >>> personal_value 로 입력됨\n",
    "    plot_complexity = plot_conf_re[0]\n",
    "    emotional_rollercoaster = plot_conf_re[1]\n",
    "    degree_conflict = plot_conf_re[2]\n",
    "    \n",
    "    graph_calculation_list = plot_conf_re[4]\n",
    "\n",
    "\n",
    "    ########################################################\n",
    "    ## 1000명 데이터의 각 값(char_desc_mean)의 평균 값 전달.>>> 고정값으로 미리 계산하여 입력 ai_plot_coflict_1000data_preprocessing 코드 참조\n",
    "    plot_conflict_all_mean = [80, 15, 0.314] # 이랬음 >>> [80, 64, 0.314]\n",
    "    group_db_fin_result_plot = [5.0]\n",
    "    ########################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plot_complexity_mean = plot_conflict_all_mean[0] #첫번째 값을 가져옴\n",
    "    emotional_rollercoaster_mean = plot_conflict_all_mean[1] #\n",
    "    degree_conflict_mean = plot_conflict_all_mean[2] #\n",
    "\n",
    "\n",
    "    def lackigIdealOverboard(group_mean, personal_value): # group_mean: 1000명 평균, personal_value|:개인값\n",
    "        ideal_mean = group_mean\n",
    "        one_ps_char_desc = personal_value\n",
    "        #최대, 최소값 기준으로 구간설정. 구간비율 30% => 0.3으로 설정\n",
    "        min_ = int(ideal_mean-ideal_mean*0.6)\n",
    "        print('min_', min_)\n",
    "        max_ = int(ideal_mean+ideal_mean*0.6)\n",
    "        print('max_: ', max_)\n",
    "        div_ = int(((ideal_mean+ideal_mean*0.6)-(ideal_mean-ideal_mean*0.6))/3)\n",
    "        print('div_:', div_)\n",
    "\n",
    "        #결과 판단 Lacking, Ideal, Overboard\n",
    "        cal_abs = abs(ideal_mean - one_ps_char_desc) # 개인 - 단체 값의 절대값계산\n",
    "\n",
    "        print('cal_abs 절대값 :', cal_abs)\n",
    "        compare7 = (one_ps_char_desc + ideal_mean)/6\n",
    "        compare6 = (one_ps_char_desc + ideal_mean)/5\n",
    "        compare5 = (one_ps_char_desc + ideal_mean)/4\n",
    "        compare4 = (one_ps_char_desc + ideal_mean)/3\n",
    "        compare3 = (one_ps_char_desc + ideal_mean)/2\n",
    "        print('compare7 :', compare7)\n",
    "        print('compare6 :', compare6)\n",
    "        print('compare5 :', compare5)\n",
    "        print('compare4 :', compare4)\n",
    "        print('compare3 :', compare3)\n",
    "\n",
    "\n",
    "\n",
    "        if one_ps_char_desc > ideal_mean: # 개인점수가 평균보다 클 경우는 overboard\n",
    "            if cal_abs > compare3: # 37 개인점수가 개인평균차의 절대값보다 클 경우, 즉 차이가 많이 날경우\n",
    "                print(\"Overboard: 2\")\n",
    "                result = 2 #overboard\n",
    "                score = 1\n",
    "            elif cal_abs > compare4: # 28\n",
    "                print(\"Overvoard: 2\")\n",
    "                result = 2\n",
    "                score = 2\n",
    "            elif cal_abs > compare5: # 22\n",
    "                print(\"Overvoard: 2\")\n",
    "                result = 2\n",
    "                score = 3\n",
    "            elif cal_abs > compare6: # 18\n",
    "                print(\"Overvoard: 2\")\n",
    "                result = 2\n",
    "                score = 4\n",
    "            else:\n",
    "                print(\"Ideal: 1\")\n",
    "                result = 1\n",
    "                score = 5\n",
    "        elif one_ps_char_desc < ideal_mean: # 개인점수가 평균보다 작을 경우 lacking\n",
    "            if cal_abs > compare3: # 37 개인점수가 개인평균차의 절대값보다 클 경우, 즉 차이가 많이 날경우\n",
    "                print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                score = 1\n",
    "            elif cal_abs > compare4: # 28\n",
    "                print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                score = 2\n",
    "            elif cal_abs > compare5: # 22\n",
    "                print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                score = 3\n",
    "            elif cal_abs > compare6: # 18\n",
    "                print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                score = 4\n",
    "            else:\n",
    "                print(\"Ideal: 1\")\n",
    "                result = 1\n",
    "                score = 5\n",
    "                \n",
    "        else:\n",
    "            print(\"Ideal: 1\")\n",
    "            result = 1\n",
    "            score = 5\n",
    "\n",
    "        return result, score\n",
    "\n",
    "    # 값계산(0:lacking, 1:score), (0:lacking, 2:score), (2:overboard, 1:score)\n",
    "    plot_complexity_result = lackigIdealOverboard(plot_complexity_mean, plot_complexity)\n",
    "    emotional_rollercoaster_result = lackigIdealOverboard(emotional_rollercoaster_mean, emotional_rollercoaster)\n",
    "    degree_conflict_result = lackigIdealOverboard(degree_conflict_mean, degree_conflict)\n",
    "\n",
    "    fin_result = [plot_complexity_result, emotional_rollercoaster_result, degree_conflict_result]\n",
    "    print(\"fin_result:\", fin_result)  # [(0:lacking, 1:score), (0:lacking, 2:score), (2:overboard, 1:score)]\n",
    "\n",
    "    each_fin_result = [fin_result[0][0], fin_result[1][0], fin_result[2][0]]\n",
    "\n",
    "    # 최종 character  전체 점수 계산\n",
    "    overall_character_rating = [round((fin_result[0][1]+ fin_result[1][1] + fin_result[2][1])/3,2)]\n",
    "\n",
    "    result_final = each_fin_result + overall_character_rating + group_db_fin_result_plot + coflict_ratio + plot_comp_ratio\n",
    "\n",
    "    df_sent = plot_conf_re[3]\n",
    "    \n",
    "    neg =  list(map(float, df_sent[\"neg\"]))\n",
    "    neu =  list(map(float, df_sent[\"neu\"]))\n",
    "    pos =  list(map(float, df_sent[\"pos\"]))\n",
    "    compound =  list(map(float, df_sent[\"compound\"]))\n",
    "    \n",
    "    print(df_sent)\n",
    "    \n",
    "    print(\"neg>>>>>\",neg)\n",
    "    print(\"neu>>>>>\",neu)\n",
    "    print(\"pos>>>>>\",pos)\n",
    "    print(\"componud>>>\",compound)\n",
    "    \n",
    "\n",
    "    print ( \"graph_calculation_list\" , graph_calculation_list) \n",
    "\n",
    "\n",
    "    data = {\n",
    "        \n",
    "            \"result_all_plot\":result_final[3], \n",
    "            \n",
    "            \"emotional_rollercoaster\":round(emotional_rollercoaster,2), \n",
    "            \"plot_complexity\":round(plot_complexity,2), \n",
    "            \"degree_conflict\": round(degree_conflict,2), \n",
    "            \n",
    "            \"result_plot_complexity\" : result_final[0],\n",
    "            \"result_emotional_rollercoaster\": result_final[1],\n",
    "            \"result_degree_conflict\" : result_final[2],\n",
    "            \n",
    "            \"neg\" : neg,\n",
    "            \"neu\" : neu,\n",
    "            \"pos\" : pos,\n",
    "            \"compound\" : compound,\n",
    "            \"graph_calculation_list\" : graph_calculation_list\n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    return data \n",
    "\n",
    "    #  [0, 0, 2, 1.33, 5.0, 3.0, 26.52]\n",
    "\n",
    "    # [0: plot_complexity_result-lacking, \n",
    "    #  0: emotional_rollercoaster_result - lacking, \n",
    "    #  2: degree_conflict_result-overboard, \n",
    "    #  1.3: overall_character_rating,\n",
    "    #  5.0: group_db_fin_result_plot(1000명 평균값), \n",
    "    #  3.0: conflict_ratio] ----------------> Conflict\n",
    "    #  26.52] : ----------------------------> Plot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### 실행 테스트  ######\n",
    "# print(\"result\\n\\n\",ai_plot_coflict_total_analysis(input_text))\n",
    "\n",
    "\n",
    "### {'result_all_plot': 1.33, 'emotional_rollercoaster': 25.0, 'plot_complexity': 26.52, 'degree_conflict': 3.0, 'result_plot_complexity': 0, 'result_emotional_rollercoaster': 0, 'result_degree_conflict': 2, 'neg': [0.0, 0.0, 0.041, 0.044, 0.19, 0.0, 0.04, 0.0, 0.054, 0.0, 0.0, 0.118, 0.101, 0.0, 0.239, 0.133, 0.104, 0.0, 0.083, 0.09, 0.092, 0.058, 0.079, 0.121], 'neu': [0.808, 1.0, 0.778, 0.787, 0.678, 1.0, 0.884, 1.0, 0.79, 0.723, 1.0, 0.882, 0.739, 1.0, 0.761, 0.867, 0.896, 0.762, 0.702, 0.77, 0.667, 0.822, 0.702, 0.65], 'pos': [0.192, 0.0, 0.181, 0.169, 0.132, 0.0, 0.076, 0.0, 0.155, 0.277, 0.0, 0.0, 0.161, 0.0, 0.0, 0.0, 0.0, 0.238, 0.215, 0.14, 0.242, 0.119, 0.219, 0.23], 'compound': [0.228, 0.0, 0.7269, 0.6486, -0.2144, 0.0, 0.4588, 0.0, 0.624, 0.7579, 0.0, -0.5267, 0.0258, 0.0, -0.5719, -0.3354, -0.2732, 0.6486, 0.4588, 0.2206, 0.7351, 0.3182, 0.4939, 0.5719], 'graph_calculation_list': [0, -0.1, -0.2, -0.3, -0.4, -0.5, -0.6, -0.7, -0.8, -0.9, -1.0, -1.1, -1.2, -1.3, -1.4, -1.5, -1.6, -1.7, -1.8, -1.9, -2.0, 0.0, -0.1, -0.2, -0.3, -0.4, -0.5, -0.6, -0.7, -0.8, -0.9, -1.0, -1.1, -1.2, -1.3, -1.4, -1.5, -1.6, -1.7, -1.8, 0.2, 0.1, 0.0, -0.1, -0.2, -0.3, -0.4, -0.5, -0.6, -0.7, -0.8, -0.9, -1.0, -1.1, -1.2, -1.3, -1.4, -1.5, -1.6, -1.7, -1.8, -1.9, -2.0, -2.1, -2.2, -2.3, -2.4, -2.5, -2.6, -2.7, -2.8, -2.9, -3.0, -3.1, -3.2, -3.3, -3.4, -3.5, -3.6, -3.7, -3.8, -3.9, -4.0, -4.1, -4.2, -4.3, -4.4, -4.5, -4.6, -4.7, -4.8, -4.9, -5.0, -5.1, -5.2, -5.3, -5.4, -5.5, -5.6, -5.7, -5.8, -5.9, -6.0, -6.1, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8, -6.9, -7.0, -7.1, -7.2, -7.3, -7.4, -7.5, -5.5, -5.6, -5.7, -5.8, -3.8, -3.9, -4.0, -4.1, -4.2, -4.3, -2.3, -2.4, -2.5, -2.6, -2.7, -2.8, -2.9, -3.0, -3.1, -3.2, -3.3, -1.3, -1.4, -1.5, -1.6, -1.7, -1.8, -1.9, -2.0, -2.1, -2.2, -2.3, -2.4, -2.5, -2.6, -2.7, -2.8, -2.9, -3.0, -3.1, -3.2, -3.3, -3.4, -3.5, -1.5, -1.6, -1.7, -1.8, -1.9, -2.0, -2.1, -2.2, -2.3, -2.4, -2.5, -2.6, -2.7, -2.8, -2.9, -3.0, -3.1, -3.2, -3.3, -3.4, -3.5, -3.6, -3.7, -3.8, -3.9, -4.0, -4.1, -4.2, -4.3, -4.4, -4.5, -4.6, -4.7, -4.8, -4.9, -5.0, -5.1, -5.2, -5.3, -3.3, -3.4, -3.5, -3.6, -3.7, -3.8, -3.9, -4.0, -4.1, -4.2, -4.3, -4.4, -4.5, -4.6, -4.7, -4.8, -4.9, -5.0, -5.1, -5.2, -5.3, -5.4, -5.5, -5.6, -5.7, -5.8, -5.9, -6.0, -6.1, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8, -6.9, -7.0, -7.1, -7.2, -7.3, -7.4, -7.5, -7.6, -7.7, -7.8, -7.9, -8.0, -8.1, -8.2, -8.3, -8.4, -8.5, -8.6, -8.7, -8.8, -8.9, -6.9, -7.0, -7.1, -5.1, -5.2, -5.3, -5.4, -5.5, -5.6, -5.7, -5.8, -5.9, -6.0, -6.1, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -4.7, -4.8, -4.9, -5.0, -5.1, -5.2, -5.3, -5.4, -5.5, -5.6, -5.7, -5.8, -5.9, -6.0, -6.1, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8, -6.9, -7.0, -7.1, -7.2, -7.3, -7.4, -7.5, -7.6, -7.7, -7.8, -7.9, -8.0, -6.0, -6.1, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8, -6.9, -7.0, -7.1, -7.2, -7.3, -7.4, -7.5, -7.6, -7.7, -7.8, -7.9, -8.0, -8.1, -8.2, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8, -6.9, -7.0, -7.1, -7.2, -7.3, -7.4, -7.5, -7.6, -7.7, -7.8, -7.9, -8.0, -8.1, -8.2, -8.3, -8.4, -8.5, -8.6, -8.7, -8.8, -8.9, -9.0, -9.1, -9.2, -9.3, -9.4, -9.5, -9.6, -9.7, -9.8, -9.9, -10.0, -10.1, -10.2, -10.3, -10.4, -10.5, -10.6, -10.7, -10.8, -10.9, -11.0, -11.1, -11.2, -11.3, -11.4, -11.5, -11.6, -11.7, -11.8, -11.9, -12.0, -12.1, -12.2, -12.3, -12.4, -12.5, -12.6, -12.7, -12.8, -12.9, -13.0, -13.1, -11.1, -11.2, -11.3, -11.4, -11.5, -11.6, -11.7, -11.8, -11.9, -12.0, -12.1, -12.2, -12.3, -12.4, -12.5, -12.6, -12.7, -12.8, -12.9, -13.0, -13.1, -13.2, -13.3, -13.4, -13.5, -13.6, -13.7, -13.8, -13.9, -14.0, -14.1, -14.2, -14.3, -14.4, -14.5, -14.6, -14.7, -14.8, -14.9, -15.0, -15.1, -15.2, -15.3, -13.3, -13.4, -13.5, -13.6, -13.7, -13.8, -13.9, -14.0, -14.1, -14.2, -14.3, -14.4, -14.5, -14.6, -14.7, -14.8, -14.9, -15.0, -13.0, -13.1, -13.2, -13.3, -13.4, -13.5, -13.6, -13.7, -13.8, -13.9, -14.0, -14.1, -14.2, -14.3, -14.4, -12.4, -12.5, -10.5, -10.6, -10.7, -10.8, -10.9, -11.0, -11.1, -11.2, -11.3, -11.4, -11.5, -11.6, -11.7, -11.8, -11.9, -9.9, -10.0, -10.1, -10.2, -10.3, -10.4, -10.5, -10.6, -8.6, -8.7, -8.8, -8.9, -9.0, -9.1, -9.2, -9.3, -9.4, -9.5, -9.6, -9.7, -9.8, -9.9, -10.0, -10.1, -10.2, -10.3, -10.4, -8.4, -8.5, -8.6, -8.7, -8.8, -6.8, -6.9, -7.0, -7.1, -7.2, -7.3, -7.4, -7.5, -7.6, -7.7, -7.8, -7.9, -8.0, -8.1, -8.2, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8, -6.9, -4.9, -5.0, -5.1, -5.2, -5.3, -5.4, -5.5, -5.6, -5.7, -5.8, -5.9, -6.0, -6.1, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8, -6.9, -7.0, -7.1, -7.2, -7.3, -7.4, -7.5, -7.6, -7.7, -7.8, -7.9, -5.9, -6.0, -6.1, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8, -6.9, -7.0, -7.1, -7.2, -7.3, -7.4, -7.5, -7.6, -7.7, -7.8, -7.9, -8.0, -8.1, -8.2, -8.3, -8.4, -8.5, -8.6, -8.7, -8.8, -8.9, -9.0, -9.1, -9.2, -9.3, -9.4, -9.5, -7.5, -7.6, -7.7, -7.8, -7.9, -8.0, -8.1, -8.2, -8.3, -8.4, -8.5, -8.6, -8.7, -8.8, -8.9, -9.0, -9.1, -9.2, -9.3, -9.4, -9.5, -7.5, -7.6, -7.7, -7.8, -7.9, -8.0, -6.0, -6.1, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1.ai_plot_coflict_total_analysis(input_text) 실행하면, \n",
    "\n",
    "# 2.결과 나옴!(그래프 2개, 적합성, 복잡성 등등 값 도출됨)\n",
    "\n",
    "# ACTION VERBS RATIO : 8.537\n",
    "# ====================================================================\n",
    "# 에세이에 표현된 다양한 감정 수: 7\n",
    "# ====================================================================\n",
    "# 문장에 표현된 감정 비율 :  25.0\n",
    "# ====================================================================\n",
    "# ['before', 'above', 'sound', 'trail', 'by', 'way', 'until', 'city', 'from', 'to', 'after', 'against', 'forge', 'on', 'path', 'view', 'during', 'through', 'in', 'up', 'camp', 'route']\n",
    "# ====================================================================\n",
    "# SETTING RATIO :  12.34\n",
    "# ====================================================================\n",
    "# 전체 문장에서 캐릭터를 의미하는 단어나 유사어 비율 : 8.79\n",
    "# Degree of conflict  단어가 전체 문장(단어)에서 차지하는 비율 계산 : 3.0\n",
    "# 감정기복비율 : 25.0\n",
    "# 셋팅비율 계산 :  12.34\n",
    "# Plot Complxity : 26.517731803455586\n",
    "# ===============================================================================\n",
    "# ======================      Degree of Conflict   ==============================\n",
    "# ===============================================================================\n",
    "# 1명의 에세이 결과 계산점수 : (26.517731803455586, 25.0, 3.0)\n",
    "# min_ 32\n",
    "# max_:  128\n",
    "# div_: 32\n",
    "# cal_abs 절대값 : 53.48226819654441\n",
    "# compare7 : 17.75295530057593\n",
    "# compare6 : 21.30354636069112\n",
    "# compare5 : 26.629432950863897\n",
    "# compare4 : 35.50591060115186\n",
    "# compare3 : 53.258865901727795\n",
    "# Lacking: 2\n",
    "# min_ 25\n",
    "# max_:  102\n",
    "# div_: 25\n",
    "# cal_abs 절대값 : 39.0\n",
    "# compare7 : 14.833333333333334\n",
    "# compare6 : 17.8\n",
    "# compare5 : 22.25\n",
    "# compare4 : 29.666666666666668\n",
    "# compare3 : 44.5\n",
    "# Lacking: 2\n",
    "# min_ 0\n",
    "# max_:  0\n",
    "# div_: 0\n",
    "# cal_abs 절대값 : 2.686\n",
    "# compare7 : 0.5523333333333333\n",
    "# compare6 : 0.6628000000000001\n",
    "# compare5 : 0.8285\n",
    "# compare4 : 1.1046666666666667\n",
    "# compare3 : 1.657\n",
    "# Overboard: 2\n",
    "# fin_result: [(0, 1), (0, 2), (2, 1)]\n",
    "# result\n",
    "\n",
    "#  [0, 0, 2, 1.33, 5.0, 3.0, 26.52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 5804.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTION VERBS RATIO : 3.54\n",
      "====================================================================\n",
      "에세이에 표현된 다양한 감정 수: 8\n",
      "====================================================================\n",
      "문장에 표현된 감정 비율 :  28.6\n",
      "====================================================================\n",
      "['house', 'on', 'park', 'upon', 'to', 'into', 'in', 'spring', 'over']\n",
      "====================================================================\n",
      "SETTING RATIO :  9.77\n",
      "====================================================================\n",
      "전체 문장에서 캐릭터를 의미하는 단어나 유사어 비율 : 7.81\n",
      "Degree of conflict  단어가 전체 문장(단어)에서 차지하는 비율 계산 : 0.0\n",
      "감정기복비율 : 28.6\n",
      "셋팅비율 계산 :  9.77\n",
      "Plot Complxity : 33.94949659052399\n",
      "===============================================================================\n",
      "======================      Degree of Conflict   ==============================\n",
      "===============================================================================\n",
      "1명의 에세이 결과 계산점수 : (33.94949659052399, 28.6, 0.0,       neg    neu    pos  compound comp_score\n",
      "0   0.000  0.543  0.457    0.6369          1\n",
      "1   0.000  0.292  0.708    0.8051          1\n",
      "2   0.000  0.417  0.583    0.9038          1\n",
      "3   0.000  0.747  0.253    0.7269          1\n",
      "4   0.342  0.658  0.000   -0.3818         -1\n",
      "5   0.425  0.575  0.000   -0.5719         -1\n",
      "6   0.420  0.580  0.000   -0.7003         -1\n",
      "7   0.552  0.448  0.000   -0.5719         -1\n",
      "8   0.330  0.585  0.085   -0.6808         -1\n",
      "9   0.000  0.730  0.270    0.5719          1\n",
      "10  0.000  0.361  0.639    0.8910          1\n",
      "11  0.000  0.655  0.345    0.6948          1\n",
      "12  0.000  0.625  0.375    0.5574          1\n",
      "13  0.375  0.625  0.000   -0.7650         -1\n",
      "14  0.370  0.630  0.000   -0.5719         -1\n",
      "15  0.354  0.646  0.000   -0.5095         -1\n",
      "16  0.420  0.580  0.000   -0.4404         -1\n",
      "17  0.000  0.709  0.291    0.5719          1\n",
      "18  0.000  0.803  0.197    0.4005          1\n",
      "19  0.000  0.585  0.415    0.7964          1\n",
      "20  0.000  0.643  0.357    0.7269          1\n",
      "21  0.340  0.660  0.000   -0.5574         -1\n",
      "22  0.571  0.429  0.000   -0.6113         -1\n",
      "23  0.223  0.777  0.000   -0.3182         -1\n",
      "24  0.000  0.000  0.000    0.0000          1, [0.5, 1.6, 0.41, 0.97, 2.55, 2.59, 3.4, 4.21, 3.3, 2.2, 1.1, 0.0, 1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 6.89, 6.1])\n",
      "min_ 32\n",
      "max_:  128\n",
      "div_: 32\n",
      "cal_abs 절대값 : 46.05050340947601\n",
      "compare7 : 18.991582765087333\n",
      "compare6 : 22.789899318104798\n",
      "compare5 : 28.487374147630998\n",
      "compare4 : 37.98316553017467\n",
      "compare3 : 56.974748295261996\n",
      "Lacking: 2\n",
      "min_ 6\n",
      "max_:  24\n",
      "div_: 6\n",
      "cal_abs 절대값 : 13.600000000000001\n",
      "compare7 : 7.266666666666667\n",
      "compare6 : 8.72\n",
      "compare5 : 10.9\n",
      "compare4 : 14.533333333333333\n",
      "compare3 : 21.8\n",
      "Overvoard: 2\n",
      "min_ 0\n",
      "max_:  0\n",
      "div_: 0\n",
      "cal_abs 절대값 : 0.314\n",
      "compare7 : 0.052333333333333336\n",
      "compare6 : 0.0628\n",
      "compare5 : 0.0785\n",
      "compare4 : 0.10466666666666667\n",
      "compare3 : 0.157\n",
      "Lacking: 2\n",
      "fin_result: [(0, 2), (2, 3), (0, 1)]\n",
      "      neg    neu    pos  compound comp_score\n",
      "0   0.000  0.543  0.457    0.6369          1\n",
      "1   0.000  0.292  0.708    0.8051          1\n",
      "2   0.000  0.417  0.583    0.9038          1\n",
      "3   0.000  0.747  0.253    0.7269          1\n",
      "4   0.342  0.658  0.000   -0.3818         -1\n",
      "5   0.425  0.575  0.000   -0.5719         -1\n",
      "6   0.420  0.580  0.000   -0.7003         -1\n",
      "7   0.552  0.448  0.000   -0.5719         -1\n",
      "8   0.330  0.585  0.085   -0.6808         -1\n",
      "9   0.000  0.730  0.270    0.5719          1\n",
      "10  0.000  0.361  0.639    0.8910          1\n",
      "11  0.000  0.655  0.345    0.6948          1\n",
      "12  0.000  0.625  0.375    0.5574          1\n",
      "13  0.375  0.625  0.000   -0.7650         -1\n",
      "14  0.370  0.630  0.000   -0.5719         -1\n",
      "15  0.354  0.646  0.000   -0.5095         -1\n",
      "16  0.420  0.580  0.000   -0.4404         -1\n",
      "17  0.000  0.709  0.291    0.5719          1\n",
      "18  0.000  0.803  0.197    0.4005          1\n",
      "19  0.000  0.585  0.415    0.7964          1\n",
      "20  0.000  0.643  0.357    0.7269          1\n",
      "21  0.340  0.660  0.000   -0.5574         -1\n",
      "22  0.571  0.429  0.000   -0.6113         -1\n",
      "23  0.223  0.777  0.000   -0.3182         -1\n",
      "24  0.000  0.000  0.000    0.0000          1\n",
      "neg>>>>> [0.0, 0.0, 0.0, 0.0, 0.342, 0.425, 0.42, 0.552, 0.33, 0.0, 0.0, 0.0, 0.0, 0.375, 0.37, 0.354, 0.42, 0.0, 0.0, 0.0, 0.0, 0.34, 0.571, 0.223, 0.0]\n",
      "neu>>>>> [0.543, 0.292, 0.417, 0.747, 0.658, 0.575, 0.58, 0.448, 0.585, 0.73, 0.361, 0.655, 0.625, 0.625, 0.63, 0.646, 0.58, 0.709, 0.803, 0.585, 0.643, 0.66, 0.429, 0.777, 0.0]\n",
      "pos>>>>> [0.457, 0.708, 0.583, 0.253, 0.0, 0.0, 0.0, 0.0, 0.085, 0.27, 0.639, 0.345, 0.375, 0.0, 0.0, 0.0, 0.0, 0.291, 0.197, 0.415, 0.357, 0.0, 0.0, 0.0, 0.0]\n",
      "componud>>> [0.6369, 0.8051, 0.9038, 0.7269, -0.3818, -0.5719, -0.7003, -0.5719, -0.6808, 0.5719, 0.891, 0.6948, 0.5574, -0.765, -0.5719, -0.5095, -0.4404, 0.5719, 0.4005, 0.7964, 0.7269, -0.5574, -0.6113, -0.3182, 0.0]\n",
      "graph_calculation_list [0.5, 1.6, 0.41, 0.97, 2.55, 2.59, 3.4, 4.21, 3.3, 2.2, 1.1, 0.0, 1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 6.89, 6.1]\n",
      "RESULT>>>>>>>>>>>>> {'result_all_plot': 2.0, 'emotional_rollercoaster': 28.6, 'plot_complexity': 33.95, 'degree_conflict': 0.0, 'result_plot_complexity': 0, 'result_emotional_rollercoaster': 2, 'result_degree_conflict': 0, 'neg': [0.0, 0.0, 0.0, 0.0, 0.342, 0.425, 0.42, 0.552, 0.33, 0.0, 0.0, 0.0, 0.0, 0.375, 0.37, 0.354, 0.42, 0.0, 0.0, 0.0, 0.0, 0.34, 0.571, 0.223, 0.0], 'neu': [0.543, 0.292, 0.417, 0.747, 0.658, 0.575, 0.58, 0.448, 0.585, 0.73, 0.361, 0.655, 0.625, 0.625, 0.63, 0.646, 0.58, 0.709, 0.803, 0.585, 0.643, 0.66, 0.429, 0.777, 0.0], 'pos': [0.457, 0.708, 0.583, 0.253, 0.0, 0.0, 0.0, 0.0, 0.085, 0.27, 0.639, 0.345, 0.375, 0.0, 0.0, 0.0, 0.0, 0.291, 0.197, 0.415, 0.357, 0.0, 0.0, 0.0, 0.0], 'compound': [0.6369, 0.8051, 0.9038, 0.7269, -0.3818, -0.5719, -0.7003, -0.5719, -0.6808, 0.5719, 0.891, 0.6948, 0.5574, -0.765, -0.5719, -0.5095, -0.4404, 0.5719, 0.4005, 0.7964, 0.7269, -0.5574, -0.6113, -0.3182, 0.0], 'graph_calculation_list': [0.5, 1.6, 0.41, 0.97, 2.55, 2.59, 3.4, 4.21, 3.3, 2.2, 1.1, 0.0, 1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 6.89, 6.1]}\n"
     ]
    }
   ],
   "source": [
    "result__ = ai_plot_coflict_total_analysis(input_text)\n",
    "print(\"RESULT>>>>>>>>>>>>>\", result__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
