{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "continuous-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# 토픽 수\n",
    "K=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "loving-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [[\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabulous-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 토픽이 각 문서에 할당되는 횟수\n",
    "# Counter로 구성된 리스트\n",
    "# 각 Counter는 각 문서를 의미\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "\n",
    "# 각 단어가 각 토픽에 할당되는 횟수\n",
    "# Counter로 구성된 리스트\n",
    "# 각 Counter는 각 토픽을 의미\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "# 각 토픽에 할당되는 총 단어수\n",
    "# 숫자로 구성된 리스트\n",
    "# 각각의 숫자는 각 토픽을 의미함\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "# 각 문서에 포함되는 총 단어수\n",
    "# 숫자로 구성된 리스트\n",
    "# 각각의 숫자는 각 문서를 의미함\n",
    "document_lengths = list(map(len, documents))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "classified-option",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 5, 6, 5, 4, 6, 4, 4, 4, 4, 3, 4, 3, 5, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "welcome-restaurant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 단어 종류의 수\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)\n",
    "W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "federal-sword",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 총 문서의 수\n",
    "D = len(documents)\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acquired-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_weight(d, word, topic):\n",
    "    \"\"\"given a document and a word in that document,\n",
    "    return the weight for the kth topic\"\"\"\n",
    "    \n",
    "    def p_topic_given_document(topic, d, alpha=0.1):\n",
    "        \"\"\"the fraction of words in document _d_\n",
    "        that are assigned to _topic_ (plus some smoothing)\"\"\"\n",
    "        return ((document_topic_counts[d][topic] + alpha) / (document_lengths[d] + K * alpha))\n",
    "\n",
    "    def p_word_given_topic(word, topic, beta=0.1):\n",
    "        \"\"\"the fraction of words assigned to _topic_\n",
    "        that equal _word_ (plus some smoothing)\"\"\"\n",
    "        return ((topic_word_counts[topic][word] + beta) / (topic_counts[topic] + W * beta))\n",
    "    \n",
    "    \n",
    "    return p_word_given_topic(word, topic) * p_topic_given_document(topic, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "modern-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_new_topic(d, word):\n",
    "    \n",
    "    def sample_from(weights):\n",
    "        \"\"\"returns i with probability weights[i] / sum(weights)\"\"\"\n",
    "        total = sum(weights)\n",
    "        rnd = total * random.random() # uniform between 0 and total\n",
    "        for i, p in enumerate(weights):\n",
    "            rnd -= p # return the smallest i such that\n",
    "            if rnd <= 0: \n",
    "                return i # weights[0] + ... + weights[i] >= rnd\n",
    "        \n",
    "    return sample_from([topic_weight(d, word, topic) for topic in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adjustable-metro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 0, 1, 3, 3, 3, 1],\n",
       " [1, 0, 2, 1, 3],\n",
       " [2, 1, 3, 1, 1, 1],\n",
       " [2, 2, 1, 0, 2],\n",
       " [1, 3, 0, 2],\n",
       " [1, 3, 3, 1, 2, 1],\n",
       " [0, 1, 1, 1],\n",
       " [1, 2, 3, 0],\n",
       " [1, 3, 1, 2],\n",
       " [0, 3, 0, 1],\n",
       " [3, 2, 0],\n",
       " [3, 0, 3, 1],\n",
       " [3, 0, 3],\n",
       " [2, 0, 1, 1, 0],\n",
       " [0, 0, 2]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics = [[random.randrange(K) for word in document] for document in documents]\n",
    "document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "strategic-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1 \n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "governing-patient",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({2: 1, 0: 1, 1: 2, 3: 3}),\n",
       " Counter({1: 2, 0: 1, 2: 1, 3: 1}),\n",
       " Counter({2: 1, 1: 4, 3: 1}),\n",
       " Counter({2: 3, 1: 1, 0: 1}),\n",
       " Counter({1: 1, 3: 1, 0: 1, 2: 1}),\n",
       " Counter({1: 3, 3: 2, 2: 1}),\n",
       " Counter({0: 1, 1: 3}),\n",
       " Counter({1: 1, 2: 1, 3: 1, 0: 1}),\n",
       " Counter({1: 2, 3: 1, 2: 1}),\n",
       " Counter({0: 2, 3: 1, 1: 1}),\n",
       " Counter({3: 1, 2: 1, 0: 1}),\n",
       " Counter({3: 2, 0: 1, 1: 1}),\n",
       " Counter({3: 2, 0: 1}),\n",
       " Counter({2: 1, 0: 2, 1: 2}),\n",
       " Counter({0: 2, 2: 1})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acute-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1000): # repetition\n",
    "    for d in range(D): # each documnet\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],document_topics[d])):\n",
    "            \n",
    "            # gibbs sampling: 특정 하나의 topic assignment z를 제거하고 나머지들(-z)의 조건부 확률  \n",
    "            \n",
    "            # remove this word / topic from the counts\n",
    "            # so that it doesn't influence the weights\n",
    "            document_topic_counts[d][topic] -= 1 # 문서별 토픽 갯수\n",
    "            topic_word_counts[topic][word] -= 1 # 토픽별 단어 갯수\n",
    "            topic_counts[topic] -= 1 # 토픽별 카운트\n",
    "            document_lengths[d] -= 1 # 문서별 단어갯수\n",
    "            \n",
    "            # choose a new topic based on the weights\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "            \n",
    "            # and now add it back to the counts\n",
    "            document_topic_counts[d][new_topic] += 1 # 문서별 토픽 갯수\n",
    "            topic_word_counts[new_topic][word] += 1 # 토픽별 단어 갯수\n",
    "            topic_counts[new_topic] += 1 # 토픽별 카운트\n",
    "            document_lengths[d] += 1 # 문서별 단어갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "material-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Topic1','Topic2','Topic3','Topic4'], index=['Top'+str(i) for i in range(1,6)])\n",
    "\n",
    "for k, word_counts in enumerate(topic_word_counts):\n",
    "    for ix, (word, count) in enumerate(word_counts.most_common(5)): # 각 토픽별로 top 10 단어\n",
    "            df.loc['Top'+str(ix+1),'Topic'+str(k+1)] = word+'({})'.format(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "excessive-luther",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Topic1               Topic2        Topic3  \\\n",
      "Top1          MongoDB(2)            Python(4)       Java(3)   \n",
      "Top2            HBase(2)      scikit-learn(2)     Hadoop(2)   \n",
      "Top3         Postgres(2)       statsmodels(2)   Big Data(2)   \n",
      "Top4  neural networks(1)            pandas(2)  Cassandra(1)   \n",
      "Top5    deep learning(1)  machine learning(2)      Spark(1)   \n",
      "\n",
      "                          Topic4  \n",
      "Top1               regression(3)  \n",
      "Top2                        R(3)  \n",
      "Top3               statistics(3)  \n",
      "Top4              probability(3)  \n",
      "Top5  artificial intelligence(2)  \n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
