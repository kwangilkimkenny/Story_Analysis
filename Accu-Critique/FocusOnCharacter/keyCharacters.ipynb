{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아나콘다 가상환경 office:  py37TF2\n",
    "# home : py37Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import multiprocessing\n",
    "import io\n",
    "from gensim.models import Phrases\n",
    "from textblob import TextBlob\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character(text):\n",
    "\n",
    "    essay_input_corpus = str(text) #문장입력\n",
    "    essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "    sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "    total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "    total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "    \n",
    "    split_sentences = []\n",
    "    for sentence in sentences:\n",
    "        processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "        words = processed.split()\n",
    "        split_sentences.append(words)\n",
    "\n",
    "    skip_gram = 1\n",
    "    workers = multiprocessing.cpu_count()\n",
    "    bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "    model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "    model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "    \n",
    "    #모델 설계 완료\n",
    "\n",
    "    #캐릭터 표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "    character_list = ['i', 'my', 'me', 'mine', 'you', 'your', 'they','them',\n",
    "                      'yours', 'he','him','his' 'she','her','it','someone','their', 'myself', 'aunt',\n",
    "                    'brother','cousin','daughter','father','grandchild','granddaughter','granddson','grandfather',\n",
    "                    'grandmother', 'person','great-grandchild','husband','ex-husband','son-in-law', 'daughter-in-law','mother',\n",
    "                    'niece','nephew','parents','sister','son','stepfather','stepmother','stepdaughter', 'stepson',\n",
    "                    'twin','uncle','widow','widower','wife','ex-wife']\n",
    "    \n",
    "    ####문장에 char_list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "    #우선 토큰화한다.\n",
    "    retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "    token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "    #print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "    #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "    filtered_chr_text = []\n",
    "    for k in token_input_text:\n",
    "        for j in character_list:\n",
    "            if k == j:\n",
    "                filtered_chr_text.append(j)\n",
    "    \n",
    "    #print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "    \n",
    "    filtered_chr_text_ = set(filtered_chr_text) #중복제거\n",
    "    filtered_chr_text__ = list(filtered_chr_text_) #다시 리스트로 변환\n",
    "    #print (filtered_chr_text__) # 중복값 제거 확인\n",
    "    \n",
    "    for i in filtered_chr_text__:\n",
    "        ext_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "    \n",
    "    char_total_count = len(filtered_chr_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 캐릭터 표현 수\n",
    "    char_count_ = len(filtered_chr_text__) #중복제거된 캐릭터 표현 총 수\n",
    "        \n",
    "    result_char_ratio = round(char_total_count/total_words * 100, 2)\n",
    "\n",
    "    return result_char_ratio, total_sentences, total_words, char_total_count, char_count_, ext_sim_words_key\n",
    "    #return result_char_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. person-name-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_registered_name' from 'tensorflow.python.keras.utils.generic_utils' (/Users/kimkwangil/opt/anaconda3/envs/py37TF2/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8bce1a5d27f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#모델 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model-person-name-recognition.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37TF2/lib/python3.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37TF2/lib/python3.7/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/py37TF2/lib/python3.7/site-packages/tensorflow/python/keras/api/_v1/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_custom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_registered_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_registered_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_keras_serializable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_registered_name' from 'tensorflow.python.keras.utils.generic_utils' (/Users/kimkwangil/opt/anaconda3/envs/py37TF2/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py)"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#모델 불러오기\n",
    "model = load_model('model-person-name-recognition.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting raw input names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_inputs.txt', 'r') as f: # office mac에 데이터저장되어 있음\n",
    "    raw_text = f.read()\n",
    "    raw_inputs = raw_text.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dictionary of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2id = dict((c,i) for (i, c) in enumerate(sorted(set(raw_text.replace('\\n', ' '))),1))\n",
    "id2char = dict(enumerate(sorted(set(raw_text.replace('\\n', ' '))),1))\n",
    "char2id['<PAD>'] = 0\n",
    "id2char[0] = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = ['Non person name', 'Person Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pad_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-51b28eb6180b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pad_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "# 이름 인식은 잘 됨, 단 모델을 학습시키고, 데이터베이스가 있어야 함, 이름의 첫 대문자도 구분함\n",
    "\n",
    "test_string = 'Richard '\n",
    "\n",
    "test_predict = np.expand_dims([char2id[char] for char in test_string],0)\n",
    "test_predict = pad_sequences(test_predict, maxlen=40, padding='post')\n",
    "\n",
    "output = model.predict(test_predict)\n",
    "print(test_string,'is a',label_dict[np.argmax(output)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.개체명인식으로 Named Entity Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'JJ'), ('authorities', 'NNS'), ('fined', 'VBD'), ('Google', 'NNP'), ('a', 'DT'), ('record', 'NN'), ('$', '$'), ('5.1', 'CD'), ('billion', 'CD'), ('on', 'IN'), ('Wednesday', 'NNP'), ('for', 'IN'), ('abusing', 'VBG'), ('its', 'PRP$'), ('power', 'NN'), ('in', 'IN'), ('the', 'DT'), ('mobile', 'JJ'), ('phone', 'NN'), ('market', 'NN'), ('and', 'CC'), ('ordered', 'VBD'), ('the', 'DT'), ('company', 'NN'), ('to', 'TO'), ('alter', 'VB'), ('its', 'PRP$'), ('practices', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"European authorities fined Google a record $5.1 billion on Wednesday for abusing its power \\\n",
    "            in the mobile phone market and ordered the company to alter its practices\"\n",
    "sentence_pos = pos_tag(word_tokenize(sentence))\n",
    "print(sentence_pos) # 토큰화와 품사 태깅을 동시 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.spaCy NER\n",
    "\n",
    "참고 :  https://realpython.com/natural-language-processing-spacy-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Bloomington Normal is almost laughably cliché for a midwestern city. Vast swathes of corn envelop winding roads and the heady smell of BBQ smoke pervades the countryside every summer. Yet, underlying the trite norms of Normal is the prescriptive force of tradition—the expectation to fulfill my role as a female Filipino by playing Debussy in the yearly piano festival and enrolling in multivariable calculus instead of political philosophy.So when I discovered the technical demand of bebop, the triplet groove, and the intricacies of chordal harmony after ten years of grueling classical piano, I was fascinated by the music's novelty. Jazz guitar was not only evocative and creative, but also strangely liberating. I began to explore different pedagogical methods, transcribe solos from the greats, and experiment with various approaches until my own unique sound began to develop. And, although I did not know what would be the 'best' route for me to follow as a musician, the freedom to forge whatever path I felt was right seemed to be exactly what I needed; there were no expectations for me to continue in any particular way—only the way that suited my own desires.While journeying this trail, I found myself at Interlochen Arts Camp the summer before my junior year. Never before had I been immersed in an environment so conducive to musical growth: I was surrounded by people intensely passionate about pursuing all kinds of art with no regard for ideas of what art 'should' be. I knew immediately that this would be a perfect opportunity to cultivate my sound, unbounded by the limits of confining tradition. On the first day of camp, I found that my peer guitarist in big band was another Filipino girl from Illinois. Until that moment, my endeavors in jazz guitar had been a solitary effort; I had no one with whom to collaborate and no one against whom I could compare myself, much less someone from a background mirroring my own. I was eager to play with her, but while I quickly recognized a slew of differences between us—different heights, guitars, and even playing styles—others seemed to have trouble making that distinction during performances. Some even went as far as calling me 'other-Francesca.' Thus, amidst the glittering lakes and musky pine needles of Interlochen, I once again confronted Bloomington's frustrating expectations.After being mistaken for her several times, I could not help but view Francesca as a standard of what the 'female Filipino jazz guitarist' should embody. Her improvisatory language, comping style and even personal qualities loomed above me as something I had to live up to. Nevertheless, as Francesca and I continued to play together, it was not long before we connected through our creative pursuit. In time, I learned to draw inspiration from her instead of feeling pressured to follow whatever precedent I thought she set. I found that I grew because of, rather than in spite of, her presence; I could find solace in our similarities and even a sense of comfort in an unfamiliar environment without being trapped by expectation. Though the pressure to conform was still present—and will likely remain present in my life no matter what genre I'm playing or what pursuits I engage in—I learned to eschew its corrosive influence and enjoy the rewards that it brings. While my encounter with Francesca at first sparked a feeling of pressure to conform in a setting where I never thought I would feel its presence, it also carried the warmth of finding someone with whom I could connect. Like the admittedly trite conditions of my hometown, the resemblances between us provided comfort to me through their familiarity. I ultimately found that I can embrace this warmth while still rejecting the pressure to succumb to expectations, and that, in the careful balance between these elements, I can grow in a way that feels both like discove\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bloomington normal is almost laughably cliché for a midwestern city. vast swathes of corn envelop winding roads and the heady smell of bbq smoke pervades the countryside every summer. yet, underlying the trite norms of normal is the prescriptive force of tradition—the expectation to fulfill my role as a female filipino by playing debussy in the yearly piano festival and enrolling in multivariable calculus instead of political philosophy.so when i discovered the technical demand of bebop, the triplet groove, and the intricacies of chordal harmony after ten years of grueling classical piano, i was fascinated by the music's novelty. jazz guitar was not only evocative and creative, but also strangely liberating. i began to explore different pedagogical methods, transcribe solos from the greats, and experiment with various approaches until my own unique sound began to develop. and, although i did not know what would be the 'best' route for me to follow as a musician, the freedom to forge whatever path i felt was right seemed to be exactly what i needed; there were no expectations for me to continue in any particular way—only the way that suited my own desires.while journeying this trail, i found myself at interlochen arts camp the summer before my junior year. never before had i been immersed in an environment so conducive to musical growth: i was surrounded by people intensely passionate about pursuing all kinds of art with no regard for ideas of what art 'should' be. i knew immediately that this would be a perfect opportunity to cultivate my sound, unbounded by the limits of confining tradition. on the first day of camp, i found that my peer guitarist in big band was another filipino girl from illinois. until that moment, my endeavors in jazz guitar had been a solitary effort; i had no one with whom to collaborate and no one against whom i could compare myself, much less someone from a background mirroring my own. i was eager to play with her, but while i quickly recognized a slew of differences between us—different heights, guitars, and even playing styles—others seemed to have trouble making that distinction during performances. some even went as far as calling me 'other-francesca.' thus, amidst the glittering lakes and musky pine needles of interlochen, i once again confronted bloomington's frustrating expectations.after being mistaken for her several times, i could not help but view francesca as a standard of what the 'female filipino jazz guitarist' should embody. her improvisatory language, comping style and even personal qualities loomed above me as something i had to live up to. nevertheless, as francesca and i continued to play together, it was not long before we connected through our creative pursuit. in time, i learned to draw inspiration from her instead of feeling pressured to follow whatever precedent i thought she set. i found that i grew because of, rather than in spite of, her presence; i could find solace in our similarities and even a sense of comfort in an unfamiliar environment without being trapped by expectation. though the pressure to conform was still present—and will likely remain present in my life no matter what genre i'm playing or what pursuits i engage in—i learned to eschew its corrosive influence and enjoy the rewards that it brings. while my encounter with francesca at first sparked a feeling of pressure to conform in a setting where i never thought i would feel its presence, it also carried the warmth of finding someone with whom i could connect. like the admittedly trite conditions of my hometown, the resemblances between us provided comfort to me through their familiarity. i ultimately found that i can embrace this warmth while still rejecting the pressure to succumb to expectations, and that, in the careful balance between these elements, i can grow in a way that feels both like discove\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#소문자로 변환\n",
    "input_lower_text = input_text.lower()\n",
    "input_lower_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "about_doc = nlp(input_lower_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = {}\n",
    "for token in about_doc:\n",
    "    #print (token, token.idx)\n",
    "    token_list.setdefault(token, token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{bloomington: 0,\n",
       " normal: 12,\n",
       " is: 19,\n",
       " almost: 22,\n",
       " laughably: 29,\n",
       " cliché: 39,\n",
       " for: 46,\n",
       " a: 50,\n",
       " midwestern: 52,\n",
       " city: 63,\n",
       " .: 67,\n",
       " vast: 69,\n",
       " swathes: 74,\n",
       " of: 82,\n",
       " corn: 85,\n",
       " envelop: 90,\n",
       " winding: 98,\n",
       " roads: 106,\n",
       " and: 112,\n",
       " the: 116,\n",
       " heady: 120,\n",
       " smell: 126,\n",
       " of: 132,\n",
       " bbq: 135,\n",
       " smoke: 139,\n",
       " pervades: 145,\n",
       " the: 154,\n",
       " countryside: 158,\n",
       " every: 170,\n",
       " summer: 176,\n",
       " .: 182,\n",
       " yet: 184,\n",
       " ,: 187,\n",
       " underlying: 189,\n",
       " the: 200,\n",
       " trite: 204,\n",
       " norms: 210,\n",
       " of: 216,\n",
       " normal: 219,\n",
       " is: 226,\n",
       " the: 229,\n",
       " prescriptive: 233,\n",
       " force: 246,\n",
       " of: 252,\n",
       " tradition: 255,\n",
       " —: 264,\n",
       " the: 265,\n",
       " expectation: 269,\n",
       " to: 281,\n",
       " fulfill: 284,\n",
       " my: 292,\n",
       " role: 295,\n",
       " as: 300,\n",
       " a: 303,\n",
       " female: 305,\n",
       " filipino: 312,\n",
       " by: 321,\n",
       " playing: 324,\n",
       " debussy: 332,\n",
       " in: 340,\n",
       " the: 343,\n",
       " yearly: 347,\n",
       " piano: 354,\n",
       " festival: 360,\n",
       " and: 369,\n",
       " enrolling: 373,\n",
       " in: 383,\n",
       " multivariable: 386,\n",
       " calculus: 400,\n",
       " instead: 409,\n",
       " of: 417,\n",
       " political: 420,\n",
       " philosophy.so: 430,\n",
       " when: 444,\n",
       " i: 449,\n",
       " discovered: 451,\n",
       " the: 462,\n",
       " technical: 466,\n",
       " demand: 476,\n",
       " of: 483,\n",
       " bebop: 486,\n",
       " ,: 491,\n",
       " the: 493,\n",
       " triplet: 497,\n",
       " groove: 505,\n",
       " ,: 511,\n",
       " and: 513,\n",
       " the: 517,\n",
       " intricacies: 521,\n",
       " of: 533,\n",
       " chordal: 536,\n",
       " harmony: 544,\n",
       " after: 552,\n",
       " ten: 558,\n",
       " years: 562,\n",
       " of: 568,\n",
       " grueling: 571,\n",
       " classical: 580,\n",
       " piano: 590,\n",
       " ,: 595,\n",
       " i: 597,\n",
       " was: 599,\n",
       " fascinated: 603,\n",
       " by: 614,\n",
       " the: 617,\n",
       " music: 621,\n",
       " 's: 626,\n",
       " novelty: 629,\n",
       " .: 636,\n",
       " jazz: 638,\n",
       " guitar: 643,\n",
       " was: 650,\n",
       " not: 654,\n",
       " only: 658,\n",
       " evocative: 663,\n",
       " and: 673,\n",
       " creative: 677,\n",
       " ,: 685,\n",
       " but: 687,\n",
       " also: 691,\n",
       " strangely: 696,\n",
       " liberating: 706,\n",
       " .: 716,\n",
       " i: 718,\n",
       " began: 720,\n",
       " to: 726,\n",
       " explore: 729,\n",
       " different: 737,\n",
       " pedagogical: 747,\n",
       " methods: 759,\n",
       " ,: 766,\n",
       " transcribe: 768,\n",
       " solos: 779,\n",
       " from: 785,\n",
       " the: 790,\n",
       " greats: 794,\n",
       " ,: 800,\n",
       " and: 802,\n",
       " experiment: 806,\n",
       " with: 817,\n",
       " various: 822,\n",
       " approaches: 830,\n",
       " until: 841,\n",
       " my: 847,\n",
       " own: 850,\n",
       " unique: 854,\n",
       " sound: 861,\n",
       " began: 867,\n",
       " to: 873,\n",
       " develop: 876,\n",
       " .: 883,\n",
       " and: 885,\n",
       " ,: 888,\n",
       " although: 890,\n",
       " i: 899,\n",
       " did: 901,\n",
       " not: 905,\n",
       " know: 909,\n",
       " what: 914,\n",
       " would: 919,\n",
       " be: 925,\n",
       " the: 928,\n",
       " ': 932,\n",
       " best: 933,\n",
       " ': 937,\n",
       " route: 939,\n",
       " for: 945,\n",
       " me: 949,\n",
       " to: 952,\n",
       " follow: 955,\n",
       " as: 962,\n",
       " a: 965,\n",
       " musician: 967,\n",
       " ,: 975,\n",
       " the: 977,\n",
       " freedom: 981,\n",
       " to: 989,\n",
       " forge: 992,\n",
       " whatever: 998,\n",
       " path: 1007,\n",
       " i: 1012,\n",
       " felt: 1014,\n",
       " was: 1019,\n",
       " right: 1023,\n",
       " seemed: 1029,\n",
       " to: 1036,\n",
       " be: 1039,\n",
       " exactly: 1042,\n",
       " what: 1050,\n",
       " i: 1055,\n",
       " needed: 1057,\n",
       " ;: 1063,\n",
       " there: 1065,\n",
       " were: 1071,\n",
       " no: 1076,\n",
       " expectations: 1079,\n",
       " for: 1092,\n",
       " me: 1096,\n",
       " to: 1099,\n",
       " continue: 1102,\n",
       " in: 1111,\n",
       " any: 1114,\n",
       " particular: 1118,\n",
       " way: 1129,\n",
       " —: 1132,\n",
       " only: 1133,\n",
       " the: 1138,\n",
       " way: 1142,\n",
       " that: 1146,\n",
       " suited: 1151,\n",
       " my: 1158,\n",
       " own: 1161,\n",
       " desires.while: 1165,\n",
       " journeying: 1179,\n",
       " this: 1190,\n",
       " trail: 1195,\n",
       " ,: 1200,\n",
       " i: 1202,\n",
       " found: 1204,\n",
       " myself: 1210,\n",
       " at: 1217,\n",
       " interlochen: 1220,\n",
       " arts: 1232,\n",
       " camp: 1237,\n",
       " the: 1242,\n",
       " summer: 1246,\n",
       " before: 1253,\n",
       " my: 1260,\n",
       " junior: 1263,\n",
       " year: 1270,\n",
       " .: 1274,\n",
       " never: 1276,\n",
       " before: 1282,\n",
       " had: 1289,\n",
       " i: 1293,\n",
       " been: 1295,\n",
       " immersed: 1300,\n",
       " in: 1309,\n",
       " an: 1312,\n",
       " environment: 1315,\n",
       " so: 1327,\n",
       " conducive: 1330,\n",
       " to: 1340,\n",
       " musical: 1343,\n",
       " growth: 1351,\n",
       " :: 1357,\n",
       " i: 1359,\n",
       " was: 1361,\n",
       " surrounded: 1365,\n",
       " by: 1376,\n",
       " people: 1379,\n",
       " intensely: 1386,\n",
       " passionate: 1396,\n",
       " about: 1407,\n",
       " pursuing: 1413,\n",
       " all: 1422,\n",
       " kinds: 1426,\n",
       " of: 1432,\n",
       " art: 1435,\n",
       " with: 1439,\n",
       " no: 1444,\n",
       " regard: 1447,\n",
       " for: 1454,\n",
       " ideas: 1458,\n",
       " of: 1464,\n",
       " what: 1467,\n",
       " art: 1472,\n",
       " ': 1476,\n",
       " should: 1477,\n",
       " ': 1483,\n",
       " be: 1485,\n",
       " .: 1487,\n",
       " i: 1489,\n",
       " knew: 1491,\n",
       " immediately: 1496,\n",
       " that: 1508,\n",
       " this: 1513,\n",
       " would: 1518,\n",
       " be: 1524,\n",
       " a: 1527,\n",
       " perfect: 1529,\n",
       " opportunity: 1537,\n",
       " to: 1549,\n",
       " cultivate: 1552,\n",
       " my: 1562,\n",
       " sound: 1565,\n",
       " ,: 1570,\n",
       " unbounded: 1572,\n",
       " by: 1582,\n",
       " the: 1585,\n",
       " limits: 1589,\n",
       " of: 1596,\n",
       " confining: 1599,\n",
       " tradition: 1609,\n",
       " .: 1618,\n",
       " on: 1620,\n",
       " the: 1623,\n",
       " first: 1627,\n",
       " day: 1633,\n",
       " of: 1637,\n",
       " camp: 1640,\n",
       " ,: 1644,\n",
       " i: 1646,\n",
       " found: 1648,\n",
       " that: 1654,\n",
       " my: 1659,\n",
       " peer: 1662,\n",
       " guitarist: 1667,\n",
       " in: 1677,\n",
       " big: 1680,\n",
       " band: 1684,\n",
       " was: 1689,\n",
       " another: 1693,\n",
       " filipino: 1701,\n",
       " girl: 1710,\n",
       " from: 1715,\n",
       " illinois: 1720,\n",
       " .: 1728,\n",
       " until: 1730,\n",
       " that: 1736,\n",
       " moment: 1741,\n",
       " ,: 1747,\n",
       " my: 1749,\n",
       " endeavors: 1752,\n",
       " in: 1762,\n",
       " jazz: 1765,\n",
       " guitar: 1770,\n",
       " had: 1777,\n",
       " been: 1781,\n",
       " a: 1786,\n",
       " solitary: 1788,\n",
       " effort: 1797,\n",
       " ;: 1803,\n",
       " i: 1805,\n",
       " had: 1807,\n",
       " no: 1811,\n",
       " one: 1814,\n",
       " with: 1818,\n",
       " whom: 1823,\n",
       " to: 1828,\n",
       " collaborate: 1831,\n",
       " and: 1843,\n",
       " no: 1847,\n",
       " one: 1850,\n",
       " against: 1854,\n",
       " whom: 1862,\n",
       " i: 1867,\n",
       " could: 1869,\n",
       " compare: 1875,\n",
       " myself: 1883,\n",
       " ,: 1889,\n",
       " much: 1891,\n",
       " less: 1896,\n",
       " someone: 1901,\n",
       " from: 1909,\n",
       " a: 1914,\n",
       " background: 1916,\n",
       " mirroring: 1927,\n",
       " my: 1937,\n",
       " own: 1940,\n",
       " .: 1943,\n",
       " i: 1945,\n",
       " was: 1947,\n",
       " eager: 1951,\n",
       " to: 1957,\n",
       " play: 1960,\n",
       " with: 1965,\n",
       " her: 1970,\n",
       " ,: 1973,\n",
       " but: 1975,\n",
       " while: 1979,\n",
       " i: 1985,\n",
       " quickly: 1987,\n",
       " recognized: 1995,\n",
       " a: 2006,\n",
       " slew: 2008,\n",
       " of: 2013,\n",
       " differences: 2016,\n",
       " between: 2028,\n",
       " us: 2036,\n",
       " —: 2038,\n",
       " different: 2039,\n",
       " heights: 2049,\n",
       " ,: 2056,\n",
       " guitars: 2058,\n",
       " ,: 2065,\n",
       " and: 2067,\n",
       " even: 2071,\n",
       " playing: 2076,\n",
       " styles: 2084,\n",
       " —: 2090,\n",
       " others: 2091,\n",
       " seemed: 2098,\n",
       " to: 2105,\n",
       " have: 2108,\n",
       " trouble: 2113,\n",
       " making: 2121,\n",
       " that: 2128,\n",
       " distinction: 2133,\n",
       " during: 2145,\n",
       " performances: 2152,\n",
       " .: 2164,\n",
       " some: 2166,\n",
       " even: 2171,\n",
       " went: 2176,\n",
       " as: 2181,\n",
       " far: 2184,\n",
       " as: 2188,\n",
       " calling: 2191,\n",
       " me: 2199,\n",
       " ': 2202,\n",
       " other: 2203,\n",
       " -: 2208,\n",
       " francesca: 2209,\n",
       " .: 2218,\n",
       " ': 2219,\n",
       " thus: 2221,\n",
       " ,: 2225,\n",
       " amidst: 2227,\n",
       " the: 2234,\n",
       " glittering: 2238,\n",
       " lakes: 2249,\n",
       " and: 2255,\n",
       " musky: 2259,\n",
       " pine: 2265,\n",
       " needles: 2270,\n",
       " of: 2278,\n",
       " interlochen: 2281,\n",
       " ,: 2292,\n",
       " i: 2294,\n",
       " once: 2296,\n",
       " again: 2301,\n",
       " confronted: 2307,\n",
       " bloomington: 2318,\n",
       " 's: 2329,\n",
       " frustrating: 2332,\n",
       " expectations.after: 2344,\n",
       " being: 2363,\n",
       " mistaken: 2369,\n",
       " for: 2378,\n",
       " her: 2382,\n",
       " several: 2386,\n",
       " times: 2394,\n",
       " ,: 2399,\n",
       " i: 2401,\n",
       " could: 2403,\n",
       " not: 2409,\n",
       " help: 2413,\n",
       " but: 2418,\n",
       " view: 2422,\n",
       " francesca: 2427,\n",
       " as: 2437,\n",
       " a: 2440,\n",
       " standard: 2442,\n",
       " of: 2451,\n",
       " what: 2454,\n",
       " the: 2459,\n",
       " ': 2463,\n",
       " female: 2464,\n",
       " filipino: 2471,\n",
       " jazz: 2480,\n",
       " guitarist: 2485,\n",
       " ': 2494,\n",
       " should: 2496,\n",
       " embody: 2503,\n",
       " .: 2509,\n",
       " her: 2511,\n",
       " improvisatory: 2515,\n",
       " language: 2529,\n",
       " ,: 2537,\n",
       " comping: 2539,\n",
       " style: 2547,\n",
       " and: 2553,\n",
       " even: 2557,\n",
       " personal: 2562,\n",
       " qualities: 2571,\n",
       " loomed: 2581,\n",
       " above: 2588,\n",
       " me: 2594,\n",
       " as: 2597,\n",
       " something: 2600,\n",
       " i: 2610,\n",
       " had: 2612,\n",
       " to: 2616,\n",
       " live: 2619,\n",
       " up: 2624,\n",
       " to: 2627,\n",
       " .: 2629,\n",
       " nevertheless: 2631,\n",
       " ,: 2643,\n",
       " as: 2645,\n",
       " francesca: 2648,\n",
       " and: 2658,\n",
       " i: 2662,\n",
       " continued: 2664,\n",
       " to: 2674,\n",
       " play: 2677,\n",
       " together: 2682,\n",
       " ,: 2690,\n",
       " it: 2692,\n",
       " was: 2695,\n",
       " not: 2699,\n",
       " long: 2703,\n",
       " before: 2708,\n",
       " we: 2715,\n",
       " connected: 2718,\n",
       " through: 2728,\n",
       " our: 2736,\n",
       " creative: 2740,\n",
       " pursuit: 2749,\n",
       " .: 2756,\n",
       " in: 2758,\n",
       " time: 2761,\n",
       " ,: 2765,\n",
       " i: 2767,\n",
       " learned: 2769,\n",
       " to: 2777,\n",
       " draw: 2780,\n",
       " inspiration: 2785,\n",
       " from: 2797,\n",
       " her: 2802,\n",
       " instead: 2806,\n",
       " of: 2814,\n",
       " feeling: 2817,\n",
       " pressured: 2825,\n",
       " to: 2835,\n",
       " follow: 2838,\n",
       " whatever: 2845,\n",
       " precedent: 2854,\n",
       " i: 2864,\n",
       " thought: 2866,\n",
       " she: 2874,\n",
       " set: 2878,\n",
       " .: 2881,\n",
       " i: 2883,\n",
       " found: 2885,\n",
       " that: 2891,\n",
       " i: 2896,\n",
       " grew: 2898,\n",
       " because: 2903,\n",
       " of: 2911,\n",
       " ,: 2913,\n",
       " rather: 2915,\n",
       " than: 2922,\n",
       " in: 2927,\n",
       " spite: 2930,\n",
       " of: 2936,\n",
       " ,: 2938,\n",
       " her: 2940,\n",
       " presence: 2944,\n",
       " ;: 2952,\n",
       " i: 2954,\n",
       " could: 2956,\n",
       " find: 2962,\n",
       " solace: 2967,\n",
       " in: 2974,\n",
       " our: 2977,\n",
       " similarities: 2981,\n",
       " and: 2994,\n",
       " even: 2998,\n",
       " a: 3003,\n",
       " sense: 3005,\n",
       " of: 3011,\n",
       " comfort: 3014,\n",
       " in: 3022,\n",
       " an: 3025,\n",
       " unfamiliar: 3028,\n",
       " environment: 3039,\n",
       " without: 3051,\n",
       " being: 3059,\n",
       " trapped: 3065,\n",
       " by: 3073,\n",
       " expectation: 3076,\n",
       " .: 3087,\n",
       " though: 3089,\n",
       " the: 3096,\n",
       " pressure: 3100,\n",
       " to: 3109,\n",
       " conform: 3112,\n",
       " was: 3120,\n",
       " still: 3124,\n",
       " present: 3130,\n",
       " —: 3137,\n",
       " and: 3138,\n",
       " will: 3142,\n",
       " likely: 3147,\n",
       " remain: 3154,\n",
       " present: 3161,\n",
       " in: 3169,\n",
       " my: 3172,\n",
       " life: 3175,\n",
       " no: 3180,\n",
       " matter: 3183,\n",
       " what: 3190,\n",
       " genre: 3195,\n",
       " i: 3201,\n",
       " 'm: 3202,\n",
       " playing: 3205,\n",
       " or: 3213,\n",
       " what: 3216,\n",
       " pursuits: 3221,\n",
       " i: 3230,\n",
       " engage: 3232,\n",
       " in: 3239,\n",
       " —: 3241,\n",
       " i: 3242,\n",
       " learned: 3244,\n",
       " to: 3252,\n",
       " eschew: 3255,\n",
       " its: 3262,\n",
       " corrosive: 3266,\n",
       " influence: 3276,\n",
       " and: 3286,\n",
       " enjoy: 3290,\n",
       " the: 3296,\n",
       " rewards: 3300,\n",
       " that: 3308,\n",
       " it: 3313,\n",
       " brings: 3316,\n",
       " .: 3322,\n",
       " while: 3324,\n",
       " my: 3330,\n",
       " encounter: 3333,\n",
       " with: 3343,\n",
       " francesca: 3348,\n",
       " at: 3358,\n",
       " first: 3361,\n",
       " sparked: 3367,\n",
       " a: 3375,\n",
       " feeling: 3377,\n",
       " of: 3385,\n",
       " pressure: 3388,\n",
       " to: 3397,\n",
       " conform: 3400,\n",
       " in: 3408,\n",
       " a: 3411,\n",
       " setting: 3413,\n",
       " where: 3421,\n",
       " i: 3427,\n",
       " never: 3429,\n",
       " thought: 3435,\n",
       " i: 3443,\n",
       " would: 3445,\n",
       " feel: 3451,\n",
       " its: 3456,\n",
       " presence: 3460,\n",
       " ,: 3468,\n",
       " it: 3470,\n",
       " also: 3473,\n",
       " carried: 3478,\n",
       " the: 3486,\n",
       " warmth: 3490,\n",
       " of: 3497,\n",
       " finding: 3500,\n",
       " someone: 3508,\n",
       " with: 3516,\n",
       " whom: 3521,\n",
       " i: 3526,\n",
       " could: 3528,\n",
       " connect: 3534,\n",
       " .: 3541,\n",
       " like: 3543,\n",
       " the: 3548,\n",
       " admittedly: 3552,\n",
       " trite: 3563,\n",
       " conditions: 3569,\n",
       " of: 3580,\n",
       " my: 3583,\n",
       " hometown: 3586,\n",
       " ,: 3594,\n",
       " the: 3596,\n",
       " resemblances: 3600,\n",
       " between: 3613,\n",
       " us: 3621,\n",
       " provided: 3624,\n",
       " comfort: 3633,\n",
       " to: 3641,\n",
       " me: 3644,\n",
       " through: 3647,\n",
       " their: 3655,\n",
       " familiarity: 3661,\n",
       " .: 3672,\n",
       " i: 3674,\n",
       " ultimately: 3676,\n",
       " found: 3687,\n",
       " that: 3693,\n",
       " i: 3698,\n",
       " can: 3700,\n",
       " embrace: 3704,\n",
       " this: 3712,\n",
       " warmth: 3717,\n",
       " while: 3724,\n",
       " still: 3730,\n",
       " rejecting: 3736,\n",
       " the: 3746,\n",
       " pressure: 3750,\n",
       " to: 3759,\n",
       " succumb: 3762,\n",
       " to: 3770,\n",
       " expectations: 3773,\n",
       " ,: 3785,\n",
       " and: 3787,\n",
       " that: 3791,\n",
       " ,: 3795,\n",
       " in: 3797,\n",
       " the: 3800,\n",
       " careful: 3804,\n",
       " balance: 3812,\n",
       " between: 3820,\n",
       " these: 3828,\n",
       " elements: 3834,\n",
       " ,: 3842,\n",
       " i: 3844,\n",
       " can: 3846,\n",
       " grow: 3850,\n",
       " in: 3855,\n",
       " a: 3858,\n",
       " way: 3860,\n",
       " that: 3864,\n",
       " feels: 3869,\n",
       " both: 3875,\n",
       " like: 3880,\n",
       " discove: 3885}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#값 확인\n",
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token_list.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li_doc = list(token_list.keys())\n",
    "type(li_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#캐릭터 표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "i_character_list = ['i', 'my', 'me', 'mine']\n",
    "#하나씩 꺼내서 유사한 단어를 찾아내서 새로운 리스트에 담아서 출력,\n",
    "ext_i_characters = []\n",
    "for i_itm in i_character_list:\n",
    "    for k_ in li_doc:\n",
    "        if i_itm == str(k_):\n",
    "            ext_i_characters.append(i_itm)\n",
    "#I 관련 캐릭터 표현하는 단어들의 총 개수        \n",
    "len(ext_i_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'me',\n",
       " 'me',\n",
       " 'me',\n",
       " 'me',\n",
       " 'me']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#확인해보기\n",
    "ext_i_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#캐릭터 표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "you_character_list = ['you', 'your', \n",
    "                  'yours', 'he','him','his' 'she','her']\n",
    "#하나씩 꺼내서 유사한 단어를 찾아내서 새로운 리스트에 담아서 출력,\n",
    "ext_you_characters = []\n",
    "for i_itm in you_character_list:\n",
    "    for k_ in li_doc:\n",
    "        if i_itm == str(k_):\n",
    "            ext_you_characters.append(i_itm)\n",
    "#I 관련 캐릭터 표현하는 단어들의 총 개수        \n",
    "len(ext_you_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['her', 'her', 'her', 'her', 'her']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#확인해보기\n",
    "ext_you_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0, 12, 19, 22, 29, 39, 46, 50, 52, 63, 67, 69, 74, 82, 85, 90, 98, 106, 112, 116, 120, 126, 132, 135, 139, 145, 154, 158, 170, 176, 182, 184, 187, 189, 200, 204, 210, 216, 219, 226, 229, 233, 246, 252, 255, 264, 265, 269, 281, 284, 292, 295, 300, 303, 305, 312, 321, 324, 332, 340, 343, 347, 354, 360, 369, 373, 383, 386, 400, 409, 417, 420, 430, 440, 441, 444, 449, 451, 462, 466, 476, 483, 486, 491, 493, 497, 505, 511, 513, 517, 521, 533, 536, 544, 552, 558, 562, 568, 571, 580, 590, 595, 597, 599, 603, 614, 617, 621, 626, 629, 636, 638, 643, 650, 654, 658, 663, 673, 677, 685, 687, 691, 696, 706, 716, 718, 720, 726, 729, 737, 747, 759, 766, 768, 779, 785, 790, 794, 800, 802, 806, 817, 822, 830, 841, 847, 850, 854, 861, 867, 873, 876, 883, 885, 888, 890, 899, 901, 905, 909, 914, 919, 925, 928, 932, 933, 937, 939, 945, 949, 952, 955, 962, 965, 967, 975, 977, 981, 989, 992, 998, 1007, 1012, 1014, 1019, 1023, 1029, 1036, 1039, 1042, 1050, 1055, 1057, 1063, 1065, 1071, 1076, 1079, 1092, 1096, 1099, 1102, 1111, 1114, 1118, 1129, 1132, 1133, 1138, 1142, 1146, 1151, 1158, 1161, 1165, 1172, 1173, 1179, 1190, 1195, 1200, 1202, 1204, 1210, 1217, 1220, 1232, 1237, 1242, 1246, 1253, 1260, 1263, 1270, 1274, 1276, 1282, 1289, 1293, 1295, 1300, 1309, 1312, 1315, 1327, 1330, 1340, 1343, 1351, 1357, 1359, 1361, 1365, 1376, 1379, 1386, 1396, 1407, 1413, 1422, 1426, 1432, 1435, 1439, 1444, 1447, 1454, 1458, 1464, 1467, 1472, 1476, 1477, 1483, 1485, 1487, 1489, 1491, 1496, 1508, 1513, 1518, 1524, 1527, 1529, 1537, 1549, 1552, 1562, 1565, 1570, 1572, 1582, 1585, 1589, 1596, 1599, 1609, 1618, 1620, 1623, 1627, 1633, 1637, 1640, 1644, 1646, 1648, 1654, 1659, 1662, 1667, 1677, 1680, 1684, 1689, 1693, 1701, 1710, 1715, 1720, 1728, 1730, 1736, 1741, 1747, 1749, 1752, 1762, 1765, 1770, 1777, 1781, 1786, 1788, 1797, 1803, 1805, 1807, 1811, 1814, 1818, 1823, 1828, 1831, 1843, 1847, 1850, 1854, 1862, 1867, 1869, 1875, 1883, 1889, 1891, 1896, 1901, 1909, 1914, 1916, 1927, 1937, 1940, 1943, 1945, 1947, 1951, 1957, 1960, 1965, 1970, 1973, 1975, 1979, 1985, 1987, 1995, 2006, 2008, 2013, 2016, 2028, 2036, 2038, 2039, 2049, 2056, 2058, 2065, 2067, 2071, 2076, 2084, 2090, 2091, 2098, 2105, 2108, 2113, 2121, 2128, 2133, 2145, 2152, 2164, 2166, 2171, 2176, 2181, 2184, 2188, 2191, 2199, 2202, 2203, 2208, 2209, 2218, 2219, 2221, 2225, 2227, 2234, 2238, 2249, 2255, 2259, 2265, 2270, 2278, 2281, 2292, 2294, 2296, 2301, 2307, 2318, 2329, 2332, 2344, 2356, 2357, 2363, 2369, 2378, 2382, 2386, 2394, 2399, 2401, 2403, 2409, 2413, 2418, 2422, 2427, 2437, 2440, 2442, 2451, 2454, 2459, 2463, 2464, 2471, 2480, 2485, 2494, 2496, 2503, 2509, 2511, 2515, 2529, 2537, 2539, 2547, 2553, 2557, 2562, 2571, 2581, 2588, 2594, 2597, 2600, 2610, 2612, 2616, 2619, 2624, 2627, 2629, 2631, 2643, 2645, 2648, 2658, 2662, 2664, 2674, 2677, 2682, 2690, 2692, 2695, 2699, 2703, 2708, 2715, 2718, 2728, 2736, 2740, 2749, 2756, 2758, 2761, 2765, 2767, 2769, 2777, 2780, 2785, 2797, 2802, 2806, 2814, 2817, 2825, 2835, 2838, 2845, 2854, 2864, 2866, 2874, 2878, 2881, 2883, 2885, 2891, 2896, 2898, 2903, 2911, 2913, 2915, 2922, 2927, 2930, 2936, 2938, 2940, 2944, 2952, 2954, 2956, 2962, 2967, 2974, 2977, 2981, 2994, 2998, 3003, 3005, 3011, 3014, 3022, 3025, 3028, 3039, 3051, 3059, 3065, 3073, 3076, 3087, 3089, 3096, 3100, 3109, 3112, 3120, 3124, 3130, 3137, 3138, 3142, 3147, 3154, 3161, 3169, 3172, 3175, 3180, 3183, 3190, 3195, 3201, 3202, 3205, 3213, 3216, 3221, 3230, 3232, 3239, 3241, 3242, 3244, 3252, 3255, 3262, 3266, 3276, 3286, 3290, 3296, 3300, 3308, 3313, 3316, 3322, 3324, 3330, 3333, 3343, 3348, 3358, 3361, 3367, 3375, 3377, 3385, 3388, 3397, 3400, 3408, 3411, 3413, 3421, 3427, 3429, 3435, 3443, 3445, 3451, 3456, 3460, 3468, 3470, 3473, 3478, 3486, 3490, 3497, 3500, 3508, 3516, 3521, 3526, 3528, 3534, 3541, 3543, 3548, 3552, 3563, 3569, 3580, 3583, 3586, 3594, 3596, 3600, 3613, 3621, 3624, 3633, 3641, 3644, 3647, 3655, 3661, 3672, 3674, 3676, 3687, 3693, 3698, 3700, 3704, 3712, 3717, 3724, 3730, 3736, 3746, 3750, 3759, 3762, 3770, 3773, 3785, 3787, 3791, 3795, 3797, 3800, 3804, 3812, 3820, 3828, 3834, 3842, 3844, 3846, 3850, 3855, 3858, 3860, 3864, 3869, 3875, 3880, 3885])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_persons(text):\n",
    "     # Create Doc object\n",
    "     doc2 = nlp(text)\n",
    "\n",
    "     # Identify the persons\n",
    "     persons = [ent.text for ent in doc2.ents if ent.label_ == 'PERSON']\n",
    "\n",
    "     # Return persons\n",
    "     return persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_I(text):\n",
    "     # Create Doc object\n",
    "     doc2 = nlp(text)\n",
    "\n",
    "     # Identify the persons\n",
    "     persons = [ent.text for ent in doc2.ents if ent.label_ == 'PERSON']\n",
    "\n",
    "     # Return persons\n",
    "     return persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Debussy', 'Francesca', 'Francesca', 'Francesca']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사람이 검출된다.\n",
    "dtc_person = find_persons(input_text)\n",
    "dtc_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 패턴으로 특정 단어를 추출해보자\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "terms = [\"i\", \"you\"]\n",
    "# Only run nlp.make_doc to speed things up\n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"i\", patterns)\n",
    "\n",
    "doc = nlp(input_text)\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloomington Normal 0 18 ORG\n",
      "every summer 170 182 DATE\n",
      "Filipino 312 320 LANGUAGE\n",
      "Debussy 332 339 PERSON\n",
      "yearly 347 353 DATE\n",
      "ten years 558 567 DATE\n",
      "the first day 1623 1636 DATE\n",
      "Filipino 1701 1709 NORP\n",
      "Illinois 1720 1728 GPE\n",
      "Interlochen 2281 2292 GPE\n",
      "Bloomington 2318 2329 GPE\n",
      "Francesca 2427 2436 PERSON\n",
      "Filipino 2471 2479 NORP\n",
      "Francesca 2648 2657 PERSON\n",
      "Francesca 3348 3357 PERSON\n",
      "first 3361 3366 ORDINAL\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(input_text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Bloomington Normal is almost laughably cliché for a midwestern city. Vast swathes of corn envelop winding roads and the heady smell of BBQ smoke pervades the countryside every summer. Yet, underlying the trite norms of Normal is the prescriptive force of tradition—the expectation to fulfill my role as a female Filipino by playing Debussy in the yearly piano festival and enrolling in multivariable calculus instead of political philosophy.So when I discovered the technical demand of bebop, the triplet groove, and the intricacies of chordal harmony after ten years of grueling classical piano, I was fascinated by the music's novelty. Jazz guitar was not only evocative and creative, but also strangely liberating. I began to explore different pedagogical methods, transcribe solos from the greats, and experiment with various approaches until my own unique sound began to develop. And, although I did not know what would be the 'best' route for me to follow as a musician, the freedom to forge whatever path I felt was right seemed to be exactly what I needed; there were no expectations for me to continue in any particular way—only the way that suited my own desires.While journeying this trail, I found myself at Interlochen Arts Camp the summer before my junior year. Never before had I been immersed in an environment so conducive to musical growth: I was surrounded by people intensely passionate about pursuing all kinds of art with no regard for ideas of what art 'should' be. I knew immediately that this would be a perfect opportunity to cultivate my sound, unbounded by the limits of confining tradition. On the first day of camp, I found that my peer guitarist in big band was another Filipino girl from Illinois. Until that moment, my endeavors in jazz guitar had been a solitary effort; I had no one with whom to collaborate and no one against whom I could compare myself, much less someone from a background mirroring my own. I was eager to play with her, but while I quickly recognized a slew of differences between us—different heights, guitars, and even playing styles—others seemed to have trouble making that distinction during performances. Some even went as far as calling me 'other-Francesca.' Thus, amidst the glittering lakes and musky pine needles of Interlochen, I once again confronted Bloomington's frustrating expectations.After being mistaken for her several times, I could not help but view Francesca as a standard of what the 'female Filipino jazz guitarist' should embody. Her improvisatory language, comping style and even personal qualities loomed above me as something I had to live up to. Nevertheless, as Francesca and I continued to play together, it was not long before we connected through our creative pursuit. In time, I learned to draw inspiration from her instead of feeling pressured to follow whatever precedent I thought she set. I found that I grew because of, rather than in spite of, her presence; I could find solace in our similarities and even a sense of comfort in an unfamiliar environment without being trapped by expectation. Though the pressure to conform was still present—and will likely remain present in my life no matter what genre I'm playing or what pursuits I engage in—I learned to eschew its corrosive influence and enjoy the rewards that it brings. While my encounter with Francesca at first sparked a feeling of pressure to conform in a setting where I never thought I would feel its presence, it also carried the warmth of finding someone with whom I could connect. Like the admittedly trite conditions of my hometown, the resemblances between us provided comfort to me through their familiarity. I ultimately found that I can embrace this warmth while still rejecting the pressure to succumb to expectations, and that, in the careful balance between these elements, I can grow in a way that feels both like discove\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문장에서 캐릭터를 의미하는 단어나 유사어 비율 : (8.79, 21, 705, 62, 8, [('also', 0.9799678325653076), ('feel', 0.9774074554443359), ('together', 0.9741417765617371), ('rewards', 0.9704742431640625), ('carried', 0.9699242115020752), ('play', 0.9693466424942017), ('continued', 0.9683886766433716), ('long', 0.9680884480476379), ('its', 0.9672611355781555), ('enjoy', 0.9666262269020081)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:53: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n"
     ]
    }
   ],
   "source": [
    "character_ratio_result = character(input_text)\n",
    "character_ratio_result\n",
    "print(\"전체 문장에서 캐릭터를 의미하는 단어나 유사어 비율 :\", character_ratio_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
