{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Virtual Env : office : py37pytorch\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# 글속에 감정이 얼마나 표현되어 있는지 분석 - origin (Bert pre trained model 활용)\n",
    "from transformers import BertTokenizer\n",
    "from model import BertForMultiLabelClassification\n",
    "from multilabel_pipeline import MultiLabelPipeline\n",
    "import multiprocessing\n",
    "import os\n",
    "from pathlib import Path\n",
    "import io\n",
    "from gensim.models import Phrases\n",
    "from textblob import TextBlob\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START  ###\n",
    "def theme_all_section(input_text, question_num):\n",
    "\n",
    "\n",
    "        # with open('personal_statement_980_fin.json','r') as json_file :\n",
    "        #     json_data = json.load(json_file)\n",
    "\n",
    "\n",
    "\n",
    "        # # 1000명의 학생 데이터를 추출\n",
    "        # st_data_txt = json.dumps(json_data)\n",
    "\n",
    "        # #데이터 확인완료\n",
    "        ##print(st_data_txt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ######################\n",
    "        ##### QUESTION 1 #####\n",
    "        ######################\n",
    "        #표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "        qst_one_words_list = ['identity', 'background', 'interest', 'talent', 'meaningful','belief', 'explore', 'develop',\n",
    "                            'realize', 'unique', 'passion', 'different', 'culture', 'sex', 'gender', 'religion', \n",
    "                            'profession', 'major', 'ethnic', 'disability', 'excel', 'standout', 'diversity',\n",
    "                            'acculturation','alone','arouse','backdrop','background','belief','break','concern','cultural',\n",
    "                            'culture','develop','different','disability','diverseness','diversity','endowment','ethnic','evolve',\n",
    "                            'excel','explicate','explore','gain','gender','grow','heat','heathen','identity','impression','interest',\n",
    "                            'love','major','mania','meaningful','modernize','originate','passion','pastime','polish','profession',\n",
    "                            'rage','realize','recognize','religion','research','sake','setting','sex','singular','talent','train',\n",
    "                            'understand','unique','unlike','identity', 'diversity', 'inclinations', 'passion', 'culture', 'unique',\n",
    "                            'qualities', 'life story', 'values', 'experience', 'lessons', 'family', 'home', 'talent', 'personal history']   \n",
    "\n",
    "\n",
    "\n",
    "        ######################\n",
    "        ##### QUESTION 2 #####\n",
    "        ######################\n",
    "        #표현하는 단어들을 리스트에 넣어서 필터로 만들고, WORDNET에서 유사단어 추출하여 적용완료!\n",
    "        qst_two_words_list = ['obstacle', 'challenge', 'setback', 'failure', 'difficulty', 'despair', 'defeat', 'hindrance', 'impediment', \n",
    "                                'misfortune', 'trouble', 'handicap', 'stumble', 'hurdle','bankruptcy',\n",
    "                                'challenge', 'defeat', 'despair', 'difficulty', 'disability', 'disable', 'disturb', 'failure', 'frustration',\n",
    "                                'fuss', 'handicap', 'hindrance', 'hurdle', 'kill', 'lurch', 'misfortune', 'obstacle', 'obstruction', 'perturb',\n",
    "                                'reverse', 'stumble', 'trip', 'trouble', 'vault', 'worry','obstacle', 'hardship', 'challenge', 'failure', 'lessons',\n",
    "                                'values', 'triumph', 'rebound', 'courage', 'initiative', 'attitude', 'improvement', 'development']\n",
    "\n",
    "\n",
    "\n",
    "        ######################\n",
    "        ##### QUESTION 3 #####\n",
    "        ######################\n",
    "        #표현하는 단어들을 리스트에 넣어서 필터로 만들고, WORDNET에서 유사단어 추출하여 적용완료!\n",
    "        qst_three_words_list = ['idea', 'belief', 'question', 'thinking', 'prompted', 'outcome', 'challenge', 'defy', 'realize', \n",
    "                                'enlighten', 'philosophy', 'religion', 'conviction', 'believe', 'thoughts', 'reason', 'logic', 'value', \n",
    "                                'conscience', 'ethic', 'right', 'justice', 'dare', 'concept', 'existing', 'inspire', 'confront', 'oppose', \n",
    "                                'conflict', 'against','argue', 'battle', 'belief', 'believe', 'cause', 'challenge', 'cheer', 'clear', 'concept', 'conflict',\n",
    "                                'confront', 'conscience', 'consequence', 'conviction', 'correct', 'correctly', 'dare', 'defy', 'dispute', 'doctrine',\n",
    "                                'doubt', 'enlighten', 'estimate', 'ethic', 'exist', 'existent', 'existing', 'fight', 'gain', 'good', 'idea', 'impression',\n",
    "                                'inhale', 'inspire', 'intelligent', 'intend', 'interrogate', 'interview', 'judge', 'justice', 'justly', 'logic', 'measure',\n",
    "                                'mighty', 'mind', 'motion','motivate', 'opinion', 'oppose', 'philosophy', 'pit', 'prize', 'prompt', 'proper', 'properly',\n",
    "                                'question', 'r', 'rate', 'rationality', 'react', 'realize', 'reason', 'recognize', 'religion', 'remember', 'respect', 'result',\n",
    "                                'revolutionize', 'right', 'theme', 'think', 'thinking', 'thought', 'understand', 'value', 'veracious', 'wonder',\n",
    "                                'critical thinking', 'courage', 'challenging spirit', 'self-reflection', 'intellect', 'action', 'change', 'respect', 'realization',\n",
    "                                'improvement', 'curiosity', 'leadership', 'fight']\n",
    "\n",
    "\n",
    "\n",
    "        ######################\n",
    "        ##### QUESTION 4 #####\n",
    "        ######################\n",
    "        #표현하는 단어들을 리스트에 넣어서 필터로 만들고, WORDNET에서 유사단어 추출하여 적용완료!\n",
    "        qst_four_words_list = ['problem', 'solve', 'problem-solving', 'problem solving', 'intellectual', 'research','ethical dilemma', 'personal',\n",
    "                                'significance', 'solution', 'identify','challenge', 'question', 'dilemma', 'dispute', 'answer', 'clarify', \n",
    "                                'figure out', 'work out', 'fix', 'conclude', 'realize', 'discover','answer',\n",
    "                                'cerebral', 'challenge', 'clarify', 'clear', 'conclude', 'cook', 'detect', 'dilemma', 'discover', 'dispute',\n",
    "                                'doubt', 'fasten', 'fix', 'fixate', 'gain', 'identify', 'inquiry', 'intellectual', 'interrogate', 'interview',\n",
    "                                'learn', 'localization', 'meaning', 'motion', 'name', 'personal', 'problem', 'quarrel', 'question', 'realize',\n",
    "                                'reason', 'recognize', 'repair', 'research', 'resolve', 'significance', 'situate', 'solution', 'solve', 'specify',\n",
    "                                'sterilize', 'suffice', 'trouble', 'understand', 'unwrap', 'wonder', 'gratitude', 'altruism', 'hero', 'heroine',\n",
    "                                'philanthropy', 'caring', 'preconception', 'realization', 'maturity', 'sacrifice', 'reward', 'common good', 'hardship', 'virtue']\n",
    "\n",
    "\n",
    "\n",
    "        ######################\n",
    "        ##### QUESTION 5 #####\n",
    "        ######################\n",
    "        #표현하는 단어들을 리스트에 넣어서 필터로 만들고, WORDNET에서 유사단어 추출하여 적용완료!\n",
    "        qst_five_words_list = ['accomplishment', 'event', 'realization', 'spark', 'growth', 'understanding', 'myself', 'others'\n",
    "                                'realization', 'realize', 'accomplish', 'event', 'incident', 'happening', 'understanding', 'insight', 'insightful', 'mature', 'maturity',\n",
    "                                'growth', 'enlightenment', 'enlighten', 'perspective', 'empathize', 'empathy', 'sympathize', 'sympathy', 'appreciate',\n",
    "                                'acknowledge', 'respect', 'humble','accomplishment', 'achieve', 'acknowledge', 'admit', 'adulthood', 'agreement', 'appreciate',\n",
    "                                'base', 'clear', 'commiserate', 'consequence', 'deference', 'discharge', 'emergence', 'empathy', 'enlighten', 'enlightenment', 'esteem',\n",
    "                                'event', 'find', 'fledged', 'flicker', 'gain', 'growth', 'happen', 'happening', 'humble', 'humiliate', 'incident', 'incidental', 'increase',\n",
    "                                'insight', 'insightful', 'mature', 'maturity', 'nirvana', 'notice', 'obedience', 'penetration', 'perspective', 'position', 'prize', 'realization',\n",
    "                                'realize', 'reason', 'recognize', 'regard', 'respect', 'ripe', 'ripen', 'senesce', 'skill', 'spark', 'sparkle', 'suppurate', 'sympathize',\n",
    "                                'sympathy', 'trip', 'understand', 'understanding', 'incident', 'initiative', 'accomplishment', 'maturity', 'perspective change', 'realization',\n",
    "                                'life story', 'personal history', 'triumph', 'community', 'team', 'people']\n",
    "\n",
    "\n",
    "\n",
    "        ######################\n",
    "        ##### QUESTION 6 #####\n",
    "        ######################\n",
    "        #표현하는 단어들을 리스트에 넣어서 필터로 만들고, WORDNET에서 유사단어 추출하여 적용완료!\n",
    "        qst_six_words_list = ['topic', 'idea', 'concept', 'engaging', 'captivate', 'learn'\n",
    "                            'learn', 'research', 'subject', 'mentor', 'teacher', 'professor', 'inspiration', 'study', 'fascinate', 'engross', 'discover', 'find',\n",
    "                            'theory', 'thought', 'think', 'mesmerize', 'delve', 'inquiry', 'inquire', 'question', 'inquisitive', 'investigate',\n",
    "                            'explore', 'absorb', 'analyze', 'ask', 'betroth', 'capable', 'capture', 'cogitation', 'concept', 'detect', 'determine',\n",
    "                            'dig', 'discipline','discover', 'discovery', 'doubt', 'engage', 'engaging', 'estimate', 'explore', 'fascinate', 'find', 'hire', 'hypnotize',\n",
    "                            'hypothesis', 'idea', 'identify', 'inhalation', 'inquiry', 'inquisitive', 'inspiration', 'intend', 'interrogate', 'interview', 'intrigue',\n",
    "                            'investigate', 'learn', 'lease', 'magnetize', 'mentor', 'mind', 'motion', 'national', 'opinion', 'professor', 'prosecute', 'question',\n",
    "                            'receive', 'recover', 'remember', 'report', 'research', 'rule', 'sketch', 'steep', 'study', 'subject', 'subjugate', 'submit', 'survey',\n",
    "                            'teacher', 'theme', 'theory', 'think', 'thinking', 'thought', 'topic', 'unwrap', 'witness', 'wonder', 'curiosity', 'intellectual', 'social science', \n",
    "                            'STEM', 'humanities', 'ideology', 'question', 'research', 'think', 'logic', 'reason', 'depth', 'academic', 'intriguing', 'goal', 'plan']\n",
    "\n",
    "\n",
    "        ######################\n",
    "        ##### QUESTION 6 #####\n",
    "        ######################\n",
    "        #표현하는 단어들을 리스트에 넣어서 필터로 만들고, WORDNET에서 유사단어 추출하여 적용완료!\n",
    "\n",
    "        qst_seven_words_list =  qst_one_words_list +  qst_two_words_list + qst_three_words_list + qst_four_words_list + qst_five_words_list + qst_six_words_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ####  문항을 선택하고 에세이를 입력했을 경우. 선택문항관련 단어리스트와 입력한 에세이의 공통적인 연관어 추출  ####\n",
    "\n",
    "\n",
    "        #text : 입력 에세이\n",
    "        #question_num_list : 선택한 질문과 연관된 단어 리스트\n",
    "\n",
    "        def sim_words_quesiton(text_input, question_num_list):\n",
    "\n",
    "            essay_input_corpus = str(text_input) #문장입력\n",
    "            essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "            sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "            total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "            total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "\n",
    "            split_sentences = []\n",
    "            for sentence in sentences:\n",
    "                processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "                words = processed.split()\n",
    "                split_sentences.append(words)\n",
    "\n",
    "            skip_gram = 1\n",
    "            workers = multiprocessing.cpu_count()\n",
    "            bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "            model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "            model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "\n",
    "            #모델 설계 완료\n",
    "\n",
    "            ######################\n",
    "            ##### QUESTION 1 ~6 ## 관련 단어는 다른 함수에서 처리하여 적용할 것!!  문항 1~6번을 선택했을 경우 이하 코드 계산(이것은 클래스로 선언)\n",
    "            ######################\n",
    "                \n",
    "            ####문장에 list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "\n",
    "            #우선 토큰화한다.\n",
    "            retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "            token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "            ##print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "            #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "            filtered_chr_text = []\n",
    "            for k in token_input_text:\n",
    "                for j in question_num_list:\n",
    "                    if k == j:\n",
    "                        filtered_chr_text.append(j)\n",
    "\n",
    "            ##print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "\n",
    "            filtered_chr_text_ = set(filtered_chr_text) #중복제거\n",
    "            filtered_chr_text__ = list(filtered_chr_text_) #다시 리스트로 변환\n",
    "            ##print (filtered_chr_text__) # 중복값 제거 확인\n",
    "\n",
    "            # for i in filtered_chr_text__:\n",
    "            #     ext_sim_words_key = model.most_similar_cosmul(i,topn=50) #모델적용\n",
    "\n",
    "            # char_total_count = len(filtered_chr_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 표현 수\n",
    "            # char_count_ = len(filtered_chr_text__) #중복제거된  표현 총 수\n",
    "\n",
    "            # result_char_ratio = round(char_total_count/total_words * 100, 2)\n",
    "\n",
    "            # import pandas as pd\n",
    "\n",
    "            # df_conf_words = pd.DataFrame(ext_sim_words_key, columns=['words','values']) #데이터프레임으로 변환\n",
    "            # df_r = df_conf_words['words'] #words 컬럼 값 추출\n",
    "            # ext_sim_words_key = df_r.values.tolist() # 유사단어 추출\n",
    "\n",
    "\n",
    "            ext_sim_words_key = filtered_chr_text_ \n",
    "            #return result_char_ratio, total_sentences, total_words, char_total_count, char_count_, ext_sim_words_key\n",
    "            return ext_sim_words_key\n",
    "\n",
    "\n",
    "\n",
    "        ########## 선택한 질문에 의해 해당하는 코드가 실행되는 부분  ###########\n",
    "\n",
    "        if 'ques_one' == question_num: #선택한 질문이 ques_one 이면\n",
    "\n",
    "            result_ques_ = sim_words_quesiton(input_text, qst_one_words_list) #입력한 에세이에 관하여 관련단어를 추출을 시작하라\n",
    "            print(\"질문 1에 해당하는 1명 데이터 관련어 :\", result_ques_)\n",
    "            \n",
    "            # load\n",
    "            with open('question_one_1000_dataset.pickle', 'rb') as f:\n",
    "                result_most_simWords = pickle.load(f)\n",
    "            print(\"1000명 관련 data loaded :\", result_most_simWords)\n",
    "\n",
    "        elif 'ques_two' == question_num:\n",
    "\n",
    "            result_ques_ = sim_words_quesiton(input_text, qst_two_words_list)\n",
    "            print(\"result_ques_two :\", result_ques_)\n",
    "            \n",
    "            # load\n",
    "            with open('question_two_1000_dataset.pickle', 'rb') as f:\n",
    "                result_most_simWords = pickle.load(f)\n",
    "            print(\"1000명 관련 data loaded :\", result_most_simWords)\n",
    "\n",
    "        elif 'ques_three' == question_num:\n",
    "\n",
    "            result_ques_ = sim_words_quesiton(input_text, qst_three_words_list)\n",
    "            print(\"result_ques_three :\", result_ques_)\n",
    "            \n",
    "            # load\n",
    "            with open('question_three_1000_dataset.pickle', 'rb') as f:\n",
    "                result_most_simWords = pickle.load(f)\n",
    "            print(\"1000명 관련 data loaded :\", result_most_simWords)\n",
    "\n",
    "\n",
    "        elif 'ques_four' == question_num:\n",
    "\n",
    "            result_ques_ = sim_words_quesiton(input_text, qst_four_words_list)\n",
    "            print(\"result_ques_four :\", result_ques_)\n",
    "            \n",
    "            # load\n",
    "            with open('question_four_1000_dataset.pickle', 'rb') as f:\n",
    "                result_most_simWords = pickle.load(f)\n",
    "            print(\"1000명 관련 data loaded :\", result_most_simWords)\n",
    "            \n",
    "        elif 'ques_five' == question_num:\n",
    "\n",
    "            result_ques_ = sim_words_quesiton(input_text, qst_five_words_list)\n",
    "            print(\"result_ques_five :\", result_ques_)\n",
    "            \n",
    "            # load\n",
    "            with open('question_five_1000_dataset.pickle', 'rb') as f:\n",
    "                result_most_simWords = pickle.load(f)\n",
    "            print(\"1000명 관련 data loaded :\", result_most_simWords)\n",
    "            \n",
    "        elif 'ques_six' == question_num:\n",
    "\n",
    "            result_ques_ = sim_words_quesiton(input_text, qst_six_words_list)\n",
    "            print(\"result_ques_six :\", result_ques_)\n",
    "            \n",
    "            # load\n",
    "            with open('question_six_1000_dataset.pickle', 'rb') as f:\n",
    "                result_most_simWords = pickle.load(f)\n",
    "            print(\"1000명 관련 data loaded :\", result_most_simWords)    \n",
    "\n",
    "        elif 'ques_seven' == question_num:\n",
    "            print(\"let me think...\")\n",
    "            result_ques_ = sim_words_quesiton(input_text, qst_seven_words_list)\n",
    "           \n",
    "            # load\n",
    "            with open('question_seven_1000_dataset.pickle', 'rb') as f:\n",
    "                result_most_simWords = pickle.load(f)\n",
    "            print(\"1000명 관련 data loaded :\", result_most_simWords)\n",
    "\n",
    "        else :\n",
    "            pass\n",
    "\n",
    "\n",
    "        #############################################################################################################\n",
    "\n",
    "        ##### 1번 문항에 해당하는 1000명의 학생 에세이 분석결과 ####\n",
    "        # 유사단어를 문장에서 추출하여 반환한다.\n",
    "        # st_data_txt >> 1000명의 에세이이다.\n",
    "        # qst_one_words_list >>> 1번재 질문을 선택했을 경우다.\n",
    "\n",
    "        #que_no_one_sim_words_ratio_result = sim_words_quesiton(st_data_txt, qst_one_words_list)\n",
    "\n",
    "        # 위 결과(학생에세이분석결과)를 하나씩 꺼내서 1000명에세이 분석결과와 Doc2Vec로 개별 비교한다.\n",
    "        # 분석하기 위하여 입력데이터 전처리  예 ) ['학생데이터리스트중 1개', '나머지는 1000명의 데이터리스트']\n",
    "        # 6번 처리해야 하리때문에 함수로 변환적용할것!!!!\n",
    "        import numpy as np\n",
    "\n",
    "        #분석데이터 합치기\n",
    "        input_data_preprocessed = []\n",
    "        for std_keyword in result_ques_:# 위 결과(학생에세이분석결과)를 하나씩 꺼내서 \n",
    "            input_data_preprocessed.append(std_keyword) #리스트에 첫 단어를 담고, 나머지 리스트데이터는 1000명것을 붙여넣는다.\n",
    "            for item_ in result_most_simWords:\n",
    "                input_data_preprocessed.append(item_) #리스트 합치기\n",
    "            #input_data_preprocessed.append('.') #리스트를 구분한다. '.'로 구분\n",
    "\n",
    "        #input_data_preprocessed #분석데이터 합친 결과 리스트, 이 리스트 데이터를 구간별(학생1단어, 1000개 단어가 1set)로 나누어서  DOC2VEC를 적용해보자 한번에 싹 처리해부러~\n",
    "\n",
    "        #질문에 대한 1명의 학생에세이 분석결과\n",
    "        ps_documents_df=pd.DataFrame(result_ques_, columns=['documents_cleaned'])\n",
    "\n",
    "\n",
    "        def most_similar(doc_id,similarity_matrix, matrix):\n",
    "            ###print (f'대표 WORD: {ps_documents_df.iloc[doc_id][\"documents_cleaned\"]}')\n",
    "            ###print ('\\n')\n",
    "            ###print (f'Similar Words using {matrix}:')\n",
    "            if matrix=='Cosine Similarity':\n",
    "                similar_ix=np.argsort(similarity_matrix[doc_id])[::-1]\n",
    "            elif matrix=='Euclidean Distance':\n",
    "                similar_ix=np.argsort(similarity_matrix[doc_id])\n",
    "                \n",
    "            re_simil_words = []\n",
    "            re_simil_cos = []\n",
    "            for ix in similar_ix:\n",
    "                if ix==doc_id:\n",
    "                    continue\n",
    "                ##print('\\n')\n",
    "                ##print (f'{ps_documents_df.iloc[ix][\"documents_cleaned\"]} {similarity_matrix[doc_id][ix]}')\n",
    "                re_simil_words.append(ps_documents_df.iloc[ix][\"documents_cleaned\"])\n",
    "                re_simil_words.append(similarity_matrix[doc_id][ix])\n",
    "        #        #print (f'{matrix} : {similarity_matrix[doc_id][ix]}')\n",
    "        #         #print (f'Word: {ps_documents_df.iloc[ix][\"documents_cleaned\"]}')\n",
    "        #         #print (f'{matrix} : {similarity_matrix[doc_id][ix]}')\n",
    "            return re_simil_words,re_simil_cos\n",
    "\n",
    "\n",
    "\n",
    "        #####  이걸 실행하라고~!\n",
    "        def doctovec_run(input_value):\n",
    "            #1번 질문에 대한 1명의 학생에세이 분석결과\n",
    "            ps_documents_df=pd.DataFrame(input_value, columns=['documents_cleaned'])\n",
    "            tagged_data = [TaggedDocument(words=word_tokenize(doc), tags=[i]) for i, doc in enumerate(ps_documents_df.documents_cleaned)]\n",
    "            \n",
    "            model_d2v = Doc2Vec(vector_size=100,alpha=0.0025, min_count=1)\n",
    "\n",
    "            model_d2v.build_vocab(tagged_data)\n",
    "\n",
    "            for epoch in range(100):\n",
    "                model_d2v.train(tagged_data,\n",
    "                            total_examples=model_d2v.corpus_count,\n",
    "                            epochs=model_d2v.epochs)\n",
    "                \n",
    "            document_embeddings=np.zeros((ps_documents_df.shape[0],100))\n",
    "\n",
    "            for i in range(len(document_embeddings)):\n",
    "                document_embeddings[i]=model_d2v.docvecs[i]\n",
    "                \n",
    "            pairwise_similarities=cosine_similarity(document_embeddings)\n",
    "            \n",
    "\n",
    "            re_most_simWords = most_similar(0,pairwise_similarities,'Cosine Similarity',ps_documents_df)\n",
    "            \n",
    "            ###print(\"re_most_sim_words :\" , re_most_simWords)\n",
    "            \n",
    "            return re_most_simWords\n",
    "\n",
    "            ##################################################################################### \n",
    "\n",
    "            # def get_quet_numb_len(std_essay_dataset, selected_que_words):\n",
    "            \n",
    "            #     #1000명의 에세이, 선택된 질문관련 단어입력\n",
    "                \n",
    "            #     que_no_one_sim_words_ratio_result = sim_words_quesiton(std_essay_dataset, selected_que_words)\n",
    "            #     quet_numb_len = len(que_no_one_sim_words_ratio_result)\n",
    "                \n",
    "            #     return quet_numb_len\n",
    "\n",
    "            ############################################\n",
    "\n",
    "        import time\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        result_most_simWords = []\n",
    "\n",
    "        cont = 0\n",
    "        #이 코드는 문제없이 잘 돌아감\n",
    "        for j in tqdm(result_ques_): #학생데이터를 하나씩 가져와서\n",
    "            for k in tqdm(input_data_preprocessed): # 합친데이터를 하나씩 꺼내서\n",
    "                if j == k: #같으면, 그 위치로부터 시작해서 비교 구간까지의 데이터를 꺼내온다.\n",
    "                    ###print('j',j)\n",
    "                    ###print('k',k)\n",
    "\n",
    "                    input_data_preprocessed  #1명과 1000명데이터 분석결과 합친결과(1단어:1000명단어)\n",
    "\n",
    "                    input_data_preprocessed_length = len(input_data_preprocessed) #1000명 처리 결과 길이구하기\n",
    "\n",
    "\n",
    "                    end_numb = input_data_preprocessed.index(j) + len(result_most_simWords) + 1\n",
    "                    ###print(\"input_data_preprocessed.index(j) : \", input_data_preprocessed.index(j))\n",
    "                    ###print(\"end_numb :\", end_numb)\n",
    "                    in_text = input_data_preprocessed[input_data_preprocessed.index(j):end_numb]\n",
    "                    ###print('분석할 단어 그룹', in_text)\n",
    "                    # 첫 계산(학생 키워드와 전체 키워드 데이터의 거리를 각각 계산)을 하고, 다음 구간으로 넘어가자\n",
    "\n",
    "                    #doctovec_run(in_text) #함수실행\n",
    "\n",
    "                    #1번 질문에 대한 1명의 학생에세이 분석결과\n",
    "                    ps_documents_df=pd.DataFrame(in_text, columns=['documents_cleaned'])\n",
    "                    tagged_data = [TaggedDocument(words=word_tokenize(doc), tags=[i]) for i, doc in enumerate(ps_documents_df.documents_cleaned)]\n",
    "\n",
    "                    model_d2v = Doc2Vec(vector_size=100,alpha=0.0025, min_count=1)\n",
    "\n",
    "                    model_d2v.build_vocab(tagged_data)\n",
    "\n",
    "                    for epoch in tqdm(range(100)):\n",
    "                        model_d2v.train(tagged_data,\n",
    "                                    total_examples=model_d2v.corpus_count,\n",
    "                                    epochs=model_d2v.epochs)\n",
    "\n",
    "                    document_embeddings=np.zeros((ps_documents_df.shape[0],100))\n",
    "\n",
    "                    for i in range(len(document_embeddings)):\n",
    "                        document_embeddings[i]=model_d2v.docvecs[i]\n",
    "\n",
    "\n",
    "                    pairwise_similarities=cosine_similarity(document_embeddings)\n",
    "                    ###print('pairwise_similarities ::::::::::' , pairwise_similarities)\n",
    "\n",
    "                    re_most_simWords = most_similar(cont, pairwise_similarities,'Cosine Similarity')\n",
    "                    cont += 1\n",
    "                    result_most_simWords.append(re_most_simWords)\n",
    "                    ###print('re_most_simWords :', re_most_simWords)\n",
    "\n",
    "\n",
    "        rlt = [x[0] for x in result_most_simWords]\n",
    "        rlt = sum(rlt, [])\n",
    "        rlt = pd.DataFrame(rlt, columns = ['result_var'])\n",
    "\n",
    "        # 연관단어와 1000명의 통계데이터 비교값   분산을 계산하자.\n",
    "        rlt= rlt[1::2]\n",
    "        var_re = rlt.var() * 1000 #분산 구하기  이 값이 크면 변화율이 크기때문에 글의 내용이 퍼져있다는 것이다.상관관계가 낮다는 것이다.\n",
    "        result = var_re.tolist()[0] # pandas.core.series.series to list, to float number\n",
    " \n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "  0%|          | 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 42/100 [00:00<00:00, 411.45it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_ques_four : {'personal', 'trouble'}\n",
      "1000명 관련 data loaded : ['discover', 'question', 'conclude', 'problem', 'motion', 'interview', 'solution', 'personal', 'dilemma', 'cerebral', 'answer', 'realize', 'gain', 'learn', 'recognize', 'resolve', 'interrogate', 'identify', 'clear', 'detect', 'cook', 'fix', 'suffice', 'research', 'trouble', 'solve', 'inquiry', 'challenge', 'repair', 'wonder', 'meaning', 'clarify', 'understand', 'doubt', 'quarrel', 'specify', 'significance', 'name', 'reason', 'intellectual']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 409.55it/s][A\u001b[A\n",
      "\n",
      "  1%|          | 1/82 [00:00<00:20,  4.04it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 40/100 [00:00<00:00, 395.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 399.53it/s][A\u001b[A\n",
      "\n",
      " 11%|█         | 9/82 [00:00<00:03, 20.28it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 39/100 [00:00<00:00, 387.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 379.89it/s][A\u001b[A\n",
      "\n",
      "100%|██████████| 82/82 [00:00<00:00, 106.10it/s][A\n",
      " 50%|█████     | 1/2 [00:00<00:00,  1.29it/s]\n",
      "  0%|          | 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 36/100 [00:00<00:00, 359.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 361.92it/s][A\u001b[A\n",
      "\n",
      " 32%|███▏      | 26/82 [00:00<00:00, 92.36it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 37/100 [00:00<00:00, 364.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 357.88it/s][A\u001b[A\n",
      "\n",
      " 51%|█████     | 42/82 [00:00<00:00, 70.84it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 36/100 [00:00<00:00, 351.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 344.68it/s][A\u001b[A\n",
      "\n",
      "100%|██████████| 82/82 [00:00<00:00, 94.74it/s]\u001b[A\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.390936968121764"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmt_re = theme_all_section(input_text, 'ques_four')\n",
    "pmt_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감정분석 시작\n",
    "\n",
    "def emo_analysis(input_text):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "    model = BertForMultiLabelClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "\n",
    "    goemotions = MultiLabelPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    threshold=0.3\n",
    "    )\n",
    "\n",
    "    #결과확인\n",
    "    ##print(goemotions(texts))\n",
    "    ########## 여기서는 최초 입력 에세이를 적용한다. input_text !!!!!!!!\n",
    "    re_text = input_text.split(\".\")\n",
    "\n",
    "    #데이터 전처리 \n",
    "    def cleaning(datas):\n",
    "\n",
    "        fin_datas = []\n",
    "\n",
    "        for data in datas:\n",
    "            # 영문자 이외 문자는 공백으로 변환\n",
    "            only_english = re.sub('[^a-zA-Z]', ' ', data)\n",
    "\n",
    "            # 데이터를 리스트에 추가 \n",
    "            fin_datas.append(only_english)\n",
    "\n",
    "        return fin_datas\n",
    "\n",
    "    texts = cleaning(re_text)\n",
    "\n",
    "    #분석된 감정만 추출\n",
    "    emo_re = goemotions(texts)\n",
    "\n",
    "    emo_all = []\n",
    "    for list_val in range(0, len(emo_re)):\n",
    "        ##print(emo_re[list_val]['labels'],emo_re[list_val]['scores'])\n",
    "        #mo_all.append((emo_re[list_val]['labels'],emo_re[list_val]['scores'])) #KEY, VALUE만 추출하여 리스트로 저장\n",
    "        #emo_all.append(emo_re[list_val]['scores'])\n",
    "        emo_all.append((emo_re[list_val]['labels']))\n",
    "\n",
    "    from pandas.core.common import flatten #이중리스틀 FLATTEN하게 변환\n",
    "    flat_list = list(flatten(emo_all))\n",
    "        #중립적인 감정을 제외하고, 입력한 문장에서 다양한 감정을 모두 추출하고 어떤 감정이 있는지 계산\n",
    "        \n",
    "    unique = []\n",
    "    for r in flat_list:\n",
    "        if r == 'neutral':\n",
    "            pass\n",
    "        else:\n",
    "            unique.append(r)\n",
    "\n",
    "    #빈도수 계산하여 오름차순 정렬\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    count = Counter(unique)\n",
    "    words = dict(count.most_common())\n",
    "\n",
    "    ######  워드크라우드 구현  start  #####\n",
    "    # #분석가능한 감정 총 감정 수 - Bert origin model 적용시 28개 감정 추출돰\n",
    "    # total_num_emotion_analyzed = 28\n",
    "\n",
    "\n",
    "    # ########## wodCloud 설정 ########\n",
    "    # from wordcloud import WordCloud \n",
    "\n",
    "    # import matplotlib.pyplot as plt\n",
    "\n",
    "    # import nltk\n",
    "    # from nltk.corpus import stopwords\n",
    "    # %matplotlib inline\n",
    "\n",
    "    # import matplotlib\n",
    "    # from IPython.display import set_matplotlib_formats\n",
    "    # matplotlib.rc('font',family = 'Malgun Gothic')\n",
    "\n",
    "    # set_matplotlib_formats('retina')\n",
    "\n",
    "    # matplotlib.rc('axes',unicode_minus = False)\n",
    "\n",
    "    # #문장의 핵심감정을 워드크라우드로 표현(큰 글자가 가장 빈도수가 많이 나온 분석결과다)\n",
    "\n",
    "    # wordcloud = WordCloud(background_color='white',\n",
    "    #                     colormap = \"Accent_r\",\n",
    "    #                     width=1500, height=1000).generate_from_frequencies(words) \n",
    "\n",
    "    # plt.imshow(wordcloud)\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "    ######  워드크라우드 구현  end  #####\n",
    " \n",
    "    # 에세이에 표현된 핵심 감정값 도출 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    return words\n",
    "\n",
    "    # return 값 words  >>>>  딕셔너리로 출력됨\n",
    "\n",
    "    # {'approval': 7,\n",
    "    # 'admiration': 3,\n",
    "    # 'realization': 3,\n",
    "    # 'amusement': 1,\n",
    "    # 'confusion': 1,\n",
    "    # 'excitement': 1,\n",
    "    # 'annoyance': 1}\n",
    "\n",
    "    ############### 감정분석 ##### end #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Bloomington Normal is almost laughably cliché for a midwestern city. Vast swathes of corn envelop winding roads and the heady smell of BBQ smoke pervades the countryside every summer. Yet, underlying the trite norms of Normal is the prescriptive force of tradition—the expectation to fulfill my role as a female Filipino by playing Debussy in the yearly piano festival and enrolling in multivariable calculus instead of political philosophy.So when I discovered the technical demand of bebop, the triplet groove, and the intricacies of chordal harmony after ten years of grueling classical piano, I was fascinated by the music's novelty. Jazz guitar was not only evocative and creative, but also strangely liberating. I began to explore different pedagogical methods, transcribe solos from the greats, and experiment with various approaches until my own unique sound began to develop. And, although I did not know what would be the 'best' route for me to follow as a musician, the freedom to forge whatever path I felt was right seemed to be exactly what I needed; there were no expectations for me to continue in any particular way—only the way that suited my own desires.While journeying this trail, I found myself at Interlochen Arts Camp the summer before my junior year. Never before had I been immersed in an environment so conducive to musical growth: I was surrounded by people intensely passionate about pursuing all kinds of art with no regard for ideas of what art 'should' be. I knew immediately that this would be a perfect opportunity to cultivate my sound, unbounded by the limits of confining tradition. On the first day of camp, I found that my peer guitarist in big band was another Filipino girl from Illinois. Until that moment, my endeavors in jazz guitar had been a solitary effort; I had no one with whom to collaborate and no one against whom I could compare myself, much less someone from a background mirroring my own. I was eager to play with her, but while I quickly recognized a slew of differences between us—different heights, guitars, and even playing styles—others seemed to have trouble making that distinction during performances. Some even went as far as calling me 'other-Francesca.' Thus, amidst the glittering lakes and musky pine needles of Interlochen, I once again confronted Bloomington's frustrating expectations.After being mistaken for her several times, I could not help but view Francesca as a standard of what the 'female Filipino jazz guitarist' should embody. Her improvisatory language, comping style and even personal qualities loomed above me as something I had to live up to. Nevertheless, as Francesca and I continued to play together, it was not long before we connected through our creative pursuit. In time, I learned to draw inspiration from her instead of feeling pressured to follow whatever precedent I thought she set. I found that I grew because of, rather than in spite of, her presence; I could find solace in our similarities and even a sense of comfort in an unfamiliar environment without being trapped by expectation. Though the pressure to conform was still present—and will likely remain present in my life no matter what genre I'm playing or what pursuits I engage in—I learned to eschew its corrosive influence and enjoy the rewards that it brings. While my encounter with Francesca at first sparked a feeling of pressure to conform in a setting where I never thought I would feel its presence, it also carried the warmth of finding someone with whom I could connect. Like the admittedly trite conditions of my hometown, the resemblances between us provided comfort to me through their familiarity. I ultimately found that I can embrace this warmth while still rejecting the pressure to succumb to expectations, and that, in the careful balance between these elements, I can grow in a way that feels both like discove\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e9d94490f44a8e8b019269acffc4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760eefe67ffd4893a7211fe3258d5a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/182 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd6ee37b4194208ab54708a496ad5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/235 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc53518e8e94654a6514ffc4cca6401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94709233e35f45acb32a92d275c97555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_result = emo_analysis(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'approval': 6,\n",
       " 'realization': 5,\n",
       " 'admiration': 4,\n",
       " 'excitement': 2,\n",
       " 'amusement': 1,\n",
       " 'desire': 1,\n",
       " 'anger': 1,\n",
       " 'annoyance': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt에 따른 답변 에세이의 핵심감성 매칭 분석(excel-참고)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.input input_essay(text)\n",
    "2.sentiment analysis of input_essay\n",
    "3.compare result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Bloomington Normal is almost laughably cliché for a midwestern city. Vast swathes of corn envelop winding roads and the heady smell of BBQ smoke pervades the countryside every summer. Yet, underlying the trite norms of Normal is the prescriptive force of tradition—the expectation to fulfill my role as a female Filipino by playing Debussy in the yearly piano festival and enrolling in multivariable calculus instead of political philosophy.So when I discovered the technical demand of bebop, the triplet groove, and the intricacies of chordal harmony after ten years of grueling classical piano, I was fascinated by the music's novelty. Jazz guitar was not only evocative and creative, but also strangely liberating. I began to explore different pedagogical methods, transcribe solos from the greats, and experiment with various approaches until my own unique sound began to develop. And, although I did not know what would be the 'best' route for me to follow as a musician, the freedom to forge whatever path I felt was right seemed to be exactly what I needed; there were no expectations for me to continue in any particular way—only the way that suited my own desires.While journeying this trail, I found myself at Interlochen Arts Camp the summer before my junior year. Never before had I been immersed in an environment so conducive to musical growth: I was surrounded by people intensely passionate about pursuing all kinds of art with no regard for ideas of what art 'should' be. I knew immediately that this would be a perfect opportunity to cultivate my sound, unbounded by the limits of confining tradition. On the first day of camp, I found that my peer guitarist in big band was another Filipino girl from Illinois. Until that moment, my endeavors in jazz guitar had been a solitary effort; I had no one with whom to collaborate and no one against whom I could compare myself, much less someone from a background mirroring my own. I was eager to play with her, but while I quickly recognized a slew of differences between us—different heights, guitars, and even playing styles—others seemed to have trouble making that distinction during performances. Some even went as far as calling me 'other-Francesca.' Thus, amidst the glittering lakes and musky pine needles of Interlochen, I once again confronted Bloomington's frustrating expectations.After being mistaken for her several times, I could not help but view Francesca as a standard of what the 'female Filipino jazz guitarist' should embody. Her improvisatory language, comping style and even personal qualities loomed above me as something I had to live up to. Nevertheless, as Francesca and I continued to play together, it was not long before we connected through our creative pursuit. In time, I learned to draw inspiration from her instead of feeling pressured to follow whatever precedent I thought she set. I found that I grew because of, rather than in spite of, her presence; I could find solace in our similarities and even a sense of comfort in an unfamiliar environment without being trapped by expectation. Though the pressure to conform was still present—and will likely remain present in my life no matter what genre I'm playing or what pursuits I engage in—I learned to eschew its corrosive influence and enjoy the rewards that it brings. While my encounter with Francesca at first sparked a feeling of pressure to conform in a setting where I never thought I would feel its presence, it also carried the warmth of finding someone with whom I could connect. Like the admittedly trite conditions of my hometown, the resemblances between us provided comfort to me through their familiarity. I ultimately found that I can embrace this warmth while still rejecting the pressure to succumb to expectations, and that, in the careful balance between these elements, I can grow in a way that feels both like discove\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senti_ays_by_prompt(input_text, question_num):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "    model = BertForMultiLabelClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "\n",
    "    goemotions = MultiLabelPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    threshold=0.3\n",
    "    )\n",
    "    \n",
    "    re_text = input_text.split(\".\")\n",
    "\n",
    "    #데이터 전처리 \n",
    "    def cleaning(datas):\n",
    "\n",
    "        fin_datas = []\n",
    "\n",
    "        for data in datas:\n",
    "            # 영문자 이외 문자는 공백으로 변환\n",
    "            only_english = re.sub('[^a-zA-Z]', ' ', data)\n",
    "\n",
    "            # 데이터를 리스트에 추가 \n",
    "            fin_datas.append(only_english)\n",
    "\n",
    "        return fin_datas\n",
    "\n",
    "    texts = cleaning(re_text)\n",
    "\n",
    "    #분석된 감정만 추출\n",
    "    emo_re = goemotions(texts)\n",
    "\n",
    "    emo_all = []\n",
    "    for list_val in range(0, len(emo_re)):\n",
    "        emo_all.append((emo_re[list_val]['labels']))\n",
    "\n",
    "    from pandas.core.common import flatten #이중리스틀 FLATTEN하게 변환\n",
    "    flat_list = list(flatten(emo_all))\n",
    "        #중립적인 감정을 제외하고, 입력한 문장에서 다양한 감정을 모두 추출하고 어떤 감정이 있는지 계산\n",
    "        \n",
    "    unique = []\n",
    "    for r in flat_list:\n",
    "        if r == 'neutral':\n",
    "            pass\n",
    "        else:\n",
    "            unique.append(r)\n",
    "\n",
    "    #빈도수 계산하여 오름차순 정렬\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    count = Counter(unique)\n",
    "    words = dict(count.most_common())\n",
    "    # 딕셔너리의 keys 추출하여 리스트로\n",
    "    senti_re = list(words.keys())\n",
    "    \n",
    "    #질문별 감성 키워드 분류\n",
    "    ques_1_senti_keywords_high_score = ['joy', 'pride', 'approval']\n",
    "    ques_2_senti_keywords_high_score = ['disappointment', 'fear', 'confusion']\n",
    "    ques_3_senti_keywords_high_score = ['curiosity', 'disapproval', 'realization']\n",
    "    ques_4_senti_keywords_high_score = ['gratitude', 'surprise', 'admiration']\n",
    "    ques_5_senti_keywords_high_score = ['realization', 'pride', 'admiration']\n",
    "    ques_6_senti_keywords_high_score = ['curiosity', 'excitement', 'confusion']\n",
    "    ques_7_senti_keywords_high_score = []\n",
    "    \n",
    "    ques_1_senti_keywords_low_score = ['curiosity', 'amusement', 'admiration', 'excitement', 'realization']\n",
    "    ques_2_senti_keywords_low_score = ['anger', 'relief', 'embarrassment', 'disapproval', 'nervousness']\n",
    "    ques_3_senti_keywords_low_score = ['disappointment', 'anger', 'nervousness', 'confusion', 'approval']\n",
    "    ques_4_senti_keywords_low_score = ['caring', 'joy', 'love', 'optimism']\n",
    "    ques_5_senti_keywords_low_score = ['approval', 'curiosity', 'gratitude', 'caring', 'joy']\n",
    "    ques_6_senti_keywords_low_score = ['desire', 'realization', 'amusement', 'joy', 'surprise']\n",
    "    ques_7_senti_keywords_low_score = []\n",
    "    \n",
    "    # 결과 비교하기\n",
    "    re_comp_high = []\n",
    "    re_comp_low = []\n",
    "    if \"ques_1\" == question_num:\n",
    "        selected_pmpt_number = \"1\" # 선택한 prompt 항목\n",
    "        for i in senti_re:\n",
    "            if i in ques_1_senti_keywords_high_score:\n",
    "                re_comp_high.append(i) # add score\n",
    "            elif i in ques_1_senti_keywords_low_score:\n",
    "                re_comp_low.appenbd(i) # minus score\n",
    "    elif \"ques_2\" == question_num:\n",
    "        selected_pmpt_number = \"2\"\n",
    "        for i in senti_re:\n",
    "            if i in ques_2_senti_keywords_high_score:\n",
    "                re_comp_high.append(i) # add score\n",
    "            elif i in ques_2_senti_keywords_low_score:\n",
    "                re_comp_low.appenbd(i) # minus score\n",
    "    elif \"ques_3\" == question_num:\n",
    "        selected_pmpt_number = \"3\"\n",
    "        for i in senti_re:\n",
    "            if i in ques_3_senti_keywords_high_score:\n",
    "                re_comp_high.append(i) # add score\n",
    "            elif i in ques_3_senti_keywords_low_score:\n",
    "                re_comp_low.appenbd(i) # minus score\n",
    "    elif \"ques_4\" == question_num:\n",
    "        selected_pmpt_number = \"4\" # 선택한 prompt 항목\n",
    "        for i in senti_re:\n",
    "            if i in ques_4_senti_keywords_high_score:\n",
    "                re_comp_high.append(i) # add score\n",
    "            elif i in ques_4_senti_keywords_low_score:\n",
    "                re_comp_low.appenbd(i) # minus score\n",
    "    elif \"ques_5\" == question_num:\n",
    "        selected_pmpt_number = \"5\" # 선택한 prompt 항목\n",
    "        for i in senti_re:\n",
    "            if i in ques_5_senti_keywords_high_score:\n",
    "                re_comp_high.append(i) # add score\n",
    "            elif i in ques_5_senti_keywords_low_score:\n",
    "                re_comp_low.appenbd(i) # minus score\n",
    "    elif \"ques_6\" == question_num:\n",
    "        selected_pmpt_number = \"6\" # 선택한 prompt 항목\n",
    "        for i in senti_re:\n",
    "            if i in ques_6_senti_keywords_high_score:\n",
    "                re_comp_high.append(i) # add score\n",
    "            elif i in ques_6_senti_keywords_low_score:\n",
    "                re_comp_low.appenbd(i) # minus score\n",
    "    elif \"ques_7\"  == question_num:\n",
    "        selected_pmpt_number = \"7\" # 선택한 prompt 항목\n",
    "        re_comp_high.append(senti_re)\n",
    "    else :\n",
    "        pass\n",
    "    \n",
    "    #결과비교 및 스코어링\n",
    "    if not re_comp_high and not re_comp_low: # 각 리스트에 값이 없다면, 매칭되는 것이 없기 때문에 점수 없음\n",
    "        print(\"can't not get result, try again\")\n",
    "        score_re = \"can't not get result\"\n",
    "    elif not re_comp_low and re_comp_low: # high 값이 없고, low값이 있다면\n",
    "        result_ = re_comp_high + re_comp_low\n",
    "        score_re = \"get low score!\"\n",
    "        print(\"get low score!\")\n",
    "    elif not re_comp_low and re_comp_high: # high 값이 있고 low값이 없다면\n",
    "        result_ = re_comp_high + re_comp_low\n",
    "        score_re = \"get high score!\"\n",
    "        print(\"get high score!\")\n",
    "    else:\n",
    "        result_ = re_comp_high # 7번 prompt일 경우 분석결과는 입력한 에세이와 일치할 테니까 있는 그대로 점수를 줌(높은점수)\n",
    "        score_re = \"get high score!\"\n",
    "        \n",
    "    # 에세이에 표현된 감정과 Prompt 비교분석 결과(높은점수에 해당하는 관련 키워드, 낮은점수에 해당하는 관련 키워드)   \n",
    "    return result_, score_re , selected_pmpt_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get high score!\n",
      "sentiment analysis result of essay, score, seletec prompt number :  ([['approval', 'realization', 'admiration', 'excitement', 'amusement', 'desire', 'anger', 'annoyance']], 'get high score!', '7')\n"
     ]
    }
   ],
   "source": [
    "re__ = senti_ays_by_prompt(input_text, 'ques_7')\n",
    "print(\"sentiment analysis result of essay, score, seletec prompt number : \", re__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comment generator\n",
    "위의 계산 결과를 토대로 코멘트를 생성하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(re__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re__[2] # selected prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['approval',\n",
       "  'realization',\n",
       "  'admiration',\n",
       "  'excitement',\n",
       "  'amusement',\n",
       "  'desire',\n",
       "  'anger',\n",
       "  'annoyance']]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re__[0] # 추출된 감성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_no = ['1', '2', '3','4','5','6','7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prompt_no[0] == re__[2]: # prompt#1 을 선택했을 경우\n",
    "    comment1 = [\"Experts advise that prompt #1 seeks the following key qualities: identity, diversity, inclinations, passion, culture, unique qualities, life story, values, experience, lessons, family, home, talent, personal history, and more.\"]\n",
    "    comment2 = [\"Sentiments relevant to such qualities are joy, pride, approval, and more.\"]\n",
    "    comment3 = [\"Our AI analysis shows that the dominant sentiments in your essay are \", re__[0]]\n",
    "    comment4 = [\"Such pattern in your essay seem to be\", re__[0],\"aligned with the sentiments intended by the prompt.\"]\n",
    "    \n",
    "elif prompt_no[1] == re__[2]: # prompt#2 을 선택했을 경우\n",
    "    comment1 = [\"Experts advise that prompt #2 seeks the following key qualities: obstacle, hardship, challenge, failure, lessons, values, triumph, rebound, courage, initiative, attitude, improvement, development, and more.\"]\n",
    "    comment2 = [\"Sentiments relevant to such qualities are disappointment, fear, confusion, and more.\"]\n",
    "    comment3 = [\"Our AI analysis shows that the dominant sentiments in your essay are \", re__[0]]\n",
    "    comment4 = [\"Such pattern in your essay seem to be\", re__[0],\"aligned with the sentiments intended by the prompt.\"]\n",
    "    \n",
    "elif prompt_no[2] == re__[2]: # prompt#3 을 선택했을 경우\n",
    "    comment1 = [\"Experts advise that prompt #3 seeks the following key qualities: critical thinking, courage, challenging spirit, self-reflection, intellect, action, change, respect, realization, improvement, curiosity, leadership, fight, and more.\"]\n",
    "    comment2 = [\"SSentiments relevant to such qualities are curiosity, disapproval, realization, and more.\"]\n",
    "    comment3 = [\"Our AI analysis shows that the dominant sentiments in your essay are \", re__[0]]\n",
    "    comment4 = [\"Such pattern in your essay seem to be\", re__[0],\"aligned with the sentiments intended by the prompt.\"]\n",
    "    \n",
    "elif prompt_no[3] == re__[2]: # prompt#4 을 선택했을 경우\n",
    "    comment1 = [\"Experts advise that prompt #4 seeks the following key qualities: gratitude, altruism, hero, heroine, philanthropy, caring, preconception, realization, maturity, sacrifice, reward, the common good, hardship, virtue, and more.\"]\n",
    "    comment2 = [\"Sentiments relevant to such qualities are gratitude, surprise, admiration, and more.\"]\n",
    "    comment3 = [\"Our AI analysis shows that the dominant sentiments in your essay are \", re__[0]]\n",
    "    comment4 = [\"Such pattern in your essay seem to be\", re__[0],\"aligned with the sentiments intended by the prompt.\"]\n",
    "    \n",
    "elif prompt_no[4] == re__[2]: # prompt#5 을 선택했을 경우\n",
    "    comment1 = [\"Experts advise that prompt #5 seeks the following key qualities: incident, initiative, accomplishment, maturity, perspective change, realization, life story, personal history, triumph, community, team, people, and more.\"]\n",
    "    comment2 = [\"Sentiments relevant to such qualities are realization, pride, admiration, and more.\"]\n",
    "    comment3 = [\"Our AI analysis shows that the dominant sentiments in your essay are \", re__[0]]\n",
    "    comment4 = [\"Such pattern in your essay seem to be\", re__[0],\"aligned with the sentiments intended by the prompt.\"]\n",
    "    \n",
    "elif prompt_no[5] == re__[2]: # prompt#6 을 선택했을 경우\n",
    "    comment1 = [\"Experts advise that prompt #6 seeks the following key qualities: curiosity, intellectual, social science, STEM, humanities, ideology, question, research, think, logic, reason, depth, academic, intriguing, goal, plan, and more.\"]\n",
    "    comment2 = [\"Sentiments relevant to such qualities are curiosity, excitement, confusion, and more.\"]\n",
    "    comment3 = [\"Our AI analysis shows that the dominant sentiments in your essay are \", re__[0]]\n",
    "    comment4 = [\"Such pattern in your essay seem to be\", re__[0],\"aligned with the sentiments intended by the prompt.\"]\n",
    "    \n",
    "elif prompt_no[6] == re__[2]: # prompt#7 을 선택했을 경우 ---> kj님에게 확인해야 함. 7번을 선택했을 경우 1~6번을 추가 선택하는지에 여부\n",
    "    comment1 = [\"Experts advise that the topic of your choice for prompt #7 seeks the following key qualities: \", re__[0]]\n",
    "    comment2 = [\"Sentiments relevant to such qualities are\", re__[0], \", and more.\"]\n",
    "    comment3 = [\"Our AI analysis shows that the dominant sentiments in your essay are \", re__[0]]\n",
    "    comment4 = [\"Such pattern in your essay seem to be\", re__[0],\"aligned with the sentiments intended by the prompt.\"]\n",
    "                \n",
    "else:\n",
    "    pass\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Experts advise that the topic of your choice for prompt #7 seeks the following key qualities: ',\n",
       " [['approval',\n",
       "   'realization',\n",
       "   'admiration',\n",
       "   'excitement',\n",
       "   'amusement',\n",
       "   'desire',\n",
       "   'anger',\n",
       "   'annoyance']],\n",
       " 'Sentiments relevant to such qualities are',\n",
       " [['approval',\n",
       "   'realization',\n",
       "   'admiration',\n",
       "   'excitement',\n",
       "   'amusement',\n",
       "   'desire',\n",
       "   'anger',\n",
       "   'annoyance']],\n",
       " ', and more.',\n",
       " 'Our AI analysis shows that the dominant sentiments in your essay are ',\n",
       " [['approval',\n",
       "   'realization',\n",
       "   'admiration',\n",
       "   'excitement',\n",
       "   'amusement',\n",
       "   'desire',\n",
       "   'anger',\n",
       "   'annoyance']],\n",
       " 'Such pattern in your essay seem to be',\n",
       " [['approval',\n",
       "   'realization',\n",
       "   'admiration',\n",
       "   'excitement',\n",
       "   'amusement',\n",
       "   'desire',\n",
       "   'anger',\n",
       "   'annoyance']],\n",
       " 'aligned with the sentiments intended by the prompt.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_comment = comment1 + comment2 + comment3 + comment4\n",
    "re_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
