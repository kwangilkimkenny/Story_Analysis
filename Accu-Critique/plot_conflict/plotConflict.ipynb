{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kimkwangil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/kimkwangil/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kimkwangil/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kimkwangil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 개발환경\n",
    "# Office \n",
    "\n",
    "# 만약 C extension 문제로 속도가 느려지면 아래 명려어로 gensim 설치할 것\n",
    "# conda install -c conda-forge gensim\n",
    "\n",
    "\n",
    "####### 실행 테스트  ########\n",
    "\n",
    "# 1.ai_plot_conf(input_text) 실행하면, \n",
    "\n",
    "# 2.결과 나옴!(그래프 2개, 적합성, 복잡성 등등 값 도출됨)\n",
    "\n",
    "# ACTION VERBS RATIO : 6.25\n",
    "# ====================================================================\n",
    "# 에세이에 표현된 다양한 감정 수: 7\n",
    "# ====================================================================\n",
    "# 문장에 표현된 감정 비율 :  25.0\n",
    "# ====================================================================\n",
    "# ['trail', 'view', 'city', 'camp', 'until', 'on', 'way', 'in', 'through', 'after', 'up', 'during', 'by', 'from', 'sound', 'to', 'path', 'above', 'against', 'before', 'route', 'forge']\n",
    "# ai_plot_conflict.py:601: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n",
    "#   ext_setting_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "# ====================================================================\n",
    "# SETTING RATIO :  12.34\n",
    "# ====================================================================\n",
    "# ai_plot_conflict.py:674: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n",
    "#   ext_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "# 전체 문장에서 캐릭터를 의미하는 단어나 유사어 비율 : 8.79\n",
    "# conflict 단어가 전체 문장(단어)에서 차지하는 비율 계산 : 1.5\n",
    "# 감정기복비율 : 25.0\n",
    "# 셋팅비율 계산 :  12.34\n",
    "# Degree of Conflict : 27.59764663879875\n",
    "\n",
    "#conflict\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "from mpld3 import plugins, fig_to_html, save_html, fig_to_dict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "#character, setting\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import multiprocessing\n",
    "import os\n",
    "from pathlib import Path\n",
    "import io\n",
    "from gensim.models import Phrases\n",
    "from textblob import TextBlob\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from model import BertForMultiLabelClassification\n",
    "from multilabel_pipeline import MultiLabelPipeline\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "model = BertForMultiLabelClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "\n",
    "goemotions = MultiLabelPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    threshold=0.3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Bloomington Normal is almost laughably cliché for a midwestern city. Vast swathes of corn envelop winding roads and the heady smell of BBQ smoke pervades the countryside every summer. Yet, underlying the trite norms of Normal is the prescriptive force of tradition—the expectation to fulfill my role as a female Filipino by playing Debussy in the yearly piano festival and enrolling in multivariable calculus instead of political philosophy.So when I discovered the technical demand of bebop, the triplet groove, and the intricacies of chordal harmony after ten years of grueling classical piano, I was fascinated by the music's novelty. Jazz guitar was not only evocative and creative, but also strangely liberating. I began to explore different pedagogical methods, transcribe solos from the greats, and experiment with various approaches until my own unique sound began to develop. And, although I did not know what would be the 'best' route for me to follow as a musician, the freedom to forge whatever path I felt was right seemed to be exactly what I needed; there were no expectations for me to continue in any particular way—only the way that suited my own desires.While journeying this trail, I found myself at Interlochen Arts Camp the summer before my junior year. Never before had I been immersed in an environment so conducive to musical growth: I was surrounded by people intensely passionate about pursuing all kinds of art with no regard for ideas of what art 'should' be. I knew immediately that this would be a perfect opportunity to cultivate my sound, unbounded by the limits of confining tradition. On the first day of camp, I found that my peer guitarist in big band was another Filipino girl from Illinois. Until that moment, my endeavors in jazz guitar had been a solitary effort; I had no one with whom to collaborate and no one against whom I could compare myself, much less someone from a background mirroring my own. I was eager to play with her, but while I quickly recognized a slew of differences between us—different heights, guitars, and even playing styles—others seemed to have trouble making that distinction during performances. Some even went as far as calling me 'other-Francesca.' Thus, amidst the glittering lakes and musky pine needles of Interlochen, I once again confronted Bloomington's frustrating expectations.After being mistaken for her several times, I could not help but view Francesca as a standard of what the 'female Filipino jazz guitarist' should embody. Her improvisatory language, comping style and even personal qualities loomed above me as something I had to live up to. Nevertheless, as Francesca and I continued to play together, it was not long before we connected through our creative pursuit. In time, I learned to draw inspiration from her instead of feeling pressured to follow whatever precedent I thought she set. I found that I grew because of, rather than in spite of, her presence; I could find solace in our similarities and even a sense of comfort in an unfamiliar environment without being trapped by expectation. Though the pressure to conform was still present—and will likely remain present in my life no matter what genre I'm playing or what pursuits I engage in—I learned to eschew its corrosive influence and enjoy the rewards that it brings. While my encounter with Francesca at first sparked a feeling of pressure to conform in a setting where I never thought I would feel its presence, it also carried the warmth of finding someone with whom I could connect. Like the admittedly trite conditions of my hometown, the resemblances between us provided comfort to me through their familiarity. I ultimately found that I can embrace this warmth while still rejecting the pressure to succumb to expectations, and that, in the careful balance between these elements, I can grow in a way that feels both like discove\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 전처리 \n",
    "def cleaning(datas):\n",
    "\n",
    "    fin_datas = []\n",
    "\n",
    "    for data in datas:\n",
    "        # 영문자 이외 문자는 공백으로 변환\n",
    "        only_english = re.sub('[^a-zA-Z]', ' ', data)\n",
    "    \n",
    "        # 데이터를 리스트에 추가 \n",
    "        fin_datas.append(only_english)\n",
    "\n",
    "    return fin_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_plot_conf(essay_input_):\n",
    "    #1.input essay\n",
    "    input_text = essay_input_\n",
    "\n",
    "    #########################################################################\n",
    "\n",
    "    #2.유사단어를 추출하여 리스트로 반환\n",
    "    def conflict_sim_words(text):\n",
    "\n",
    "        essay_input_corpus = str(text) #문장입력\n",
    "        essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "        sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "        total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "        total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "        \n",
    "        split_sentences = []\n",
    "        for sentence in sentences:\n",
    "            processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "            words = processed.split()\n",
    "            split_sentences.append(words)\n",
    "\n",
    "        skip_gram = 1\n",
    "        workers = multiprocessing.cpu_count()\n",
    "        bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "        model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "        model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "        \n",
    "        #모델 설계 완료\n",
    "\n",
    "        #표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "        confict_words_list = ['clash', 'incompatible', 'inconsistent', 'incongruous', 'opposition', 'variance','vary', 'odds', \n",
    "                                'differ', 'diverge', 'disagree', 'contrast', 'collide', 'contradictory', 'incompatible', 'conflict',\n",
    "                                'inconsistent','irreconcilable','incongruous','contrary','opposite','opposing','opposed',\n",
    "                                'antithetical','clashing','discordant','differing','different','divergent','discrepant',\n",
    "                                'varying','disagreeing','contrasting','at odds','in opposition','at variance' ]\n",
    "        \n",
    "        ####문장에 list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "        \n",
    "        #우선 토큰화한다.\n",
    "        retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "        token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "        #print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "        #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "        filtered_chr_text = []\n",
    "        for k in token_input_text:\n",
    "            for j in confict_words_list:\n",
    "                if k == j:\n",
    "                    filtered_chr_text.append(j)\n",
    "        \n",
    "        #print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "        \n",
    "        filtered_chr_text_ = set(filtered_chr_text) #중복제거\n",
    "        filtered_chr_text__ = list(filtered_chr_text_) #다시 리스트로 변환\n",
    "        #print (filtered_chr_text__) # 중복값 제거 확인\n",
    "        \n",
    "#         for i in filtered_chr_text__:\n",
    "#             ext_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "        \n",
    "#         char_total_count = len(filtered_chr_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 표현 수\n",
    "#         char_count_ = len(filtered_chr_text__) #중복제거된  표현 총 수\n",
    "            \n",
    "#         result_char_ratio = round(char_total_count/total_words * 100, 2)\n",
    "\n",
    "#         import pandas as pd\n",
    "\n",
    "#         df_conf_words = pd.DataFrame(ext_sim_words_key, columns=['words','values']) #데이터프레임으로 변환\n",
    "#         df_r = df_conf_words['words'] #words 컬럼 값 추출\n",
    "#         ext_sim_words_key = df_r.values.tolist() # 유사단어 추출\n",
    "        ext_sim_words_key = filtered_chr_text__\n",
    "\n",
    "        #return result_char_ratio, total_sentences, total_words, char_total_count, char_count_, ext_sim_words_key\n",
    "        return ext_sim_words_key\n",
    "\n",
    "\n",
    "\n",
    "    #########################################################################\n",
    "    # 3.유사단어를 문장에서 추출하여 반환한다.\n",
    "    conflict_sim_words_ratio_result = conflict_sim_words(input_text)\n",
    "\n",
    "\n",
    "\n",
    "    #########################################################################\n",
    "    # 4.CONFLICT GRAPH EXPRESSION Analysis  -- 그래프로 그리기\n",
    "    # conflict(input_text):\n",
    "    contents = str(input_text)\n",
    "    token_list_str = text_to_word_sequence(contents) #tokenize\n",
    "    # 원본문장 단어 중복제거\n",
    "    token_list_str_set = set(token_list_str)\n",
    "    print('token_list_str:', token_list_str)\n",
    "    confict_words_list_basic = ['clash', 'incompatible', 'inconsistent', 'incongruous', 'opposition', 'variance','vary', 'odds', \n",
    "                            'differ', 'diverge', 'disagree', 'contrast', 'collide', 'contradictory', 'incompatible', 'conflict',\n",
    "                            'inconsistent','irreconcilable','incongruous','contrary','opposite','opposing','opposed', 'fight',\n",
    "                            'antithetical','clashing','discordant','differing','different','divergent','discrepant', 'beef', 'bone to pick',\n",
    "                            'varying','disagreeing','contrasting','at odds','in opposition','at variance', 'different', 'bone of contention',\n",
    "                            'battle', 'competition', 'combat', 'rivalry', 'strife', 'struggle', 'war', 'collision',\n",
    "                            'contention', 'contest', 'emulation', 'encounter', 'engagement', 'fracas', 'fray', 'set-to',\n",
    "                            'striving', 'tug-of-war', 'conflicted', 'conflicting', 'conflicts', 'disagreement','contrariety',\n",
    "                            'friction', 'enmity', 'dissension', 'incongruity', 'rancor', 'resistance', 'hostility', 'hatred',\n",
    "                            'discord', 'debate', 'controversy', 'dispute', 'agitation','matter','matter at hand','problem',\n",
    "                            'point in question', 'question', 'dispute', 'issue', 'sore point', 'tender spot', 'quarrel', 'discord']\n",
    "\n",
    "    confict_words_list = confict_words_list_basic + conflict_sim_words_ratio_result #유사단어를 계산결과 반영!\n",
    "    #중복제거\n",
    "    confict_words_list_set = set(confict_words_list)\n",
    "    print('confict_words_list:', confict_words_list_set)\n",
    "    \n",
    "    # 문장에 들어있는 추출된 conflict 단어들 : count_conflict_list ==================> conflict 단어가 없음(겹치는 단어 없나?)\n",
    "    count_conflict_list = []\n",
    "    for ittm in confict_words_list_set:\n",
    "        if ittm in token_list_str_set:\n",
    "            count_conflict_list.append(ittm)\n",
    "            \n",
    "    print('문장에 들어있는 추출된 conflict 단어들:', count_conflict_list)\n",
    "    \n",
    "    # 전체문장에 들어있는 conflict 단어 수\n",
    "    print('전체문장에 들어있는 conflict 단어 수:', len(count_conflict_list))\n",
    "    \n",
    "    list_str = contents.split(\".\")  # 문장별로 분리한다. 분리는 .를 기준으로 한다.   \n",
    "\n",
    "    listSentiment = []\n",
    "\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    i=0\n",
    "    for sentence in tqdm(list_str): #한문장식 가져와서 처리한다.\n",
    "        ss = sid.polarity_scores(sentence) #긍정, 부정, 중립, 혼합점수 계산\n",
    "        #print(ss.keys())\n",
    "        #print('{}: neg:{},neu:{},pos:{},compound:{}'.format(i,ss['neg'],ss['neu'],ss['pos'],ss['compound']))\n",
    "        #print('{}: neg:{}'.format(i,ss['neg']))\n",
    "        i +=1\n",
    "        listSentiment.append([ss['neg'],ss['neu'],ss['pos'],ss['compound']])\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    df_sent = pd.DataFrame(listSentiment)\n",
    "    df_sent.columns = ['neg', 'neu', 'pos','compound']\n",
    "    reslult_df = df_sent.columns\n",
    "\n",
    "\n",
    "    df_sent['comp_score'] = df_sent['compound'].apply(lambda c: 'pos' if c >=0  else 'neg')\n",
    "\n",
    "    df_sent['comp_score'].value_counts()\n",
    "\n",
    "    conflict_ratio = df_sent['comp_score'].value_counts(normalize=True) #상대적 비율 계산\n",
    "\n",
    "    # df_sent 의 값은 아래와 같다.\n",
    "\n",
    "    # neg   neu   pos   compound   comp_score\n",
    "    # 0   0.000   0.808   0.192   0.2280   pos\n",
    "    # 1   0.000   1.000   0.000   0.0000   pos\n",
    "    # 2   0.041   0.778   0.181   0.7269   pos\n",
    "    # 3   0.044   0.787   0.169   0.6486   pos\n",
    "    # 4   0.190   0.678   0.132   -0.2144   neg\n",
    "    # 5   0.000   1.000   0.000   0.0000   pos\n",
    "\n",
    "    #comp_score를 1 -1 변환\n",
    "    df_sent.loc[df_sent[\"comp_score\"] == \"pos\",\"comp_score\"] = 1\n",
    "    df_sent.loc[df_sent[\"comp_score\"] == \"neg\",\"comp_score\"] = -1\n",
    "\n",
    "    # df_sent 의 변환된 값은 아래와 같다.\n",
    "    # neg   neu   pos   compound   comp_score\n",
    "    # 0   0.000   0.808   0.192   0.2280   1\n",
    "    # 1   0.000   1.000   0.000   0.0000   1\n",
    "    # 2   0.041   0.778   0.181   0.7269   1\n",
    "    # 3   0.044   0.787   0.169   0.6486   1\n",
    "    # 4   0.190   0.678   0.132   -0.2144   -1\n",
    "    # 5   0.000   1.000   0.000   0.0000   \n",
    "\n",
    "    #########################################################################\n",
    "    # 5. 그래프로 그려보자. 이 코드는 matplotlib 로 그린것임. 종필은 highcharts로 표현할 것\n",
    "    #########################################################################\n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    # from matplotlib import pyplot as plt\n",
    "\n",
    "    # plt.plot(df_sent)\n",
    "    # plt.xlabel('STORY')\n",
    "    # plt.ylabel('CONFLICT')\n",
    "    # plt.title('FLOW ANALYSIS')\n",
    "    # plt.legend(['neg','neu','pos','compound','reslult'])\n",
    "    # plt.show()\n",
    "\n",
    "    #########################################################################\n",
    "    # 6.ACTION VERB로 그래프 그리기\n",
    "\n",
    "\n",
    "    #입력한 글을 모두 단어로 쪼개로 리스트로 만들기 - \n",
    "    essay_input_corpus_ = str(input_text) #문장입력\n",
    "    essay_input_corpus_ = essay_input_corpus_.lower()#소문자 변환\n",
    "\n",
    "    sentences_  = sent_tokenize(essay_input_corpus_) #문장단위로 토큰화(구분)되어 리스에 담김\n",
    "\n",
    "    # 문장을 토크큰화하여 해당 문장에 Action Verbs가 있는지 분석 부분 코드임 ---> 뒤에서 나옴 아래 777로 표시된 코드부분에서 sentences_ 값 재활용\n",
    "\n",
    "    split_sentences_ = []\n",
    "    for sentence in sentences_:\n",
    "        processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "        words = processed.split()\n",
    "        split_sentences_.append(words)\n",
    "        \n",
    "    # 입력한 문장을 모두 리스트로 변환\n",
    "    input_text_list = [y for x in split_sentences_ for y in x] # 이중 리스트 Flatten\n",
    "\n",
    "    #리스로 변환된 값 확인\n",
    "    #input_text_list \n",
    "\n",
    "    #csv 파일에서 Action Verbs 단어 사전 불러오기\n",
    "    import pandas as pd\n",
    "\n",
    "    #Awards 데이터 불러오기\n",
    "    data_action_verbs = pd.read_csv('actionverbs.csv')\n",
    "    data_ac_verbs_list = data_action_verbs.values.tolist()\n",
    "    verbs_list = [y for x in data_ac_verbs_list for y in x]\n",
    "\n",
    "    #########################################################################\n",
    "    # 7.Action Verbs 유사단어를 추출하여 리스트로 반환\n",
    "\n",
    "    def actionverb_sim_words(text):\n",
    "\n",
    "        essay_input_corpus = str(text) #문장입력\n",
    "        essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "        sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "        total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "        total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "        \n",
    "        split_sentences = []\n",
    "        for sentence in sentences:\n",
    "            processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "            words = processed.split()\n",
    "            split_sentences.append(words)\n",
    "\n",
    "        skip_gram = 1\n",
    "        workers = multiprocessing.cpu_count()\n",
    "        bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "        model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "        model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "        \n",
    "        #모델 설계 완료\n",
    "\n",
    "        # ACTION VERBS 표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "        ##################################################\n",
    "        # verbs_list\n",
    "\n",
    "        ####문장에 list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "        \n",
    "        #우선 토큰화한다.\n",
    "        retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "        token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "        #print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "        #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "        filtered_chr_text = []\n",
    "        for k in token_input_text:\n",
    "            for j in verbs_list:\n",
    "                if k == j:\n",
    "                    filtered_chr_text.append(j)\n",
    "        \n",
    "        #print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "        \n",
    "        filtered_chr_text_ = set(filtered_chr_text) #중복제거\n",
    "        filtered_chr_text__ = list(filtered_chr_text_) #다시 리스트로 변환\n",
    "        #print (filtered_chr_text__) # 중복값 제거 확인\n",
    "        \n",
    "#         for i in filtered_chr_text__:\n",
    "#             ext_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "        \n",
    "#         char_total_count = len(filtered_chr_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 표현 수\n",
    "#         char_count_ = len(filtered_chr_text__) #중복제거된  표현 총 수\n",
    "            \n",
    "#         result_char_ratio = round(char_total_count/total_words * 100, 2)\n",
    "        \n",
    "#         df_conf_words = pd.DataFrame(ext_sim_words_key, columns=['words','values']) #데이터프레임으로 변환\n",
    "#         df_r = df_conf_words['words'] #words 컬럼 값 추출\n",
    "#         ext_sim_words_key = df_r.values.tolist() # 유사단어 추출\n",
    "\n",
    "        #return result_char_ratio, total_sentences, total_words, char_total_count, char_count_, ext_sim_words_key\n",
    "        ext_sim_words_key = filtered_chr_text__\n",
    "        return ext_sim_words_key\n",
    "\n",
    "\n",
    "    # 입력문장에서 맥락상 Aciton Verbs와 유사한 의미의 단어를 추출\n",
    "    ext_action_verbs = actionverb_sim_words(input_text)\n",
    "\n",
    "    #########################################################################\n",
    "    # 8.이제 입력문장에서 사용용된 Action Verbs 단어를 비교하여 추출해보자.\n",
    "\n",
    "    # Action Verbs를 모두 모음(직접적인 단어, 문맥상 유사어 포함)\n",
    "    all_ac_verbs_list = verbs_list + ext_action_verbs\n",
    "\n",
    "    #입력한 리스트 값을 하나씩 불러와서 데이터프레이에 있는지 비교 찾아내서 해당 점수를 가져오기\n",
    "    graph_calculation_list =[0]\n",
    "    get_words__ = []\n",
    "    counter= 0\n",
    "    for h in input_text_list: #데이터프레임에서 인덱스의 값과 비교하여\n",
    "        if h in all_ac_verbs_list: #df에 특정 단어가 있다면, 해당하는 컬럼의 값을 가져오기\n",
    "            get_words__.append(h) # 동일하면 저장하기\n",
    "            #print('counter :', counter)\n",
    "            graph_calculation_list.append(round(graph_calculation_list[counter]+2,2))\n",
    "            #print ('graph_calculation_list[counter]:', graph_calculation_list[counter])\n",
    "            #graph_calculation_list.append(random.randrange(1,10))\n",
    "            counter += 1\n",
    "        else: #없다면\n",
    "            #print('counter :', counter)\n",
    "            graph_calculation_list.append(round(graph_calculation_list[counter]-0.1,2)) \n",
    "            counter += 1\n",
    "    #문장에 Action Verbs 추출확인\n",
    "    #get_words__ \n",
    "\n",
    "\n",
    "    def divide_list(l, n): \n",
    "        # 리스트 l의 길이가 n이면 계속 반복\n",
    "        for i in range(0, int(len(l)), int(n)): \n",
    "            yield l[i:i + int(n)] \n",
    "        \n",
    "    # 한 리스트에 몇개씩 담을지 결정 = 20개씩\n",
    "\n",
    "    n = len(graph_calculation_list)/20\n",
    "\n",
    "    result_gr = list(divide_list(graph_calculation_list, n))\n",
    "\n",
    "    gr_cal = []\n",
    "    for regr in result_gr:\n",
    "        avg_gr = sum(regr,0.0)/len(regr) #묶어서 평균을 내고 \n",
    "        gr_cal.append(abs(round(avg_gr,2))) #절대값을 전환해서\n",
    "\n",
    "\n",
    "    graph_calculation_list = gr_cal  ## 그래프를 위한 최종결과 계산 후, 이것을 딕셔너리로 반환하여 > 그래프로 표현하기\n",
    "    #########################################################################\n",
    "    # 9. 그래프 출력 : 문장 전체를 단어로 분리하고, Action verbs가 사용된 부분을 그래프로 표시\n",
    "\n",
    "    # 전체 글에서 Action verbs가 언급된 부분을 리스트로 계산\n",
    "    # graph_calculation_list \n",
    "\n",
    "    #그래프로 표시됨\n",
    "    # plt.plot(graph_calculation_list)\n",
    "    # plt.xlabel('STORY')\n",
    "    # plt.ylabel('ACTON VERBS')\n",
    "    # plt.title('USAGE OF ACTION VERBS ANALYSIS')\n",
    "    # plt.legend(['action verbs'])\n",
    "    # plt.show()\n",
    "\n",
    "    #########################################################################\n",
    "    # 10.입력한 에세이 문장에서 Action Verbs가 얼마나 포함되어 있는지 포함비율 분석\n",
    "    action_verbs_ratio = round(len(get_words__)/len(input_text_list) *100, 3)\n",
    "\n",
    "    print (\"ACTION VERBS RATIO :\", action_verbs_ratio )\n",
    "\n",
    "\n",
    "    #########################################################################\n",
    "    # 11. 글속에 감정이 얼마나 표현되어 있는지 분석 - origin (Bert pre trained model 활용)\n",
    "    from transformers import BertTokenizer\n",
    "    from model import BertForMultiLabelClassification\n",
    "    from multilabel_pipeline import MultiLabelPipeline\n",
    "    from pprint import pprint\n",
    "\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "    model = BertForMultiLabelClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "\n",
    "    goemotions = MultiLabelPipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        threshold=0.3\n",
    "    )\n",
    "    #결과확인\n",
    "    #print(goemotions(texts))\n",
    "    ########## 여기서는 최초 입력 에세이를 적용한다. input_text !!!!!!!!\n",
    "    re_text = input_text.split(\".\")\n",
    "\n",
    "    #데이터 전처리 \n",
    "    def cleaning(datas):\n",
    "\n",
    "        fin_datas = []\n",
    "\n",
    "        for data in datas:\n",
    "            # 영문자 이외 문자는 공백으로 변환\n",
    "            only_english = re.sub('[^a-zA-Z]', ' ', data)\n",
    "        \n",
    "            # 데이터를 리스트에 추가 \n",
    "            fin_datas.append(only_english)\n",
    "\n",
    "        return fin_datas\n",
    "\n",
    "    texts = cleaning(re_text)\n",
    "\n",
    "    #분석된 감정만 추출\n",
    "    emo_re = goemotions(texts)\n",
    "\n",
    "    emo_all = []\n",
    "    for list_val in range(0, len(emo_re)):\n",
    "        #print(emo_re[list_val]['labels'],emo_re[list_val]['scores'])\n",
    "        #mo_all.append((emo_re[list_val]['labels'],emo_re[list_val]['scores'])) #KEY, VALUE만 추출하여 리스트로 저장\n",
    "        #emo_all.append(emo_re[list_val]['scores'])\n",
    "        emo_all.append((emo_re[list_val]['labels']))\n",
    "        \n",
    "    #추출결과 확인 \n",
    "    # emo_all\n",
    "\n",
    "    # ['sadness'],\n",
    "    #  ['anger'],\n",
    "    #  ['admiration', 'realization'],\n",
    "    #  ['admiration', 'disappointment'],\n",
    "    #  ['love'],\n",
    "    #  ['sadness', 'neutral'],\n",
    "    #  ['realization', 'neutral'],\n",
    "    #  ['neutral'],\n",
    "    #  ['optimism'],\n",
    "    #  ['neutral'],\n",
    "    #  ['excitement'],\n",
    "    #  ['neutral'],\n",
    "    #  ['neutral'],\n",
    "    #  ['caring'],\n",
    "    #  ['gratitude'],\n",
    "    #  ['admiration', 'approval'], ...\n",
    "\n",
    "    from pandas.core.common import flatten #이중리스틀 FLATTEN하게 변환\n",
    "    flat_list = list(flatten(emo_all))\n",
    "\n",
    "    # ['neutral',\n",
    "    #  'neutral',\n",
    "    #  'sadness',\n",
    "    #  'anger',\n",
    "    #  'admiration',\n",
    "    #  'realization',\n",
    "    #  'admiration',\n",
    "    #  'disappointment',\n",
    "\n",
    "\n",
    "    #중립적인 감정을 제외하고, 입력한 문장에서 다양한 감정을 모두 추출하고 어떤 감정이 있는지 계산해보자\n",
    "    unique = []\n",
    "    for r in flat_list:\n",
    "        if r == 'neutral':\n",
    "            pass\n",
    "        else:\n",
    "            unique.append(r)\n",
    "\n",
    "    #중립감정 제거 및 유일한 감정값 확인\n",
    "    #unique\n",
    "    unique_re = set(unique) #중복제거\n",
    "\n",
    "    ############################################################################\n",
    "    # 글에 표현된 감정이 얼마나 다양한지 분석 결과!!!¶\n",
    "    print(\"====================================================================\")\n",
    "    print(\"에세이에 표현된 다양한 감정 수:\", len(unique_re))\n",
    "    print(\"====================================================================\")\n",
    "\n",
    "    #분석가능한 감정 총 감정 수 - Bert origin model 적용시 28개 감정 추출돰\n",
    "    total_num_emotion_analyzed = 28\n",
    "\n",
    "    # 감정기복 비율 계산 !!!\n",
    "    result_emo_swings =round(len(unique_re)/total_num_emotion_analyzed *100,1) #소숫점 첫째자리만 표현\n",
    "    result_emo_swings\n",
    "    print(\"문장에 표현된 감정 비율 : \", result_emo_swings)\n",
    "    print(\"====================================================================\")\n",
    "\n",
    "\n",
    "    #########################################################################\n",
    "    # 12. SETTING RATIO 계산\n",
    "\n",
    "    def setting_anaysis(text):\n",
    "\n",
    "        essay_input_corpus = str(text) #문장입력\n",
    "        essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "        sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "        total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "        total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "        split_sentences = []\n",
    "        for sentence in sentences:\n",
    "            processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "            words = processed.split()\n",
    "            split_sentences.append(words)\n",
    "\n",
    "        skip_gram = 1\n",
    "        workers = multiprocessing.cpu_count()\n",
    "        bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "        model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "        model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "        \n",
    "        #모델 설계 완료\n",
    "\n",
    "        #setting을 표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "        location_list = ['above', 'behind','below','beside','betweed','by','in','inside','near',\n",
    "                        'on','over','through']\n",
    "        time_list = ['after', 'before','by','during','from','on','past','since','through','to','until','upon']\n",
    "        \n",
    "        movement_list = ['against','along','down','from','into','off','on','onto','out of','toward','up','upon']\n",
    "        \n",
    "        palce_terrain_type_list = ['wood', 'forest', 'copse', 'bush', 'trees', 'stand',\n",
    "                                    'swamp', 'marsh', 'wetland', 'fen', 'bog', 'moor', 'heath', 'fells', 'morass',\n",
    "                                    'jungle', 'rainforest', 'cloud forest','plains', 'fields', 'grass', 'grassland', \n",
    "                                    'savannah', 'flood plain', 'flats', 'prairie','tundra', 'iceberg', 'glacier', \n",
    "                                    'snowfields','hills', 'highland,' 'heights', 'plateau', 'badland', 'kame', 'shield',\n",
    "                                    'downs', 'downland', 'ridge', 'ridgeline','hollow,' 'valley',' vale','glen', 'dell',\n",
    "                                    'mountain', 'peak', 'summit', 'rise', 'pass', 'notch', 'crown', 'mount', 'switchback',\n",
    "                                    'furth','canyon', 'cliff', 'bluff,' 'ravine', 'gully', 'gulch', 'gorge',\n",
    "                                    'desert', 'scrub', 'waste', 'wasteland', 'sands', 'dunes',\n",
    "                                    'volcano', 'crater', 'cone', 'geyser', 'lava fields']\n",
    "        \n",
    "        water_list = ['ocean', 'sea', 'coast', 'beach', 'shore', 'strand','bay', 'port', 'harbour', 'fjord', 'vike',\n",
    "                    'cove', 'shoals', 'lagoon', 'firth', 'bight', 'sound', 'strait', 'gulf', 'inlet', 'loch', \n",
    "                    'bayou','dock', 'pier', 'anchorage', 'jetty', 'wharf', 'marina', 'landing', 'mooring', 'berth', \n",
    "                    'quay', 'staith','river', 'stream', 'creek', 'brook', 'waterway', 'rill','delta', 'bank', 'runoff',\n",
    "                    'bend', 'meander', 'backwater','lake', 'pool', 'pond', 'dugout', 'fountain', 'spring', \n",
    "                    'watering-hole', 'oasis','well', 'cistern', 'reservoir','waterfall', 'falls', 'rapids', 'cataract', \n",
    "                    'cascade','bridge', 'crossing', 'causeway', 'viaduct', 'aquaduct', 'ford', 'ferry','dam', 'dike', \n",
    "                    'bar', 'canal', 'ditch','peninsula', 'isthmus', 'island', 'isle', 'sandbar', 'reef', 'atoll', \n",
    "                    'archipelago', 'cay','shipwreck', 'derelict']\n",
    "        \n",
    "        \n",
    "        outdoor_places_list = ['clearing', 'meadow', 'grove', 'glade', 'fairy ring','earldom', 'fief', 'shire',\n",
    "                                'ruin', 'acropolis', 'desolation', 'remnant', 'remains',\n",
    "                                'henge', 'cairn', 'circle', 'mound', 'barrow', 'earthworks', 'petroglyphs',\n",
    "                                'lookout', 'aerie', 'promontory', 'outcropping', 'ledge', 'overhang', 'mesa', 'butte',\n",
    "                                'outland', 'outback', 'territory', 'reaches', 'wild', 'wilderness', 'expanse',\n",
    "                                'view', 'vista', 'tableau', 'spectacle', 'landscape', 'seascape', 'aurora', 'landmark',\n",
    "                                'battlefield', 'trenches', 'gambit', 'folly', 'conquest', 'claim', 'muster', 'post',\n",
    "                                'path', 'road', 'track', 'route', 'highway', 'way', 'trail', 'lane', 'thoroughfare', 'pike',\n",
    "                                'alley', 'street', 'avenue', 'boulevard', 'promenade', 'esplande', 'boardwalk',\n",
    "                                'crossroad', 'junction', 'intersection', 'turn', 'corner','plaza', 'terrace', 'square', \n",
    "                                'courtyard', 'court', 'park', 'marketplace', 'bazaar', 'fairground','realm', 'land', 'country',\n",
    "                                'nation', 'state', 'protectorate', 'empire', 'kingdom', 'principality','domain', 'dominion',\n",
    "                                'demesne', 'province', 'county', 'duchy', 'barony', 'baronetcy', 'march', 'canton']\n",
    "\n",
    "        \n",
    "        underground_list = ['pit', 'hole', 'abyss', 'sinkhole', 'crack', 'chasm', 'scar', 'rift', 'trench', 'fissure',\n",
    "                            'cavern', 'cave', 'gallery', 'grotto', 'karst',\n",
    "                            'mine', 'quarry', 'shaft', 'vein','graveyard', 'cemetery',\n",
    "                            'darkness', 'shadow', 'depths', 'void','maze', 'labyrinth'\n",
    "                            'tomb', 'grave', 'crypt', 'sepulchre', 'mausoleum', 'ossuary', 'boneyard']\n",
    "                            \n",
    "        living_places_list = ['nest', 'burrow', 'lair', 'den', 'bolt-hole', 'warren', 'roost', 'rookery', 'hibernaculum',\n",
    "                            'home', 'rest', 'hideout', 'hideaway', 'retreat', 'resting-place', 'safehouse', 'sanctuary',\n",
    "                            'respite', 'lodge','slum', 'shantytown', 'ghetto','camp', 'meeting place,' 'bivouac', 'campsite', \n",
    "                            'encampment','tepee', 'tent', 'wigwam', 'shelter', 'lean-to', 'yurt','house', 'mansion', 'estate',\n",
    "                            'villa','hut', 'palace', 'outbuilding', 'shack tenement', 'hovel', 'manse', 'manor', 'longhouse',\n",
    "                            'cottage', 'cabin','parsonage', 'rectory', 'vicarge', 'friary', 'priory','abbey', 'monastery', \n",
    "                            'nunnery', 'cloister', 'convent', 'hermitage','castle', 'keep', 'fort', 'fortress', 'citadel', \n",
    "                            'bailey', 'motte', 'stronghold', 'hold', 'chateau', 'outpost', 'redoubt',\n",
    "                            'town', 'village', 'hamlet', 'city', 'metropolis','settlement', 'commune']\n",
    "\n",
    "        building_facilities_list = ['temple', 'shrine', 'church', 'cathedral', 'tabernacle', 'ark', 'sanctum', 'parish', 'university',\n",
    "                                    'chapel', 'synagogue', 'mosque','pyramid', 'ziggurat', 'prison', 'jail', 'dungeon',\n",
    "                                    'oubliette', 'hospital', 'hospice', 'stocks', 'gallows','asylum', 'madhouse', 'bedlam',\n",
    "                                    'vault', 'treasury', 'warehouse', 'cellar', 'relicry', 'repository',\n",
    "                                    'barracks', 'armoury','sewer', 'gutter', 'catacombs', 'dump', 'middens', 'pipes', 'baths', 'heap',\n",
    "                                    'mill', 'windmill', 'sawmill', 'smithy', 'forge', 'workshop', 'brickyard', 'shipyard', 'forgeworks',\n",
    "                                    'foundry','bakery', 'brewery', 'almshouse', 'counting house', 'courthouse', 'apothecary', 'haberdashery', 'cobbler',\n",
    "                                    'garden', 'menagerie', 'zoo', 'aquarium', 'terrarium', 'conservatory', 'lawn', 'greenhouse',\n",
    "                                    'farm', 'orchard', 'vineyard', 'ranch', 'apiary', 'farmstead', 'homestead',\n",
    "                                    'pasture', 'commons', 'granary', 'silo', 'crop','barn', 'stable', 'pen', 'kennel', 'mews', 'hutch', \n",
    "                                    'pound', 'coop', 'stockade', 'yard', 'lumber yard','tavern', 'inn', 'pub', 'brothel', 'whorehouse',\n",
    "                                    'cathouse', 'discotheque','lighthouse', 'beacon','amphitheatre', 'colosseum', 'stadium', 'arena', \n",
    "                                    'circus','academy', 'university', 'campus', 'college', 'library', 'scriptorium', 'laboratory', \n",
    "                                    'observatory', 'museum']\n",
    "        \n",
    "        \n",
    "        architecture_list = ['hall', 'chamber', 'room','nave', 'aisle', 'vestibule',\n",
    "                            'antechamber', 'chantry', 'pulpit','dome', 'arch', 'colonnade',\n",
    "                            'stair', 'ladder', 'climb', 'ramp', 'steps',\n",
    "                            'portal', 'mouth', 'opening', 'door', 'gate', 'entrance', 'maw',\n",
    "                            'tunnel', 'passage', 'corridor', 'hallway', 'chute', 'slide', 'tube', 'trapdoor',\n",
    "                            'tower', 'turret', 'belfry','wall', 'fortifications', 'ramparts', 'pallisade', 'battlements',\n",
    "                            'portcullis', 'barbican','throne room', 'ballroom','roof', 'rooftops', 'chimney', 'attic',\n",
    "                            'loft', 'gable', 'eaves', 'belvedere','balcony', 'balustrade', 'parapet', 'walkway', 'catwalk',\n",
    "                            'pavillion', 'pagoda', 'gazebo','mirror', 'glass', 'mere','throne', 'seat', 'dais',\n",
    "                            'pillar', 'column', 'stone', 'spike', 'rock', 'megalith', 'menhir', 'dolmen', 'obelisk',\n",
    "                            'statue', 'giant', 'head', 'arm', 'leg', 'body', 'chest', 'body', 'face', 'visage', 'gargoyle', 'grotesque',\n",
    "                            'fire', 'flame', 'bonfire', 'hearth', 'fireplace', 'furnace', 'stove','window', 'grate', 'peephole', \n",
    "                            'arrowslit', 'slit', 'balistraria', 'lancet', 'aperture', 'dormerl']\n",
    "        \n",
    "        \n",
    "        setting_words_filter_list = location_list + time_list + movement_list + palce_terrain_type_list + water_list + outdoor_places_list + underground_list + underground_list + living_places_list + building_facilities_list + architecture_list\n",
    "\n",
    "        \n",
    "        ####문장에 setting_words_filter_list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "        #우선 토큰화한다.\n",
    "        retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "        token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "        #print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "        #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "        filtered_setting_text = []\n",
    "        for k in token_input_text:\n",
    "            for j in setting_words_filter_list:\n",
    "                if k == j:\n",
    "                    filtered_setting_text.append(j)\n",
    "        \n",
    "        #print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "        \n",
    "        filtered_setting_text_ = set(filtered_setting_text) #중복제거\n",
    "        filtered_setting_text__ = list(filtered_setting_text_) #다시 리스트로 변환\n",
    "        print (filtered_setting_text__) # 중복값 제거 확인\n",
    "        \n",
    "#         for i in filtered_setting_text__:\n",
    "#             ext_setting_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "        \n",
    "        setting_total_count = len(filtered_setting_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 setting 표현 수\n",
    "        setting_count_ = len(filtered_setting_text__) #중복제거된 setting표현 총 수\n",
    "            \n",
    "        result_setting_words_ratio = round(setting_total_count/total_words * 100, 2)\n",
    "        #return result_setting_words_ratio, total_sentences, total_words, setting_total_count, setting_count_, ext_setting_sim_words_key\n",
    "        return result_setting_words_ratio\n",
    "\n",
    "\n",
    "    # 셋팅 비율 계산\n",
    "    settig_ratio_re = setting_anaysis(input_text)\n",
    "    print(\"====================================================================\")\n",
    "    print(\"SETTING RATIO : \", settig_ratio_re)\n",
    "    print(\"====================================================================\")\n",
    "\n",
    "\n",
    "    ###################################################################################\n",
    "    # 13. PLOT COMPLEXITY 계산¶ - 캐릭터 20% + conflict 40% + 감정기복 30% + setting 10%\n",
    "    ###################################################################################\n",
    "\n",
    "    def character(text):\n",
    "\n",
    "        essay_input_corpus = str(text) #문장입력\n",
    "        essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "        sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "        total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "        total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "        \n",
    "        split_sentences = []\n",
    "        for sentence in sentences:\n",
    "            processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "            words = processed.split()\n",
    "            split_sentences.append(words)\n",
    "\n",
    "        skip_gram = 1\n",
    "        workers = multiprocessing.cpu_count()\n",
    "        bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "        model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "        model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "        \n",
    "        #모델 설계 완료\n",
    "\n",
    "        #캐릭터 표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "        character_list = ['i', 'my', 'me', 'mine', 'you', 'your', 'they','them',\n",
    "                        'yours', 'he','him','his' 'she','her','it','someone','their', 'myself', 'aunt',\n",
    "                        'brother','cousin','daughter','father','grandchild','granddaughter','granddson','grandfather',\n",
    "                        'grandmother','great-grandchild','husband','ex-husband','son-in-law', 'daughter-in-law','mother',\n",
    "                        'niece','nephew','parents','sister','son','stepfather','stepmother','stepdaughter', 'stepson',\n",
    "                        'twin','uncle','widow','widower','wife','ex-wife']\n",
    "        \n",
    "        ####문장에 char_list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "        #우선 토큰화한다.\n",
    "        retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "        token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "        #print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "        #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "        filtered_chr_text = []\n",
    "        for k in token_input_text:\n",
    "            for j in character_list:\n",
    "                if k == j:\n",
    "                    filtered_chr_text.append(j)\n",
    "        \n",
    "        #print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "        \n",
    "        filtered_chr_text_ = set(filtered_chr_text) #중복제거\n",
    "        filtered_chr_text__ = list(filtered_chr_text_) #다시 리스트로 변환\n",
    "        #print (filtered_chr_text__) # 중복값 제거 확인\n",
    "        \n",
    "        for i in filtered_chr_text__:\n",
    "            ext_sim_words_key = model.wv.most_similar_cosmul(i) #모델적용\n",
    "        \n",
    "        char_total_count = len(filtered_chr_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 캐릭터 표현 수\n",
    "        char_count_ = len(filtered_chr_text__) #중복제거된 캐릭터 표현 총 수\n",
    "            \n",
    "        result_char_ratio = round(char_total_count/total_words * 100, 2)\n",
    "\n",
    "        #return result_char_ratio, total_sentences, total_words, char_total_count, char_count_, ext_sim_words_key\n",
    "        return result_char_ratio\n",
    "\n",
    "\n",
    "    #######################################################################\n",
    "    ##########################   Plot complexity  ######################\n",
    "    #######################################################################\n",
    "    # 이제 최종 계산을 해보자.\n",
    "    # character_ratio_result #캐릭터 비율 20%\n",
    "    # result_emo_swings # 감정기복 비율 30%\n",
    "    # conflict_word_ratio #CONFLICT 비율 계산 40%\n",
    "    # settig_ratio_re #Setting 비율 계산 10%\n",
    "    # 전체 문장에서 캐릭터를 의미하는 단어나 유사어 비율 \n",
    "\n",
    "    character_ratio_result = character(input_text)\n",
    "    character_ratio_result\n",
    "    print(\"전체 문장에서 캐릭터를 의미하는 단어나 유사어 비율 :\", character_ratio_result)\n",
    "\n",
    "    ###########################################################\n",
    "    ############# Degree of Conflict  비율 계산 #################\n",
    "    conflict_word_ratio = round(len(count_conflict_list) / len(input_text_list) * 1000, 1)  \n",
    "    print(\"Degree of conflict  단어가 전체 문장(단어)에서 차지하는 비율 계산 :\", conflict_word_ratio)\n",
    "\n",
    "    global coflict_ratio\n",
    "    coflict_ratio = [conflict_word_ratio] #그래프로 표현하는 값\n",
    "\n",
    "\n",
    "\n",
    "    ###########################################################\n",
    "    ############# Emotional Rollercoaster  비율 계산 #################\n",
    "    print(\"감정기복비율 :\", result_emo_swings) \n",
    "\n",
    "    # 셋팅비율 계산\n",
    "    print(\"셋팅비율 계산 : \", settig_ratio_re)\n",
    "\n",
    "    # 4개의 값을 리스트로 담는다.\n",
    "    de_flt_list = [character_ratio_result, result_emo_swings, conflict_word_ratio, settig_ratio_re]\n",
    "\n",
    "\n",
    "    import numpy\n",
    "    numpy.mean(de_flt_list) #평균\n",
    "    numpy.var(de_flt_list) #분산\n",
    "    numpy.std(de_flt_list) #표준편차\n",
    "\n",
    "    #######################################################################\n",
    "    ############# Plot complexity  st_input 표준편차 비율 계산 #################\n",
    "    de_flt_list_ = [character_ratio_result*2, result_emo_swings*3, conflict_word_ratio*4, settig_ratio_re]\n",
    "    numpy.mean(de_flt_list_) # 평균\n",
    "    numpy.var(de_flt_list_) # 분산\n",
    "    st_input = numpy.std(de_flt_list_) # 표준편차 ----> 이 값으로 계산\n",
    "    print(\"Plot Complxity :\", st_input )\n",
    "\n",
    "    global plot_comp_ratio\n",
    "\n",
    "    plot_comp_ratio = [round(st_input, 2)]\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"===============================================================================\")\n",
    "    print(\"======================      Degree of Conflict   ==============================\")\n",
    "    print(\"===============================================================================\")\n",
    "\n",
    "    \n",
    "    # return 값 설명  ====  \n",
    "#     plot complexity :st_input \n",
    "#     emotion rollercoster: result_emo_swings\n",
    "#     degree of conflict: conflict_word_ratio\n",
    "#     count_conflict_list : 컨플릭 단어들 리스트\n",
    "    \n",
    "    return st_input, result_emo_swings, conflict_word_ratio,df_sent, graph_calculation_list\n",
    "\n",
    "\n",
    "#     return { \n",
    "\n",
    "#             \"result_all_plot\":result_all_avg, \n",
    "#             \"emotional_rollercoaster\":result_emo_swings, \n",
    "#             \"plot_complexity\":st_input, \n",
    "#             \"degree_conflict\": conflict_word_ratio, \n",
    "#             \"result_emotional_rollercoaster\": result_emotional_rollwercoaster,\n",
    "#             \"result_plot_complexity\" : result_plot_complexity,\n",
    "#             \"result_degree_conflict\" : result_degree_conflict,\n",
    "            \n",
    "#             \"neg\" : df_sent[\"neg\"],\n",
    "#             \"neu\" : df_sent[\"neu\"],\n",
    "#             \"pos\" : df_sent[\"pos\"],\n",
    "#             \"compound\" : df_sent[\"compound\"],\n",
    "#             \"graph_calculation_list\" : graph_calculation_list\n",
    "            \n",
    "#             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ai_plot_coflict_total_analysis(input_text):\n",
    "\n",
    "    plot_conf_re = ai_plot_conf(input_text)\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"1명의 에세이 결과 계산점수 :\", plot_conf_re)\n",
    "    #1명의 에세이 결과 계산점수 : (28.602484157848945, 25.0, 0.3,df_sent)\n",
    "\n",
    "    # 위에서 계산한 총 4개의 값을 개인, 그룹의 값과 비교하여 lacking, ideal, overboard 계산\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 개인에세이 값 계산 4가지 결과 추출 >>> personal_value 로 입력됨\n",
    "    plot_complexity = plot_conf_re[0]\n",
    "    emotional_rollercoaster = plot_conf_re[1]\n",
    "    degree_conflict = plot_conf_re[2]\n",
    "    \n",
    "    graph_calculation_list = plot_conf_re[4]\n",
    "\n",
    "\n",
    "    ########################################################\n",
    "    ## 1000명 데이터의 각 값(char_desc_mean)의 평균 값 전달.>>> 고정값으로 미리 계산하여 입력 ai_plot_coflict_1000data_preprocessing 코드 참조\n",
    "    plot_conflict_all_mean = [80, 64, 0.314]\n",
    "    group_db_fin_result_plot = [5.0]\n",
    "    ########################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plot_complexity_mean = plot_conflict_all_mean[0] #첫번째 값을 가져옴\n",
    "    emotional_rollercoaster_mean = plot_conflict_all_mean[1] #\n",
    "    degree_conflict_mean = plot_conflict_all_mean[2] #\n",
    "\n",
    "\n",
    "    def lackigIdealOverboard(group_mean, personal_value): # group_mean: 1000명 평균, personal_value|:개인값\n",
    "        ideal_mean = group_mean\n",
    "        one_ps_char_desc = personal_value\n",
    "        #최대, 최소값 기준으로 구간설정. 구간비율 30% => 0.3으로 설정\n",
    "        min_ = int(ideal_mean-ideal_mean*0.6)\n",
    "        print('min_', min_)\n",
    "        max_ = int(ideal_mean+ideal_mean*0.6)\n",
    "        print('max_: ', max_)\n",
    "        div_ = int(((ideal_mean+ideal_mean*0.6)-(ideal_mean-ideal_mean*0.6))/3)\n",
    "        print('div_:', div_)\n",
    "\n",
    "        #결과 판단 Lacking, Ideal, Overboard\n",
    "        cal_abs = abs(ideal_mean - one_ps_char_desc) # 개인 - 단체 값의 절대값계산\n",
    "\n",
    "        print('cal_abs 절대값 :', cal_abs)\n",
    "        compare7 = (one_ps_char_desc + ideal_mean)/6\n",
    "        compare6 = (one_ps_char_desc + ideal_mean)/5\n",
    "        compare5 = (one_ps_char_desc + ideal_mean)/4\n",
    "        compare4 = (one_ps_char_desc + ideal_mean)/3\n",
    "        compare3 = (one_ps_char_desc + ideal_mean)/2\n",
    "        print('compare7 :', compare7)\n",
    "        print('compare6 :', compare6)\n",
    "        print('compare5 :', compare5)\n",
    "        print('compare4 :', compare4)\n",
    "        print('compare3 :', compare3)\n",
    "\n",
    "\n",
    "\n",
    "        if one_ps_char_desc > ideal_mean: # 개인점수가 평균보다 클 경우는 overboard\n",
    "            if cal_abs > compare3: # 37 개인점수가 개인평균차의 절대값보다 클 경우, 즉 차이가 많이 날경우\n",
    "                print(\"Overboard: 2\")\n",
    "                result = 2 #overboard\n",
    "                score = 1\n",
    "            elif cal_abs > compare4: # 28\n",
    "                print(\"Overvoard: 2\")\n",
    "                result = 2\n",
    "                score = 2\n",
    "            elif cal_abs > compare5: # 22\n",
    "                print(\"Overvoard: 2\")\n",
    "                result = 2\n",
    "                score = 3\n",
    "            elif cal_abs > compare6: # 18\n",
    "                print(\"Overvoard: 2\")\n",
    "                result = 2\n",
    "                score = 4\n",
    "            else:\n",
    "                print(\"Ideal: 1\")\n",
    "                result = 1\n",
    "                score = 5\n",
    "        elif one_ps_char_desc < ideal_mean: # 개인점수가 평균보다 작을 경우 lacking\n",
    "            if cal_abs > compare3: # 37 개인점수가 개인평균차의 절대값보다 클 경우, 즉 차이가 많이 날경우\n",
    "                print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                score = 1\n",
    "            elif cal_abs > compare4: # 28\n",
    "                print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                score = 2\n",
    "            elif cal_abs > compare5: # 22\n",
    "                print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                score = 3\n",
    "            elif cal_abs > compare6: # 18\n",
    "                print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                score = 4\n",
    "            else:\n",
    "                print(\"Ideal: 1\")\n",
    "                result = 1\n",
    "                score = 5\n",
    "                \n",
    "        else:\n",
    "            print(\"Ideal: 1\")\n",
    "            result = 1\n",
    "            score = 5\n",
    "\n",
    "        return result, score\n",
    "\n",
    "\n",
    "    plot_complexity_result = lackigIdealOverboard(plot_complexity_mean, plot_complexity)\n",
    "    emotional_rollercoaster_result = lackigIdealOverboard(emotional_rollercoaster_mean, emotional_rollercoaster)\n",
    "    degree_conflict_result = lackigIdealOverboard(degree_conflict_mean, degree_conflict)\n",
    "\n",
    "    fin_result = [plot_complexity_result, emotional_rollercoaster_result, degree_conflict_result]\n",
    "    print(\"fin_result:\", fin_result)  # [(0:lacking, 1:score), (0:lacking, 2:score), (2:overboard, 1:score)]\n",
    "\n",
    "    each_fin_result = [fin_result[0][0], fin_result[1][0], fin_result[2][0]]\n",
    "\n",
    "    # 최종 character  전체 점수 계산\n",
    "    overall_character_rating = [round((fin_result[0][1]+ fin_result[1][1] + fin_result[2][1])/3,2)]\n",
    "\n",
    "    result_final = each_fin_result + overall_character_rating + group_db_fin_result_plot + coflict_ratio + plot_comp_ratio\n",
    "\n",
    "    df_sent = plot_conf_re[3]\n",
    "    \n",
    "    neg =  list(map(float, df_sent[\"neg\"]))\n",
    "    neu =  list(map(float, df_sent[\"neu\"]))\n",
    "    pos =  list(map(float, df_sent[\"pos\"]))\n",
    "    compound =  list(map(float, df_sent[\"compound\"]))\n",
    "    \n",
    "    print(df_sent)\n",
    "    \n",
    "    print(\"neg>>>>>\",neg)\n",
    "    print(\"neu>>>>>\",neu)\n",
    "    print(\"pos>>>>>\",pos)\n",
    "    print(\"componud>>>\",compound)\n",
    "    \n",
    "\n",
    "    print ( \"graph_calculation_list\" , graph_calculation_list) \n",
    "\n",
    "\n",
    "    data = {\n",
    "        \n",
    "            \"result_all_plot\":result_final[3], \n",
    "            \n",
    "            \"emotional_rollercoaster\":round(emotional_rollercoaster,2), \n",
    "            \"plot_complexity\":round(plot_complexity,2), \n",
    "            \"degree_conflict\": round(degree_conflict,2), \n",
    "            \n",
    "            \"result_plot_complexity\" : result_final[0],\n",
    "            \"result_emotional_rollercoaster\": result_final[1],\n",
    "            \"result_degree_conflict\" : result_final[2],\n",
    "            \n",
    "            \"neg\" : neg,\n",
    "            \"neu\" : neu,\n",
    "            \"pos\" : pos,\n",
    "            \"compound\" : compound,\n",
    "            \"graph_calculation_list\" : graph_calculation_list\n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    return data \n",
    "\n",
    "    #  [0, 0, 2, 1.33, 5.0, 3.0, 26.52]\n",
    "\n",
    "    # [0: plot_complexity_result-lacking, \n",
    "    #  0: emotional_rollercoaster_result - lacking, \n",
    "    #  2: degree_conflict_result-overboard, \n",
    "    #  1.3: overall_character_rating,\n",
    "    #  5.0: group_db_fin_result_plot(1000명 평균값), \n",
    "    #  3.0: conflict_ratio] ----------------> Conflict\n",
    "    #  26.52] : ----------------------------> Plot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/gensim/models/base_any2vec.py:323: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n",
      "100%|██████████| 24/24 [00:00<00:00, 4142.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_list_str: ['bloomington', 'normal', 'is', 'almost', 'laughably', 'cliché', 'for', 'a', 'midwestern', 'city', 'vast', 'swathes', 'of', 'corn', 'envelop', 'winding', 'roads', 'and', 'the', 'heady', 'smell', 'of', 'bbq', 'smoke', 'pervades', 'the', 'countryside', 'every', 'summer', 'yet', 'underlying', 'the', 'trite', 'norms', 'of', 'normal', 'is', 'the', 'prescriptive', 'force', 'of', 'tradition—the', 'expectation', 'to', 'fulfill', 'my', 'role', 'as', 'a', 'female', 'filipino', 'by', 'playing', 'debussy', 'in', 'the', 'yearly', 'piano', 'festival', 'and', 'enrolling', 'in', 'multivariable', 'calculus', 'instead', 'of', 'political', 'philosophy', 'so', 'when', 'i', 'discovered', 'the', 'technical', 'demand', 'of', 'bebop', 'the', 'triplet', 'groove', 'and', 'the', 'intricacies', 'of', 'chordal', 'harmony', 'after', 'ten', 'years', 'of', 'grueling', 'classical', 'piano', 'i', 'was', 'fascinated', 'by', 'the', \"music's\", 'novelty', 'jazz', 'guitar', 'was', 'not', 'only', 'evocative', 'and', 'creative', 'but', 'also', 'strangely', 'liberating', 'i', 'began', 'to', 'explore', 'different', 'pedagogical', 'methods', 'transcribe', 'solos', 'from', 'the', 'greats', 'and', 'experiment', 'with', 'various', 'approaches', 'until', 'my', 'own', 'unique', 'sound', 'began', 'to', 'develop', 'and', 'although', 'i', 'did', 'not', 'know', 'what', 'would', 'be', 'the', \"'best'\", 'route', 'for', 'me', 'to', 'follow', 'as', 'a', 'musician', 'the', 'freedom', 'to', 'forge', 'whatever', 'path', 'i', 'felt', 'was', 'right', 'seemed', 'to', 'be', 'exactly', 'what', 'i', 'needed', 'there', 'were', 'no', 'expectations', 'for', 'me', 'to', 'continue', 'in', 'any', 'particular', 'way—only', 'the', 'way', 'that', 'suited', 'my', 'own', 'desires', 'while', 'journeying', 'this', 'trail', 'i', 'found', 'myself', 'at', 'interlochen', 'arts', 'camp', 'the', 'summer', 'before', 'my', 'junior', 'year', 'never', 'before', 'had', 'i', 'been', 'immersed', 'in', 'an', 'environment', 'so', 'conducive', 'to', 'musical', 'growth', 'i', 'was', 'surrounded', 'by', 'people', 'intensely', 'passionate', 'about', 'pursuing', 'all', 'kinds', 'of', 'art', 'with', 'no', 'regard', 'for', 'ideas', 'of', 'what', 'art', \"'should'\", 'be', 'i', 'knew', 'immediately', 'that', 'this', 'would', 'be', 'a', 'perfect', 'opportunity', 'to', 'cultivate', 'my', 'sound', 'unbounded', 'by', 'the', 'limits', 'of', 'confining', 'tradition', 'on', 'the', 'first', 'day', 'of', 'camp', 'i', 'found', 'that', 'my', 'peer', 'guitarist', 'in', 'big', 'band', 'was', 'another', 'filipino', 'girl', 'from', 'illinois', 'until', 'that', 'moment', 'my', 'endeavors', 'in', 'jazz', 'guitar', 'had', 'been', 'a', 'solitary', 'effort', 'i', 'had', 'no', 'one', 'with', 'whom', 'to', 'collaborate', 'and', 'no', 'one', 'against', 'whom', 'i', 'could', 'compare', 'myself', 'much', 'less', 'someone', 'from', 'a', 'background', 'mirroring', 'my', 'own', 'i', 'was', 'eager', 'to', 'play', 'with', 'her', 'but', 'while', 'i', 'quickly', 'recognized', 'a', 'slew', 'of', 'differences', 'between', 'us—different', 'heights', 'guitars', 'and', 'even', 'playing', 'styles—others', 'seemed', 'to', 'have', 'trouble', 'making', 'that', 'distinction', 'during', 'performances', 'some', 'even', 'went', 'as', 'far', 'as', 'calling', 'me', \"'other\", 'francesca', \"'\", 'thus', 'amidst', 'the', 'glittering', 'lakes', 'and', 'musky', 'pine', 'needles', 'of', 'interlochen', 'i', 'once', 'again', 'confronted', \"bloomington's\", 'frustrating', 'expectations', 'after', 'being', 'mistaken', 'for', 'her', 'several', 'times', 'i', 'could', 'not', 'help', 'but', 'view', 'francesca', 'as', 'a', 'standard', 'of', 'what', 'the', \"'female\", 'filipino', 'jazz', \"guitarist'\", 'should', 'embody', 'her', 'improvisatory', 'language', 'comping', 'style', 'and', 'even', 'personal', 'qualities', 'loomed', 'above', 'me', 'as', 'something', 'i', 'had', 'to', 'live', 'up', 'to', 'nevertheless', 'as', 'francesca', 'and', 'i', 'continued', 'to', 'play', 'together', 'it', 'was', 'not', 'long', 'before', 'we', 'connected', 'through', 'our', 'creative', 'pursuit', 'in', 'time', 'i', 'learned', 'to', 'draw', 'inspiration', 'from', 'her', 'instead', 'of', 'feeling', 'pressured', 'to', 'follow', 'whatever', 'precedent', 'i', 'thought', 'she', 'set', 'i', 'found', 'that', 'i', 'grew', 'because', 'of', 'rather', 'than', 'in', 'spite', 'of', 'her', 'presence', 'i', 'could', 'find', 'solace', 'in', 'our', 'similarities', 'and', 'even', 'a', 'sense', 'of', 'comfort', 'in', 'an', 'unfamiliar', 'environment', 'without', 'being', 'trapped', 'by', 'expectation', 'though', 'the', 'pressure', 'to', 'conform', 'was', 'still', 'present—and', 'will', 'likely', 'remain', 'present', 'in', 'my', 'life', 'no', 'matter', 'what', 'genre', \"i'm\", 'playing', 'or', 'what', 'pursuits', 'i', 'engage', 'in—i', 'learned', 'to', 'eschew', 'its', 'corrosive', 'influence', 'and', 'enjoy', 'the', 'rewards', 'that', 'it', 'brings', 'while', 'my', 'encounter', 'with', 'francesca', 'at', 'first', 'sparked', 'a', 'feeling', 'of', 'pressure', 'to', 'conform', 'in', 'a', 'setting', 'where', 'i', 'never', 'thought', 'i', 'would', 'feel', 'its', 'presence', 'it', 'also', 'carried', 'the', 'warmth', 'of', 'finding', 'someone', 'with', 'whom', 'i', 'could', 'connect', 'like', 'the', 'admittedly', 'trite', 'conditions', 'of', 'my', 'hometown', 'the', 'resemblances', 'between', 'us', 'provided', 'comfort', 'to', 'me', 'through', 'their', 'familiarity', 'i', 'ultimately', 'found', 'that', 'i', 'can', 'embrace', 'this', 'warmth', 'while', 'still', 'rejecting', 'the', 'pressure', 'to', 'succumb', 'to', 'expectations', 'and', 'that', 'in', 'the', 'careful', 'balance', 'between', 'these', 'elements', 'i', 'can', 'grow', 'in', 'a', 'way', 'that', 'feels', 'both', 'like', 'discove']\n",
      "confict_words_list: {'odds', 'contradictory', 'irreconcilable', 'at odds', 'differ', 'opposing', 'vary', 'in opposition', 'contrast', 'incongruous', 'clash', 'divergent', 'collide', 'variance', 'different', 'clashing', 'inconsistent', 'incompatible', 'diverge', 'discordant', 'contrary', 'contrasting', 'opposition', 'differing', 'varying', 'antithetical', 'opposed', 'at variance', 'opposite', 'disagree', 'discrepant', 'conflict', 'disagreeing'}\n",
      "문장에 들어있는 추출된 conflict 단어들: []\n",
      "전체문장에 들어있는 conflict 단어 수: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTION VERBS RATIO : 4.268\n",
      "====================================================================\n",
      "에세이에 표현된 다양한 감정 수: 7\n",
      "====================================================================\n",
      "문장에 표현된 감정 비율 :  25.0\n",
      "====================================================================\n",
      "['camp', 'through', 'route', 'forge', 'after', 'to', 'trail', 'before', 'during', 'above', 'until', 'view', 'way', 'on', 'city', 'against', 'sound', 'path', 'in', 'up', 'from', 'by']\n",
      "====================================================================\n",
      "SETTING RATIO :  12.34\n",
      "====================================================================\n",
      "전체 문장에서 캐릭터를 의미하는 단어나 유사어 비율 : 8.79\n",
      "Degree of conflict  단어가 전체 문장(단어)에서 차지하는 비율 계산 : 1.5\n",
      "감정기복비율 : 25.0\n",
      "셋팅비율 계산 :  12.34\n",
      "Plot Complxity : 27.59764663879875\n",
      "===============================================================================\n",
      "======================      Degree of Conflict   ==============================\n",
      "===============================================================================\n",
      "1명의 에세이 결과 계산점수 : (27.59764663879875, 25.0, 1.5,       neg    neu    pos  compound comp_score\n",
      "0   0.000  0.808  0.192    0.2280          1\n",
      "1   0.000  1.000  0.000    0.0000          1\n",
      "2   0.041  0.778  0.181    0.7269          1\n",
      "3   0.044  0.787  0.169    0.6486          1\n",
      "4   0.190  0.678  0.132   -0.2144         -1\n",
      "5   0.000  1.000  0.000    0.0000          1\n",
      "6   0.040  0.884  0.076    0.4588          1\n",
      "7   0.000  1.000  0.000    0.0000          1\n",
      "8   0.054  0.790  0.155    0.6240          1\n",
      "9   0.000  0.723  0.277    0.7579          1\n",
      "10  0.000  1.000  0.000    0.0000          1\n",
      "11  0.118  0.882  0.000   -0.5267         -1\n",
      "12  0.101  0.739  0.161    0.0258          1\n",
      "13  0.000  1.000  0.000    0.0000          1\n",
      "14  0.239  0.761  0.000   -0.5719         -1\n",
      "15  0.133  0.867  0.000   -0.3354         -1\n",
      "16  0.104  0.896  0.000   -0.2732         -1\n",
      "17  0.000  0.762  0.238    0.6486          1\n",
      "18  0.083  0.702  0.215    0.4588          1\n",
      "19  0.090  0.770  0.140    0.2206          1\n",
      "20  0.092  0.667  0.242    0.7351          1\n",
      "21  0.058  0.822  0.119    0.3182          1\n",
      "22  0.079  0.702  0.219    0.4939          1\n",
      "23  0.121  0.650  0.230    0.5719          1, [0.83, 1.07, 3.75, 5.9, 2.47, 2.98, 4.54, 7.15, 5.95, 6.72, 7.23, 9.45, 11.93, 13.75, 13.93, 10.7, 8.58, 6.21, 7.17, 8.6, 7.24])\n",
      "min_ 32\n",
      "max_:  128\n",
      "div_: 32\n",
      "cal_abs 절대값 : 52.40235336120125\n",
      "compare7 : 17.93294110646646\n",
      "compare6 : 21.51952932775975\n",
      "compare5 : 26.899411659699688\n",
      "compare4 : 35.86588221293292\n",
      "compare3 : 53.798823319399375\n",
      "Lacking: 2\n",
      "min_ 25\n",
      "max_:  102\n",
      "div_: 25\n",
      "cal_abs 절대값 : 39.0\n",
      "compare7 : 14.833333333333334\n",
      "compare6 : 17.8\n",
      "compare5 : 22.25\n",
      "compare4 : 29.666666666666668\n",
      "compare3 : 44.5\n",
      "Lacking: 2\n",
      "min_ 0\n",
      "max_:  0\n",
      "div_: 0\n",
      "cal_abs 절대값 : 1.186\n",
      "compare7 : 0.30233333333333334\n",
      "compare6 : 0.3628\n",
      "compare5 : 0.4535\n",
      "compare4 : 0.6046666666666667\n",
      "compare3 : 0.907\n",
      "Overboard: 2\n",
      "fin_result: [(0, 2), (0, 2), (2, 1)]\n",
      "      neg    neu    pos  compound comp_score\n",
      "0   0.000  0.808  0.192    0.2280          1\n",
      "1   0.000  1.000  0.000    0.0000          1\n",
      "2   0.041  0.778  0.181    0.7269          1\n",
      "3   0.044  0.787  0.169    0.6486          1\n",
      "4   0.190  0.678  0.132   -0.2144         -1\n",
      "5   0.000  1.000  0.000    0.0000          1\n",
      "6   0.040  0.884  0.076    0.4588          1\n",
      "7   0.000  1.000  0.000    0.0000          1\n",
      "8   0.054  0.790  0.155    0.6240          1\n",
      "9   0.000  0.723  0.277    0.7579          1\n",
      "10  0.000  1.000  0.000    0.0000          1\n",
      "11  0.118  0.882  0.000   -0.5267         -1\n",
      "12  0.101  0.739  0.161    0.0258          1\n",
      "13  0.000  1.000  0.000    0.0000          1\n",
      "14  0.239  0.761  0.000   -0.5719         -1\n",
      "15  0.133  0.867  0.000   -0.3354         -1\n",
      "16  0.104  0.896  0.000   -0.2732         -1\n",
      "17  0.000  0.762  0.238    0.6486          1\n",
      "18  0.083  0.702  0.215    0.4588          1\n",
      "19  0.090  0.770  0.140    0.2206          1\n",
      "20  0.092  0.667  0.242    0.7351          1\n",
      "21  0.058  0.822  0.119    0.3182          1\n",
      "22  0.079  0.702  0.219    0.4939          1\n",
      "23  0.121  0.650  0.230    0.5719          1\n",
      "neg>>>>> [0.0, 0.0, 0.041, 0.044, 0.19, 0.0, 0.04, 0.0, 0.054, 0.0, 0.0, 0.118, 0.101, 0.0, 0.239, 0.133, 0.104, 0.0, 0.083, 0.09, 0.092, 0.058, 0.079, 0.121]\n",
      "neu>>>>> [0.808, 1.0, 0.778, 0.787, 0.678, 1.0, 0.884, 1.0, 0.79, 0.723, 1.0, 0.882, 0.739, 1.0, 0.761, 0.867, 0.896, 0.762, 0.702, 0.77, 0.667, 0.822, 0.702, 0.65]\n",
      "pos>>>>> [0.192, 0.0, 0.181, 0.169, 0.132, 0.0, 0.076, 0.0, 0.155, 0.277, 0.0, 0.0, 0.161, 0.0, 0.0, 0.0, 0.0, 0.238, 0.215, 0.14, 0.242, 0.119, 0.219, 0.23]\n",
      "componud>>> [0.228, 0.0, 0.7269, 0.6486, -0.2144, 0.0, 0.4588, 0.0, 0.624, 0.7579, 0.0, -0.5267, 0.0258, 0.0, -0.5719, -0.3354, -0.2732, 0.6486, 0.4588, 0.2206, 0.7351, 0.3182, 0.4939, 0.5719]\n",
      "graph_calculation_list [0.83, 1.07, 3.75, 5.9, 2.47, 2.98, 4.54, 7.15, 5.95, 6.72, 7.23, 9.45, 11.93, 13.75, 13.93, 10.7, 8.58, 6.21, 7.17, 8.6, 7.24]\n",
      "result:  {'result_all_plot': 1.67, 'emotional_rollercoaster': 25.0, 'plot_complexity': 27.6, 'degree_conflict': 1.5, 'result_plot_complexity': 0, 'result_emotional_rollercoaster': 0, 'result_degree_conflict': 2, 'neg': [0.0, 0.0, 0.041, 0.044, 0.19, 0.0, 0.04, 0.0, 0.054, 0.0, 0.0, 0.118, 0.101, 0.0, 0.239, 0.133, 0.104, 0.0, 0.083, 0.09, 0.092, 0.058, 0.079, 0.121], 'neu': [0.808, 1.0, 0.778, 0.787, 0.678, 1.0, 0.884, 1.0, 0.79, 0.723, 1.0, 0.882, 0.739, 1.0, 0.761, 0.867, 0.896, 0.762, 0.702, 0.77, 0.667, 0.822, 0.702, 0.65], 'pos': [0.192, 0.0, 0.181, 0.169, 0.132, 0.0, 0.076, 0.0, 0.155, 0.277, 0.0, 0.0, 0.161, 0.0, 0.0, 0.0, 0.0, 0.238, 0.215, 0.14, 0.242, 0.119, 0.219, 0.23], 'compound': [0.228, 0.0, 0.7269, 0.6486, -0.2144, 0.0, 0.4588, 0.0, 0.624, 0.7579, 0.0, -0.5267, 0.0258, 0.0, -0.5719, -0.3354, -0.2732, 0.6486, 0.4588, 0.2206, 0.7351, 0.3182, 0.4939, 0.5719], 'graph_calculation_list': [0.83, 1.07, 3.75, 5.9, 2.47, 2.98, 4.54, 7.15, 5.95, 6.72, 7.23, 9.45, 11.93, 13.75, 13.93, 10.7, 8.58, 6.21, 7.17, 8.6, 7.24]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "###### 실행 테스트  ######\n",
    "print(\"result: \",ai_plot_coflict_total_analysis(input_text))\n",
    "\n",
    "\n",
    "### {'result_all_plot': 1.33, 'emotional_rollercoaster': 25.0, 'plot_complexity': 26.52, 'degree_conflict': 3.0, 'result_plot_complexity': 0, 'result_emotional_rollercoaster': 0, 'result_degree_conflict': 2, 'neg': [0.0, 0.0, 0.041, 0.044, 0.19, 0.0, 0.04, 0.0, 0.054, 0.0, 0.0, 0.118, 0.101, 0.0, 0.239, 0.133, 0.104, 0.0, 0.083, 0.09, 0.092, 0.058, 0.079, 0.121], 'neu': [0.808, 1.0, 0.778, 0.787, 0.678, 1.0, 0.884, 1.0, 0.79, 0.723, 1.0, 0.882, 0.739, 1.0, 0.761, 0.867, 0.896, 0.762, 0.702, 0.77, 0.667, 0.822, 0.702, 0.65], 'pos': [0.192, 0.0, 0.181, 0.169, 0.132, 0.0, 0.076, 0.0, 0.155, 0.277, 0.0, 0.0, 0.161, 0.0, 0.0, 0.0, 0.0, 0.238, 0.215, 0.14, 0.242, 0.119, 0.219, 0.23], 'compound': [0.228, 0.0, 0.7269, 0.6486, -0.2144, 0.0, 0.4588, 0.0, 0.624, 0.7579, 0.0, -0.5267, 0.0258, 0.0, -0.5719, -0.3354, -0.2732, 0.6486, 0.4588, 0.2206, 0.7351, 0.3182, 0.4939, 0.5719], 'graph_calculation_list': [0, -0.1, -0.2, -0.3, -0.4, -0.5, -0.6, -0.7, -0.8, -0.9, -1.0, -1.1, -1.2, -1.3, -1.4, -1.5, -1.6, -1.7, -1.8, -1.9, -2.0, 0.0, -0.1, -0.2, -0.3, -0.4, -0.5, -0.6, -0.7, -0.8, -0.9, -1.0, -1.1, -1.2, -1.3, -1.4, -1.5, -1.6, -1.7, -1.8, 0.2, 0.1, 0.0, -0.1, -0.2, -0.3, -0.4, -0.5, -0.6, -0.7, -0.8, -0.9, -1.0, -1.1, -1.2, -1.3, -1.4, -1.5, -1.6, -1.7, -1.8, -1.9, -2.0, -2.1, -2.2, -2.3, -2.4, -2.5, -2.6, -2.7, -2.8, -2.9, -3.0, -3.1, -3.2, -3.3, -3.4, -3.5, -3.6, -3.7, -3.8, -3.9, -4.0, -4.1, -4.2, -4.3, -4.4, -4.5, -4.6, -4.7, -4.8, -4.9, -5.0, -5.1, -5.2, -5.3, -5.4, -5.5, -5.6, -5.7, -5.8, -5.9, -6.0, -6.1, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8, -6.9, -7.0, -7.1, -7.2, -7.3, -7.4, -7.5, -5.5, -5.6, -5.7, -5.8, -3.8, -3.9, -4.0, -4.1, -4.2, -4.3, -2.3, -2.4, -2.5, -2.6, -2.7, -2.8, -2.9, -3.0, -3.1, -3.2, -3.3, -1.3, -1.4, -1.5, -1.6, -1.7, -1.8, -1.9, -2.0, -2.1, -2.2, -2.3, -2.4, -2.5, -2.6, -2.7, -2.8, -2.9, -3.0, -3.1, -3.2, -3.3, -3.4, -3.5, -1.5, -1.6, -1.7, -1.8, -1.9, -2.0, -2.1, -2.2, -2.3, -2.4, -2.5, -2.6, -2.7, -2.8, -2.9, -3.0, -3.1, -3.2, -3.3, -3.4, -3.5, -3.6, -3.7, -3.8, -3.9, -4.0, -4.1, -4.2, -4.3, -4.4, -4.5, -4.6, -4.7, -4.8, -4.9, -5.0, -5.1, -5.2, -5.3, -3.3, -3.4, -3.5, -3.6, -3.7, -3.8, -3.9, -4.0, -4.1, -4.2, -4.3, -4.4, -4.5, -4.6, -4.7, -4.8, -4.9, -5.0, -5.1, -5.2, -5.3, -5.4, -5.5, -5.6, -5.7, -5.8, -5.9, -6.0, -6.1, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8, -6.9, -7.0, -7.1, -7.2, -7.3, -7.4, -7.5, -7.6, -7.7, -7.8, -7.9, -8.0, -8.1, -8.2, -8.3, -8.4, -8.5, -8.6, -8.7, -8.8, -8.9, -6.9, -7.0, -7.1, -5.1, -5.2, -5.3, -5.4, -5.5, -5.6, -5.7, -5.8, -5.9, -6.0, -6.1, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -4.7, -4.8, -4.9, -5.0, -5.1, -5.2, -5.3, -5.4, -5.5, -5.6, -5.7, -5.8, -5.9, -6.0, -6.1, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8, -6.9, -7.0, -7.1, -7.2, -7.3, -7.4, -7.5, -7.6, -7.7, -7.8, -7.9, -8.0, -6.0, -6.1, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8, -6.9, -7.0, -7.1, -7.2, -7.3, -7.4, -7.5, -7.6, -7.7, -7.8, -7.9, -8.0, -8.1, -8.2, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8, -6.9, -7.0, -7.1, -7.2, -7.3, -7.4, -7.5, -7.6, -7.7, -7.8, -7.9, -8.0, -8.1, -8.2, -8.3, -8.4, -8.5, -8.6, -8.7, -8.8, -8.9, -9.0, -9.1, -9.2, -9.3, -9.4, -9.5, -9.6, -9.7, -9.8, -9.9, -10.0, -10.1, -10.2, -10.3, -10.4, -10.5, -10.6, -10.7, -10.8, -10.9, -11.0, -11.1, -11.2, -11.3, -11.4, -11.5, -11.6, -11.7, -11.8, -11.9, -12.0, -12.1, -12.2, -12.3, -12.4, -12.5, -12.6, -12.7, -12.8, -12.9, -13.0, -13.1, -11.1, -11.2, -11.3, -11.4, -11.5, -11.6, -11.7, -11.8, -11.9, -12.0, -12.1, -12.2, -12.3, -12.4, -12.5, -12.6, -12.7, -12.8, -12.9, -13.0, -13.1, -13.2, -13.3, -13.4, -13.5, -13.6, -13.7, -13.8, -13.9, -14.0, -14.1, -14.2, -14.3, -14.4, -14.5, -14.6, -14.7, -14.8, -14.9, -15.0, -15.1, -15.2, -15.3, -13.3, -13.4, -13.5, -13.6, -13.7, -13.8, -13.9, -14.0, -14.1, -14.2, -14.3, -14.4, -14.5, -14.6, -14.7, -14.8, -14.9, -15.0, -13.0, -13.1, -13.2, -13.3, -13.4, -13.5, -13.6, -13.7, -13.8, -13.9, -14.0, -14.1, -14.2, -14.3, -14.4, -12.4, -12.5, -10.5, -10.6, -10.7, -10.8, -10.9, -11.0, -11.1, -11.2, -11.3, -11.4, -11.5, -11.6, -11.7, -11.8, -11.9, -9.9, -10.0, -10.1, -10.2, -10.3, -10.4, -10.5, -10.6, -8.6, -8.7, -8.8, -8.9, -9.0, -9.1, -9.2, -9.3, -9.4, -9.5, -9.6, -9.7, -9.8, -9.9, -10.0, -10.1, -10.2, -10.3, -10.4, -8.4, -8.5, -8.6, -8.7, -8.8, -6.8, -6.9, -7.0, -7.1, -7.2, -7.3, -7.4, -7.5, -7.6, -7.7, -7.8, -7.9, -8.0, -8.1, -8.2, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8, -6.9, -4.9, -5.0, -5.1, -5.2, -5.3, -5.4, -5.5, -5.6, -5.7, -5.8, -5.9, -6.0, -6.1, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8, -6.9, -7.0, -7.1, -7.2, -7.3, -7.4, -7.5, -7.6, -7.7, -7.8, -7.9, -5.9, -6.0, -6.1, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8, -6.9, -7.0, -7.1, -7.2, -7.3, -7.4, -7.5, -7.6, -7.7, -7.8, -7.9, -8.0, -8.1, -8.2, -8.3, -8.4, -8.5, -8.6, -8.7, -8.8, -8.9, -9.0, -9.1, -9.2, -9.3, -9.4, -9.5, -7.5, -7.6, -7.7, -7.8, -7.9, -8.0, -8.1, -8.2, -8.3, -8.4, -8.5, -8.6, -8.7, -8.8, -8.9, -9.0, -9.1, -9.2, -9.3, -9.4, -9.5, -7.5, -7.6, -7.7, -7.8, -7.9, -8.0, -6.0, -6.1, -6.2, -6.3, -6.4, -6.5, -6.6, -6.7, -6.8]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1.ai_plot_coflict_total_analysis(input_text) 실행하면, \n",
    "\n",
    "# 2.결과 나옴!(그래프 2개, 적합성, 복잡성 등등 값 도출됨)\n",
    "\n",
    "# ACTION VERBS RATIO : 8.537\n",
    "# ====================================================================\n",
    "# 에세이에 표현된 다양한 감정 수: 7\n",
    "# ====================================================================\n",
    "# 문장에 표현된 감정 비율 :  25.0\n",
    "# ====================================================================\n",
    "# ['before', 'above', 'sound', 'trail', 'by', 'way', 'until', 'city', 'from', 'to', 'after', 'against', 'forge', 'on', 'path', 'view', 'during', 'through', 'in', 'up', 'camp', 'route']\n",
    "# ====================================================================\n",
    "# SETTING RATIO :  12.34\n",
    "# ====================================================================\n",
    "# 전체 문장에서 캐릭터를 의미하는 단어나 유사어 비율 : 8.79\n",
    "# Degree of conflict  단어가 전체 문장(단어)에서 차지하는 비율 계산 : 3.0\n",
    "# 감정기복비율 : 25.0\n",
    "# 셋팅비율 계산 :  12.34\n",
    "# Plot Complxity : 26.517731803455586\n",
    "# ===============================================================================\n",
    "# ======================      Degree of Conflict   ==============================\n",
    "# ===============================================================================\n",
    "# 1명의 에세이 결과 계산점수 : (26.517731803455586, 25.0, 3.0)\n",
    "# min_ 32\n",
    "# max_:  128\n",
    "# div_: 32\n",
    "# cal_abs 절대값 : 53.48226819654441\n",
    "# compare7 : 17.75295530057593\n",
    "# compare6 : 21.30354636069112\n",
    "# compare5 : 26.629432950863897\n",
    "# compare4 : 35.50591060115186\n",
    "# compare3 : 53.258865901727795\n",
    "# Lacking: 2\n",
    "# min_ 25\n",
    "# max_:  102\n",
    "# div_: 25\n",
    "# cal_abs 절대값 : 39.0\n",
    "# compare7 : 14.833333333333334\n",
    "# compare6 : 17.8\n",
    "# compare5 : 22.25\n",
    "# compare4 : 29.666666666666668\n",
    "# compare3 : 44.5\n",
    "# Lacking: 2\n",
    "# min_ 0\n",
    "# max_:  0\n",
    "# div_: 0\n",
    "# cal_abs 절대값 : 2.686\n",
    "# compare7 : 0.5523333333333333\n",
    "# compare6 : 0.6628000000000001\n",
    "# compare5 : 0.8285\n",
    "# compare4 : 1.1046666666666667\n",
    "# compare3 : 1.657\n",
    "# Overboard: 2\n",
    "# fin_result: [(0, 1), (0, 2), (2, 1)]\n",
    "\n",
    "# result\n",
    "\n",
    "#  [0, 0, 2, 1.33, 5.0, 3.0, 26.52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select prompt number to get intended mood\n",
    "\n",
    "def intended_mood_by_prompt(promptNo):\n",
    "    if promptNo == 'prompt_1':\n",
    "        intended_mood = ['joy', 'pride', 'approval']\n",
    "    elif promptNo == \"prompt_2\":\n",
    "        intended_mood = ['disappointment', 'fear', 'confusion']\n",
    "    elif promptNo == \"prompt_3\":\n",
    "        intended_mood = ['curiosity', 'disapproval', 'realization']\n",
    "    elif promptNo == \"prompt_4\":\n",
    "        intended_mood = ['gratitude', 'surprise', 'admiration']\n",
    "    elif promptNo == \"prompt_5\":\n",
    "        intended_mood = ['realization', 'pride', 'admiration']\n",
    "    elif promptNo == \"prompt_6\":\n",
    "        intended_mood = ['curiosity', 'excitement', 'confusion']\n",
    "    elif promptNo == \"prompt_7\":\n",
    "        intended_mood = ['joy', 'approval','disappointment', 'fear', \n",
    "                         'confusion', 'disapproval', 'realization',\n",
    "                        'gratitude', 'surprise', 'admiration', 'pride',\n",
    "                        'curiosity', 'excitement', ]\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return intended_mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에세이의 감성분석, 입력값(essay, selected prompt number)\n",
    "def ai_emotion_analysis(input_text, promt_number):\n",
    "    # . 로 구분하여 리스트로 변환\n",
    "    re_text = input_text.split(\".\")\n",
    "    #print(\"re_text type: \", type(re_text))\n",
    "        \n",
    "    texts = cleaning(re_text)\n",
    "    re_emot =  goemotions(texts)\n",
    "    df = pd.DataFrame(re_emot)\n",
    "    #print(\"dataframe:\", df)\n",
    "    label_cnt = df.count()\n",
    "    #print(label_cnt)\n",
    " \n",
    "    #추출된 감성중 레이블만 다시 추출하고, 이것을 리스트로 변환 후, 이중리스트 flatten하고, 가장 많이 추출된 대표감성을 카운트하여 산출한다.\n",
    "    result_emotion = list(df['labels'])\n",
    "    #이중리스트 flatten\n",
    "    all_emo_types = sum(result_emotion, [])\n",
    "    #대표감성 추출 : 리스트 항목 카운트하여 가장 높은 값 산출\n",
    "    ext_emotion = {}\n",
    "    for i in all_emo_types:\n",
    "        if i == 'neutral': # neutral 감정은 제거함\n",
    "            pass\n",
    "        else:\n",
    "            try: ext_emotion[i] += 1\n",
    "            except: ext_emotion[i]=1    \n",
    "    #print(ext_emotion)\n",
    "    #결과값 오름차순 정렬 : 추출된 감성 결과가 높은 순서대로 정려하기\n",
    "    key_emo = sorted(ext_emotion.items(), key=lambda x: x[1], reverse=True)\n",
    "    #print(\"Key extract emoitons: \", key_emo)\n",
    "    \n",
    "    #가장 많이 추출된 감성 1개\n",
    "    #key_emo[0]\n",
    "    \n",
    "    #가장 많이 추출된 감성 3개\n",
    "    #key_emo[:2]\n",
    "    \n",
    "    #가장 많이 추출된 감성 5개\n",
    "    key_emo[:5]\n",
    "    \n",
    "    result_emo_list = [*sum(zip(re_text, result_emotion),())]\n",
    "    \n",
    "    # 결과해석\n",
    "    # result_emo_list >>> 문장, 분석감성\n",
    "    # key_emo[0] >>> 가장 많이 추출된 감성 1개로 이것이 에세이이 포함된 대표감성\n",
    "    # key_emo[:2] 가장 많이 추출된 감성 3개\n",
    "    # key_emo[:5] 가장 많이 추출된 감성 5개\n",
    "    top5Emo = key_emo[:5]\n",
    "    #print('top5Emo : ', top5Emo)\n",
    "    top5Emotions = [] # ['approval', 'realization', 'admiration', 'excitement', 'amusement']\n",
    "    top5Emotions.append(top5Emo[0][0])\n",
    "    top5Emotions.append(top5Emo[1][0])\n",
    "    top5Emotions.append(top5Emo[2][0])\n",
    "    top5Emotions.append(top5Emo[3][0])\n",
    "    top5Emotions.append(top5Emo[4][0])\n",
    "    \n",
    "    # 감성추출결과 분류항목 - Intended Mood 별 연관 sentiment\n",
    "    disturbed =['anger', 'annoyance', 'disapproval', 'confusion', 'disappointment', 'disgust', 'anger']\n",
    "    suspenseful = ['fear', 'nervousness', 'confusion', 'surprise', 'excitement']\n",
    "    sad = ['disappointment', 'embarrassment', 'grief', 'remorse', 'sadness']\n",
    "    joyful = ['admiration', 'amusement', 'excitement', 'joy', 'optimism']\n",
    "    calm = ['caring', 'gratitude', 'realization', 'curiosity', 'admiration', 'neutral']\n",
    "    \n",
    "    re_mood ='' \n",
    "    for each_emo in top5Emotions:\n",
    "        if each_emo in disturbed:\n",
    "            re_mood = \"disturbed\"\n",
    "        elif each_emo in suspenseful:\n",
    "            re_mood = \"suspensefull\"\n",
    "        elif each_emo in sad:\n",
    "            re_mood = \"sad\"\n",
    "        elif each_emo in joyful:\n",
    "            re_mood =\"joyful\"\n",
    "        elif each_emo in calm:\n",
    "            re_mood =\"calm\"\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    #입력한 에세이에서 추출한 mood의 str을 리스트로 변환    \n",
    "    detected_mood = [] #결과값으로 이것을 return할 거임\n",
    "    detected_mood.append(re_mood)\n",
    "    \n",
    "    # intended mood, prompt에서 선택한 내용대로 관련 mood 를 추출\n",
    "    get_intended_mood = intended_mood_by_prompt(promt_number) # ex) ['disappointment', 'fear', 'confusion']\n",
    "    \n",
    "    \n",
    "    #1, 2nd Senctece 생성\n",
    "    if re_mood == 'disturbed':\n",
    "        sentence1 = ['You’ve intended to write the essay in a disturbed mood.']\n",
    "        sentence2 = ['The AI’s analysis shows that your personal statement’s mood seems to be disturbed.']\n",
    "\n",
    "    elif re_mood == 'suspenseful':\n",
    "        sentence1 = ['You’ve intended to write the essay in a suspenseful mood.']\n",
    "        sentence2 = ['The AI’s analysis shows that your personal statement’s mood seems to be suspenseful.']\n",
    "\n",
    "    elif re_mood == 'sad':\n",
    "        sentence1 = ['You’ve intended to write the essay in a sad mood.']\n",
    "        sentence2 = ['The AI’s analysis shows that your personal statement’s mood seems to be sad.']\n",
    "\n",
    "    elif re_mood == 'joyful':\n",
    "        sentence1 = ['You’ve intended to write the essay in a joyful mood.']\n",
    "        sentence2 = ['The AI’s analysis shows that your personal statement’s mood seems to be joyful.']\n",
    "                     \n",
    "    elif re_mood == 'calm':\n",
    "        sentence1 = ['You’ve intended to write the essay in a calm mood.']\n",
    "        sentence2 = ['The AI’s analysis shows that your personal statement’s mood seems to be calm.']\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "                    \n",
    "    # intended mood vs. your essay mood\n",
    "    intendedMoodByPmt = []\n",
    "    for each_mood in get_intended_mood: # prompt에서 추출된 mood를 하나씩 가져와서 에세이에서 추출된 mood와 비교\n",
    "        if each_mood in disturbed:\n",
    "            intendedMoodByPmt.append(each_mood) \n",
    "        elif each_mood in suspenseful:\n",
    "            intendedMoodByPmt.append(each_mood)\n",
    "        elif each_mood in sad:\n",
    "            intendedMoodByPmt.append(each_mood)\n",
    "        elif each_mood in joyful:\n",
    "            intendedMoodByPmt.append(each_mood)\n",
    "        elif each_mood in calm:\n",
    "            intendedMoodByPmt.append(each_mood)\n",
    "            \n",
    "    # 비교하여 3rd Sentece 생성 \n",
    "    if intendedMoodByPmt == detected_mood: # 두 개의 mood에 해당하는 리스트의 값이 같으면\n",
    "        sentence3 = \"\"\"It seems that the mood portrayed in your essay is coherent with what you've intended!\"\"\"\n",
    "    elif intendedMoodByPmt == ['disturbed']: # 같지 않다면 다음 항목을 각각 비교\n",
    "        sentence3 = \"\"\"If you wish to shift the essay’s direction towards your original intention, you may consider including more conflicts and how you’ve struggled to resolve them.\"\"\"\n",
    "    elif intendedMoodByPmt == ['suspenseful']:\n",
    "        sentence3 = \"\"\"If you wish to shift the essay’s direction towards your original intention, you may consider including more incidents, actions, and dynamic elements.\"\"\"\n",
    "    elif intendedMoodByPmt == ['sad']:\n",
    "        sentence3 = \"\"\"If you wish to shift the essay’s direction towards your original intention, you may consider including more sympathetic stories about difficult times in life.\"\"\"\n",
    "    elif intendedMoodByPmt == ['joy']:\n",
    "        sentence3 = \"\"\"If you wish to shift the essay’s direction towards your original intention, you may consider including more lighthearted life stories and the positive lessons you draw from them.\"\"\"\n",
    "    elif intendedMoodByPmt == ['calm']:\n",
    "        sentence3 = \"\"\"If you wish to shift the essay’s direction towards your original intention, you may consider including more self-reflection, intellectual topics, or observations that shaped you.\"\"\"\n",
    "    else:\n",
    "        sentence3 = \"\"\" Try Again! \"\"\"\n",
    "        \n",
    "    #################################################################################       \n",
    "    #1000 합격한 에세이의 평균 Top 5 sentiment\n",
    "    #결과는 very close / somewhat close / weak 으로 나와야함\n",
    "    # 각 값은 1000명의 평균에세이값을 산출하여 적용해야함, 지금 값은 dummmy values\n",
    "    prompt_1_sent_mean = [('joy', 8), ('approval', 5), ('disappointment',6),('confusion',7),('gratitude',7)] \n",
    "    prompt_2_sent_mean = [('disappointment',6),('confusion',7),('joy', 8), ('approval', 5), ('disappointment',6)]\n",
    "    prompt_3_sent_mean = [('curiosity',7),('disapproval',6),('disappointment',6),('confusion',7),('gratitude',7)]\n",
    "    prompt_4_sent_mean = [('gratitude',8),('surprise',6),('disappointment',6),('confusion',7),('gratitude',7)]\n",
    "    prompt_5_sent_mean = [('realization',5),('admiration',4),('disappointment',6),('confusion',7),('gratitude',7)]\n",
    "    prompt_6_sent_mean = [('excitement',9),('confusion',5),('disappointment',6),('confusion',7),('gratitude',7)]\n",
    "    prompt_7_sent_mean = [('gratitude',7),('joy',5),('disappointment',6),('confusion',7),('gratitude',7)]\n",
    "    #################################################################################\n",
    "    \n",
    "    if promt_number == 'prompt_1': # 1번 문항을 선택했을 경우(문항선택 'prompt_1 ~ 7')\n",
    "        accepted_essay_av_value = prompt_1_sent_mean\n",
    "        \n",
    "    elif promt_number == 'prompt_2':\n",
    "        accepted_essay_av_value = prompt_2_sent_mean\n",
    "        \n",
    "    elif promt_number == 'prompt_3':\n",
    "        accepted_essay_av_value = prompt_3_sent_mean\n",
    "        \n",
    "    elif promt_number == 'prompt_4':\n",
    "        accepted_essay_av_value = prompt_4_sent_mean\n",
    "        \n",
    "    elif promt_number == 'prompt_5':\n",
    "        accepted_essay_av_value = accepted_essay_av_value = prompt_5_sent_mean\n",
    "        \n",
    "    elif promt_number == 'prompt_6':\n",
    "        accepted_essay_av_value = prompt_6_sent_mean\n",
    "        \n",
    "    elif promt_number == 'prompt_7':\n",
    "        accepted_essay_av_value = prompt_7_sent_mean\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # 결과해석\n",
    "  \n",
    "    # result_emo_list: 문장 + 감성분석결과\n",
    "    # intendedMoodByPmt : intended mood \n",
    "    # detected_mood : 대표 Mood\n",
    "    # sentence1,sentence2, sentence3 : intended mood vs. your mood 비교결과에 대한 문장생성 커멘트 \n",
    "    \n",
    "    # 대표감성 5개 추출(학생 1명거임) : key_emo[:5]\n",
    "    # 합격한 한생의 prompt별 대표감성 2개(1000명 평균) : accepted_essay_av_value\n",
    "    \n",
    "    # In-depth Sentiment Analysis 매칭되는 결과에따라서 very close / somewhat close / weak 결정\n",
    "    ps_ext_emo =[] # 개인 에세이에서 추출한 5개의 대표감성\n",
    "    for itm in key_emo[:5]:\n",
    "        #print(itm[0])\n",
    "        ps_ext_emo.append(itm[0])\n",
    " \n",
    "    print(ps_ext_emo)\n",
    "    \n",
    "    group_ext_emo = [] # 그룹 에세이에서 추출한 5개의 평균 대표감성 5개\n",
    "    for item_2 in accepted_essay_av_value:\n",
    "        group_ext_emo.append(item_2[0])\n",
    "    \n",
    "    print(group_ext_emo)\n",
    "    \n",
    "    #두 값을 비교하여 very close / somewhat close / weak 결정\n",
    "    #중복요소를 추출하여 카운팅하면 두 총 리스트의 값 중에서 중복요소가 몇개 있는지 알 수 있을때 유사도를 계산할 수 있음\n",
    "    count={}\n",
    "    sum_emo = ps_ext_emo + group_ext_emo\n",
    "    for m in sum_emo:\n",
    "        try: count[m] += 1\n",
    "        except: count[m] = 1\n",
    "    print('중복값:', count)\n",
    "    \n",
    "    compare_re = []\n",
    "    for value in count.values(): # 딕셔너리의 벨류 값을 하나씩 가져와서 \n",
    "        if value > 1: # 1보다 큰 수는 중복된 수 이기 때문에 \n",
    "            compare_re.append(value) # 중복된 수를 새로운 리스트 compare_re에 넣고\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    sum_compare_re = sum(compare_re) \n",
    "    # 리스트의 숫자를 모두 더해서 최종 비교를 할거임,\n",
    "    # 총 리스틔 수는 10개이고 중복 최대값은 5개 모두가 중복되는 10이고 최소값은 0(아무것도 중복되지 않음)   0~10까지의 수로 표현됨\n",
    "    print(sum_compare_re)\n",
    "    \n",
    "    if sum_compare_re >= 0 and sum_compare_re <= 3:\n",
    "        in_depth_sent_result = 'weak'\n",
    "    elif sum_compare_re > 3 and sum_compare_re <= 7:\n",
    "        in_depth_sent_result = 'somewhat close'\n",
    "    elif sum_compare_re > 7 :\n",
    "        in_depth_sent_result = 'very close'\n",
    "        \n",
    "        \n",
    "    # 0.result_emo_list: 문장 + 감성분석결과\n",
    "    # 1.intendedMoodByPmt : intended mood \n",
    "    # 2.detected_mood : 대표 Mood\n",
    "    # 3.sentence1,sentence2, sentence3 : intended mood vs. your mood 비교결과에 대한 문장생성 커멘트\n",
    "    # 4.key_emo[:5] : 학생 한명의 에세이에서 추출한 대표감성 5개\n",
    "    # 5.accepted_essay_av_value : 1000명의 합격한 학생의 대표감서 5개\n",
    "    # 6.in_depth_sent_result : 최종 심층 분석결과\n",
    "    # 7.re_mood : 개인 mood 분석 추출결과\n",
    "\n",
    "    return result_emo_list, intendedMoodByPmt, detected_mood, sentence1, sentence2, sentence3, key_emo[:5], accepted_essay_av_value, in_depth_sent_result, re_mood\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedback_plot_conflict(prompt_no, ps_input_text):\n",
    "    \n",
    "    #############################################\n",
    "    # 합격한 학생의 평균 mood 평균 값\n",
    "    # gruop_input_text_re : 합격한 학생의 평균값\n",
    "    group_input_text_re = 'Joyful'\n",
    "    \n",
    "    # intended mood\n",
    "    int_mood_by_prompt_re = intended_mood_by_prompt(prompt_no)\n",
    "    \n",
    "    # detected mood\n",
    "    dtc = ai_emotion_analysis(ps_input_text, prompt_no)\n",
    "    intended_mood = dtc[1] # intended Mood \n",
    "    print('intended_mood:', intended_mood)\n",
    "    detected_mood = dtc[2] # detected Mood\n",
    "    print('detected_mood:', detected_mood)\n",
    "    \n",
    "    # print(\"추출된 감성 5개: \", dtc[7])\n",
    "    \n",
    "    \n",
    "    # intended value\n",
    "    if intended_mood == ['disturbed']:\n",
    "        pc_intended_mood_result = 'Disturbed'\n",
    "        pc_tension = 'Moderate & High Tension'\n",
    "    elif intended_mood == ['suspenseful']:\n",
    "        pc_intended_mood_result = 'Suspenseful'\n",
    "        pc_tension = 'High Tension'\n",
    "    elif intended_mood == ['sad']:\n",
    "        pc_intended_mood_result = 'Sad'\n",
    "        pc_tension = 'Moderate Tension'\n",
    "    elif intended_mood == ['joy']:\n",
    "        pc_intended_mood_result = 'Joyful'\n",
    "        pc_tension = 'Moderate & Low Tension'\n",
    "    else: # carm\n",
    "        pc_intended_mood_result = 'Carm'\n",
    "        pc_tension = 'Low Tension'\n",
    "        \n",
    "    # Tension Value 계산하기 'Moderate & High Tension', 'High Tension', 'Moderate Tension', 'Moderate & Low Tension', 'Low Tension'\n",
    "    if detected_mood == ['disturbed']:\n",
    "        dtc_intended_mood_result = 'Disturbed'\n",
    "        dtc_tension = 'Moderate & High Tension'\n",
    "    elif detected_mood == ['suspenseful']:\n",
    "        dtc_intended_mood_result = 'Suspenseful'\n",
    "        dtc_tension = 'High Tension'\n",
    "    elif detected_mood == ['sad']:\n",
    "        dtc_intended_mood_result = 'Sad'\n",
    "        dtc_tension = 'Moderate Tension'\n",
    "    elif detected_mood == ['joy']:\n",
    "        dtc_intended_mood_result = 'Joyful'\n",
    "        dtc_tension = 'Moderate & Low Tension'\n",
    "    else: # carm\n",
    "        dtc_intended_mood_result = 'Carm'\n",
    "        dtc_tension = 'Low Tension'\n",
    "        \n",
    "    \n",
    "    # Intended Mood vs. Plot & Conflict 두개의 값 비교\n",
    "    if intended_mood == detected_mood:\n",
    "        comp_int_dtc = '='\n",
    "    else: # intended_mood vs. detected_mood 같이 않을 경우\n",
    "        comp_int_dtc = '!=' # 같지 않음\n",
    "\n",
    "    \n",
    "    # 문장 생성시작 --> Sentence 1\n",
    "    if detected_mood == ['disturbed']:\n",
    "        sentence_1 = ['Your intended mood for the essay is ‘disturbed.’ It means that the story may contain matters that made you feel uneasy or upset. In addition, it may deal with a problem or disagreement and your personal struggle to resolve it. Hence, the plot is likely to be a complex one with multiple emotional fluctuations and conflicts.']\n",
    "    elif detected_mood == ['suspenseful']:\n",
    "        sentence_1 = ['Your intended mood for the essay is ‘suspenseful.’ It means that the story may contain multiple elements intertwined with one another. In addition, it may deal with incidents unfolding in a dynamic pattern. Hence, the plot is likely to show a high level of tension with multiple emotional fluctuations and conflicts.']\n",
    "    elif detected_mood == ['sad']:\n",
    "        sentence_1 = ['Your intended mood for the essay is ‘sad.’ It means that the story may contain difficult times in life that made you feel grief. In addition, it may deal with your memories that make the readers feel sympathetic. Hence, the plot is likely to be a moderately complex one with some emotional fluctuations and conflicts.']\n",
    "    elif detected_mood == ['joy']:\n",
    "        sentence_1 = ['''Your intended mood for the essay is ‘joyful.’ It means that the story may contain elements that made you feel happy. In addition, it may deal with lighthearted life stories and positive lessons you've gained. Hence, the plot is likely to be a pleasant one with moderate emotional fluctuations and conflicts.''']\n",
    "    else: # calm \n",
    "        sentence_1 = ['Your intended mood for the essay is ‘calm.’ It means that the story may contain self-reflection, intellectual topics, and observations that shaped your perspective. Hence, the plot is likely to be somewhat steady with limited emotional fluctuations and conflicts.']\n",
    "    \n",
    "    # Sentence 2\n",
    "    # tension level 계산 ( tension 레벨은 conflict words + sentiment fluctuations (negative to positive to negative 이런거) 를 함께 봅니다. 아마도 conflict words 수 (60%) + sentiment fluctuations (40%) 보면 되지 않을까해요.)\n",
    "    if dtc_tension == 'High Tension' and intended_mood == ['suspenseful']:\n",
    "        sentence_2 = ['The detected plot displays a high tension which seems to be', 'closely correlated with', 'your intended mood of the essay.']\n",
    "    elif dtc_tension == 'High Tension' and intended_mood != ['suspenseful']:\n",
    "        sentence_2 = ['The detected plot displays a high tension which seems to be', 'somewhat distant from', 'your intended mood of the essay.']\n",
    "    elif dtc_tension == 'Moderate Tension' and intended_mood == ['disturbed'] or intended_mood == ['sad'] or intended_mood == ['joy']:\n",
    "        sentence_2 = ['The detected plot displays a moderate tension which seems to be', 'closely correlated with', 'your intended mood of the essay.']\n",
    "    elif dtc_tension == 'Moderate Tension' and intended_mood != ['disturbed'] and intended_mood != ['sad'] and intended_mood != ['joy']:\n",
    "        sentence_2 = ['The detected plot displays a moderate tension which seems to be', 'somewhat distant from', 'your intended mood of the essay.']\n",
    "    elif dtc_tension == 'low tension' and intended_mood == ['carm']:\n",
    "        sentence_2 = ['The detected plot displays a low tension which seems to be', 'closely correlated with', 'your intended mood of the essay.']\n",
    "    else:\n",
    "        sentence_2 = ['The detected plot displays a low tension which seems to be', 'somewhat distant from', 'your intended mood of the essay.']\n",
    "    \n",
    "    \n",
    "    ####  number of stimulus words ####\n",
    "    # conlifct words 추출, 수량 계산\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 결과해석 #\n",
    "    # 1.pc_intended_mood_result : 개인이 선택한 intended mood 선택 결과\n",
    "    # 2.pc_tension : 개인의 선택한 tension 결과\n",
    "    \n",
    "    # 3.intended_mood : 개인의 에세이를 prompt 문항에 의해 분석한 결과 ---> 이 값을 적용할 것\n",
    "    # 4.detected_mood : 개인의 에세이를 감성분석한 결과\n",
    "    # 5. comp_int_dtc : Intended Mood vs. Plot & Conflict 부분의 두 값 비교로 같거(=)나 같지 않음(!=)\n",
    "    # 6. sentence_1 ~ 2 : 문장생성\n",
    "        \n",
    "    return pc_intended_mood_result, pc_tension, intended_mood, detected_mood, comp_int_dtc, sentence_1, sentence_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['approval', 'admiration', 'realization', 'amusement', 'confusion']\n",
      "['joy', 'approval', 'disappointment', 'confusion', 'gratitude']\n",
      "중복값: {'approval': 2, 'admiration': 1, 'realization': 1, 'amusement': 1, 'confusion': 2, 'joy': 1, 'disappointment': 1, 'gratitude': 1}\n",
      "4\n",
      "intended_mood: ['joy']\n",
      "detected_mood: ['disturbed']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Joyful',\n",
       " 'Moderate & Low Tension',\n",
       " ['joy'],\n",
       " ['disturbed'],\n",
       " '!=',\n",
       " ['Your intended mood for the essay is ‘disturbed.’ It means that the story may contain matters that made you feel uneasy or upset. In addition, it may deal with a problem or disagreement and your personal struggle to resolve it. Hence, the plot is likely to be a complex one with multiple emotional fluctuations and conflicts.'],\n",
       " ['The detected plot displays a moderate tension which seems to be',\n",
       "  'closely correlated with',\n",
       "  'your intended mood of the essay.'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 실행 ###\n",
    "#  - 입력값 - #\n",
    "prompt_no = 'prompt_1' #이런 형식으로 넣어야 함\n",
    "ps_input_text = input_text # 위에서 이미 입력\n",
    "\n",
    "feedback_plot_conflict(prompt_no, ps_input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
