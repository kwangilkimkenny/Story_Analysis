{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import multiprocessing\n",
    "import io\n",
    "from gensim.models import Phrases\n",
    "from textblob import TextBlob\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간, 공간, 장소를 알려주는 단어 추출하여 카운트\n",
    "def find_setting_words(text):\n",
    "    # Create Doc object\n",
    "    doc2 = nlp(text)\n",
    "    \n",
    "    setting_list = []\n",
    "    # Identify by label FAC(building etc), GPE(countries, cities..), LOC(locaton), TIME\n",
    "    fac_r = [ent.text for ent in doc2.ents if ent.label_ == 'FAC']\n",
    "    setting_list.append(fac_r)\n",
    "    \n",
    "    gpe_r = [ent.text for ent in doc2.ents if ent.label_ == 'GPE']\n",
    "    setting_list.append(gpe_r)\n",
    "    \n",
    "    loc_r = [ent.text for ent in doc2.ents if ent.label_ == 'LOC']\n",
    "    setting_list.append(loc_r)\n",
    "    \n",
    "    time_r = [ent.text for ent in doc2.ents if ent.label_ == 'TIME']\n",
    "    setting_list.append(time_r)\n",
    "    \n",
    "    #추출된 항목들\n",
    "    all_setting_words = sum(setting_list, [])\n",
    "    \n",
    "    #셋팅 추출 항목들의 총 수\n",
    "    get_setting_list = len(all_setting_words)\n",
    "    \n",
    "    # Return all setting words\n",
    "    return get_setting_list, all_setting_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Bloomington Normal is almost laughably cliché for a midwestern city. Vast swathes of corn envelop winding roads and the heady smell of BBQ smoke pervades the countryside every summer. Yet, underlying the trite norms of Normal is the prescriptive force of tradition—the expectation to fulfill my role as a female Filipino by playing Debussy in the yearly piano festival and enrolling in multivariable calculus instead of political philosophy.So when I discovered the technical demand of bebop, the triplet groove, and the intricacies of chordal harmony after ten years of grueling classical piano, I was fascinated by the music's novelty. Jazz guitar was not only evocative and creative, but also strangely liberating. I began to explore different pedagogical methods, transcribe solos from the greats, and experiment with various approaches until my own unique sound began to develop. And, although I did not know what would be the 'best' route for me to follow as a musician, the freedom to forge whatever path I felt was right seemed to be exactly what I needed; there were no expectations for me to continue in any particular way—only the way that suited my own desires.While journeying this trail, I found myself at Interlochen Arts Camp the summer before my junior year. Never before had I been immersed in an environment so conducive to musical growth: I was surrounded by people intensely passionate about pursuing all kinds of art with no regard for ideas of what art 'should' be. I knew immediately that this would be a perfect opportunity to cultivate my sound, unbounded by the limits of confining tradition. On the first day of camp, I found that my peer guitarist in big band was another Filipino girl from Illinois. Until that moment, my endeavors in jazz guitar had been a solitary effort; I had no one with whom to collaborate and no one against whom I could compare myself, much less someone from a background mirroring my own. I was eager to play with her, but while I quickly recognized a slew of differences between us—different heights, guitars, and even playing styles—others seemed to have trouble making that distinction during performances. Some even went as far as calling me 'other-Francesca.' Thus, amidst the glittering lakes and musky pine needles of Interlochen, I once again confronted Bloomington's frustrating expectations.After being mistaken for her several times, I could not help but view Francesca as a standard of what the 'female Filipino jazz guitarist' should embody. Her improvisatory language, comping style and even personal qualities loomed above me as something I had to live up to. Nevertheless, as Francesca and I continued to play together, it was not long before we connected through our creative pursuit. In time, I learned to draw inspiration from her instead of feeling pressured to follow whatever precedent I thought she set. I found that I grew because of, rather than in spite of, her presence; I could find solace in our similarities and even a sense of comfort in an unfamiliar environment without being trapped by expectation. Though the pressure to conform was still present—and will likely remain present in my life no matter what genre I'm playing or what pursuits I engage in—I learned to eschew its corrosive influence and enjoy the rewards that it brings. While my encounter with Francesca at first sparked a feeling of pressure to conform in a setting where I never thought I would feel its presence, it also carried the warmth of finding someone with whom I could connect. Like the admittedly trite conditions of my hometown, the resemblances between us provided comfort to me through their familiarity. I ultimately found that I can embrace this warmth while still rejecting the pressure to succumb to expectations, and that, in the careful balance between these elements, I can grow in a way that feels both like discove\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_setting = list(find_setting_words(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, ['Interlochen Arts Camp', 'Illinois', 'Interlochen', 'Bloomington']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intended Setting \n",
    "# 입력 : Surroundings matter a lot : 'alot', Somewhat important: 'impt', Not a big factor : 'notBigFactor'\n",
    "def intendedSetting(intended_setting_input):\n",
    "    if intended_setting_input == 'alot':\n",
    "        int_setting_result = 'Surroundings matter a lot'\n",
    "    elif intended_setting_input == 'impt':\n",
    "        int_setting_result = 'Somewhat important'\n",
    "    else: # not a big factor\n",
    "        int_setting_result = 'Not a big factor'\n",
    "    return int_setting_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Setting_analysis(text):\n",
    "\n",
    "    essay_input_corpus = str(text) #문장입력\n",
    "    essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "    #print('essay_input_corpus :', essay_input_corpus)\n",
    "    \n",
    "    sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화 > 문장으로 구분\n",
    "    total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "    total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "    split_sentences = []\n",
    "    for sentence in sentences:\n",
    "        processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "        words = processed.split()\n",
    "        split_sentences.append(words)\n",
    "\n",
    "    skip_gram = 1\n",
    "    workers = multiprocessing.cpu_count()\n",
    "    bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "    model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "    model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "    \n",
    "    #모델 설계 완료\n",
    "\n",
    "    #setting을 표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "    location_list = ['above', 'behind','below','beside','betweed','by','in','inside','near',\n",
    "                     'on','over','through']\n",
    "    time_list = ['after', 'before','by','during','from','on','past','since','through','to','until','upon']\n",
    "      \n",
    "    movement_list = ['against','along','down','from','into','off','on','onto','out of','toward','up','upon']\n",
    "    \n",
    "    palce_terrain_type_list = ['wood', 'forest', 'copse', 'bush', 'trees', 'stand',\n",
    "                                'swamp', 'marsh', 'wetland', 'fen', 'bog', 'moor', 'heath', 'fells', 'morass',\n",
    "                                'jungle', 'rainforest', 'cloud forest','plains', 'fields', 'grass', 'grassland', \n",
    "                                'savannah', 'flood plain', 'flats', 'prairie','tundra', 'iceberg', 'glacier', \n",
    "                                'snowfields','hills', 'highland,' 'heights', 'plateau', 'badland', 'kame', 'shield',\n",
    "                                'downs', 'downland', 'ridge', 'ridgeline','hollow,' 'valley',' vale','glen', 'dell',\n",
    "                                'mountain', 'peak', 'summit', 'rise', 'pass', 'notch', 'crown', 'mount', 'switchback',\n",
    "                                'furth','canyon', 'cliff', 'bluff,' 'ravine', 'gully', 'gulch', 'gorge',\n",
    "                                'desert', 'scrub', 'waste', 'wasteland', 'sands', 'dunes',\n",
    "                                'volcano', 'crater', 'cone', 'geyser', 'lava fields']\n",
    "    \n",
    "    water_list = ['ocean', 'sea', 'coast', 'beach', 'shore', 'strand','bay', 'port', 'harbour', 'fjord', 'vike',\n",
    "                  'cove', 'shoals', 'lagoon', 'firth', 'bight', 'sound', 'strait', 'gulf', 'inlet', 'loch', \n",
    "                  'bayou','dock', 'pier', 'anchorage', 'jetty', 'wharf', 'marina', 'landing', 'mooring', 'berth', \n",
    "                  'quay', 'staith','river', 'stream', 'creek', 'brook', 'waterway', 'rill','delta', 'bank', 'runoff',\n",
    "                  'channel', 'bend', 'meander', 'backwater','lake', 'pool', 'pond', 'dugout', 'fountain', 'spring', \n",
    "                  'watering-hole', 'oasis','well', 'cistern', 'reservoir','waterfall', 'falls', 'rapids', 'cataract', \n",
    "                  'cascade','bridge', 'crossing', 'causeway', 'viaduct', 'aquaduct', 'ford', 'ferry','dam', 'dike', \n",
    "                  'bar', 'canal', 'ditch','peninsula', 'isthmus', 'island', 'isle', 'sandbar', 'reef', 'atoll', \n",
    "                  'archipelago', 'cay','shipwreck', 'derelict']\n",
    "    \n",
    "    \n",
    "    outdoor_places_list = ['clearing', 'meadow', 'grove', 'glade', 'fairy ring','earldom', 'fief', 'shire',\n",
    "                            'ruin', 'acropolis', 'desolation', 'remnant', 'remains',\n",
    "                            'henge', 'cairn', 'circle', 'mound', 'barrow', 'earthworks', 'petroglyphs',\n",
    "                            'lookout', 'aerie', 'promontory', 'outcropping', 'ledge', 'overhang', 'mesa', 'butte',\n",
    "                            'outland', 'outback', 'territory', 'reaches', 'wild', 'wilderness', 'expanse',\n",
    "                            'view', 'vista', 'tableau', 'spectacle', 'landscape', 'seascape', 'aurora', 'landmark',\n",
    "                            'battlefield', 'trenches', 'gambit', 'folly', 'conquest', 'claim', 'muster', 'post',\n",
    "                            'path', 'road', 'track', 'route', 'highway', 'way', 'trail', 'lane', 'thoroughfare', 'pike',\n",
    "                            'alley', 'street', 'avenue', 'boulevard', 'promenade', 'esplande', 'boardwalk',\n",
    "                            'crossroad', 'junction', 'intersection', 'turn', 'corner','plaza', 'terrace', 'square', \n",
    "                            'courtyard', 'court', 'park', 'marketplace', 'bazaar', 'fairground','realm', 'land', 'country',\n",
    "                            'nation', 'state', 'protectorate', 'empire', 'kingdom', 'principality','domain', 'dominion',\n",
    "                            'demesne', 'province', 'county', 'duchy', 'barony', 'baronetcy', 'march', 'canton']\n",
    "\n",
    "    \n",
    "    underground_list = ['pit', 'hole', 'abyss', 'sinkhole', 'crack', 'chasm', 'scar', 'rift', 'trench', 'fissure',\n",
    "                        'cavern', 'cave', 'gallery', 'grotto', 'karst',\n",
    "                        'mine', 'quarry', 'shaft', 'vein','graveyard', 'cemetery',\n",
    "                        'darkness', 'shadow', 'depths', 'void','maze', 'labyrinth'\n",
    "                        'tomb', 'grave', 'crypt', 'sepulchre', 'mausoleum', 'ossuary', 'boneyard']\n",
    "                        \n",
    "    living_places_list = ['nest', 'burrow', 'lair', 'den', 'bolt-hole', 'warren', 'roost', 'rookery', 'hibernaculum',\n",
    "                         'home', 'rest', 'hideout', 'hideaway', 'retreat', 'resting-place', 'safehouse', 'sanctuary',\n",
    "                         'respite', 'lodge','slum', 'shantytown', 'ghetto','camp', 'meeting place,' 'bivouac', 'campsite', \n",
    "                         'encampment','tepee', 'tent', 'wigwam', 'shelter', 'lean-to', 'yurt','house', 'mansion', 'estate',\n",
    "                         'villa','hut', 'palace', 'outbuilding', 'shack tenement', 'hovel', 'manse', 'manor', 'longhouse',\n",
    "                         'cottage', 'cabin','parsonage', 'rectory', 'vicarge', 'friary', 'priory','abbey', 'monastery', \n",
    "                         'nunnery', 'cloister', 'convent', 'hermitage','castle', 'keep', 'fort', 'fortress', 'citadel', \n",
    "                         'bailey', 'motte', 'stronghold', 'hold', 'chateau', 'outpost', 'redoubt',\n",
    "                         'town', 'village', 'hamlet', 'city', 'metropolis','settlement', 'commune']\n",
    "\n",
    "    building_facilities_list = ['temple', 'shrine', 'church', 'cathedral', 'tabernacle', 'ark', 'sanctum', 'parish', \n",
    "                                'chapel', 'synagogue', 'mosque','pyramid', 'ziggurat', 'prison', 'jail', 'dungeon',\n",
    "                                'oubliette', 'hospital', 'hospice', 'stocks', 'gallows','asylum', 'madhouse', 'bedlam',\n",
    "                                'vault', 'treasury', 'warehouse', 'cellar', 'relicry', 'repository',\n",
    "                                'barracks', 'armoury','sewer', 'gutter', 'catacombs', 'dump', 'middens', 'pipes', 'baths', 'heap',\n",
    "                                'mill', 'windmill', 'sawmill', 'smithy', 'forge', 'workshop', 'brickyard', 'shipyard', 'forgeworks',\n",
    "                                'foundry','bakery', 'brewery', 'almshouse', 'counting house', 'courthouse', 'apothecary', 'haberdashery', 'cobbler',\n",
    "                                'garden', 'menagerie', 'zoo', 'aquarium', 'terrarium', 'conservatory', 'lawn', 'greenhouse',\n",
    "                                'farm', 'orchard', 'vineyard', 'ranch', 'apiary', 'farmstead', 'homestead',\n",
    "                                'pasture', 'commons', 'granary', 'silo', 'crop','barn', 'stable', 'pen', 'kennel', 'mews', 'hutch', \n",
    "                                'pound', 'coop', 'stockade', 'yard', 'lumber yard','tavern', 'inn', 'pub', 'brothel', 'whorehouse',\n",
    "                                'cathouse', 'discotheque','lighthouse', 'beacon','amphitheatre', 'colosseum', 'stadium', 'arena', \n",
    "                                'circus','academy', 'university', 'campus', 'college', 'library', 'scriptorium', 'laboratory', \n",
    "                                'observatory', 'museum']\n",
    "    \n",
    "    \n",
    "    architecture_list = ['hall', 'chamber', 'room','nave', 'aisle', 'vestibule',\n",
    "                        'antechamber', 'chantry', 'pulpit','dome', 'arch', 'colonnade',\n",
    "                        'stair', 'ladder', 'climb', 'ramp', 'steps',\n",
    "                        'portal', 'mouth', 'opening', 'door', 'gate', 'entrance', 'maw',\n",
    "                        'tunnel', 'passage', 'corridor', 'hallway', 'chute', 'slide', 'tube', 'trapdoor',\n",
    "                        'tower', 'turret', 'belfry','wall', 'fortifications', 'ramparts', 'pallisade', 'battlements',\n",
    "                        'portcullis', 'barbican','throne room', 'ballroom','roof', 'rooftops', 'chimney', 'attic',\n",
    "                        'loft', 'gable', 'eaves', 'belvedere','balcony', 'balustrade', 'parapet', 'walkway', 'catwalk',\n",
    "                        'pavillion', 'pagoda', 'gazebo','mirror', 'glass', 'mere','throne', 'seat', 'dais',\n",
    "                        'pillar', 'column', 'stone', 'spike', 'rock', 'megalith', 'menhir', 'dolmen', 'obelisk',\n",
    "                        'statue', 'giant', 'head', 'arm', 'leg', 'body', 'chest', 'body', 'face', 'visage', 'gargoyle', 'grotesque',\n",
    "                        'fire', 'flame', 'bonfire', 'hearth', 'fireplace', 'furnace', 'stove','window', 'grate', 'peephole', \n",
    "                        'arrowslit', 'slit', 'balistraria', 'lancet', 'aperture', 'dormerl']\n",
    "    \n",
    "    \n",
    "    setting_words_filter_list = location_list + time_list + movement_list + palce_terrain_type_list + water_list + outdoor_places_list + underground_list + underground_list + living_places_list + building_facilities_list + architecture_list\n",
    "\n",
    "    \n",
    "    ####문장에 setting_words_filter_list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "    #우선 토큰화한다.\n",
    "    retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "    token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "    # print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "    # 리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "    filtered_setting_text = []\n",
    "    for k in token_input_text:\n",
    "        for j in setting_words_filter_list:\n",
    "            if k == j:\n",
    "                filtered_setting_text.append(j)\n",
    "    \n",
    "    # print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "    \n",
    "    filtered_setting_text_ = set(filtered_setting_text) #중복제거\n",
    "    filtered_setting_text__ = list(filtered_setting_text_) #다시 리스트로 변환\n",
    "    # print (filtered_setting_text__) # 중복값 제거 확인\n",
    "    \n",
    "    # 셋팅의 장소관련 단어 추출\n",
    "    extract_setting_words = list(find_setting_words(text))\n",
    "    \n",
    "    # 문장내 모든 셋팅 단어 추출\n",
    "    tot_setting_words = extract_setting_words + filtered_setting_text__\n",
    "    \n",
    "    # 셋팅단어가 포함된 문장을 찾아내서 추출하기\n",
    "    # if 셋팅단어가 문장에 있다면, 그 문장을 추출(.로 split한 문장 리스트)해서 리스트로 저장한다.\n",
    "    \n",
    "    # print('sentences: ', sentences) # .로 구분된 전체 문장\n",
    "    \n",
    "    sentence_to_words = word_tokenize(essay_input_corpus) # 총 문장을 단어 리스트로 변환\n",
    "    # print('sentence_to_words:', sentence_to_words)\n",
    "    \n",
    "    # 셋팅단어가 포함된 문장을 찾아내서 추출\n",
    "    extrace_sentence_and_setting_words = [] # 이것은 \"문장\", '셋팅단어' ... 합쳐서 리스트로 저장\n",
    "    extract_only_sentences_include_setting_words = [] # 셋팅 단어가 포함된 문장만 리스트로 저장\n",
    "    for sentence in sentences: # 문장을 하나씩 꺼내온다.\n",
    "        for item in tot_setting_words: # 셋팅 단어를 하나씩 꺼내온다.\n",
    "            if item in word_tokenize(sentence): # 꺼낸 문장을 단어로 나누고, 그 안에 셋팅 단어가 있다면\n",
    "                extrace_sentence_and_setting_words.append(sentence) # 셋팅 단어가 포함된 문장을 별도로 저장한다.\n",
    "                extrace_sentence_and_setting_words.append(item) # 셋팅 단어도 추가로 저장한다. \n",
    "                \n",
    "                extract_only_sentences_include_setting_words.append(sentence)\n",
    "                \n",
    "                \n",
    "                ## 찾는 단어 수 대로 문장을 모두 별도 저장하기때문에 문장이 중복 저장된다. 한번만 문장이 저장되도록 하자. \n",
    "                ## 문장. '단어' , '단어' 이런 식으로다가 수정해야함. 중복리스트를 제거하면 됨.\n",
    "    # 중복리스트를 제거한다.\n",
    "    extrace_sentence_with_setting_words_re = set(extrace_sentence_and_setting_words)\n",
    "    #print('extrace_sentence_and_setting_words(문장+단어)) :', extrace_sentence_with_setting_words_re)\n",
    "    \n",
    "    extract_only_sentences_include_setting_words_re = set(extract_only_sentences_include_setting_words) #중복제거\n",
    "    #print('extract_only_sentences_include_setting_words(오직 셋팅 포함 문장):', extract_only_sentences_include_setting_words_re)\n",
    "    \n",
    "    # 단, 소문자로 문장이 저장되어 있어서, 동일한 원문을 찾을 수 없다. 소문자로 되어있는 문장을 통해서 대문자가 섞여있는 원문을 찾자\n",
    "    # 방법) 소문자 문장을 단어로로 토크나이즈한 후 리스트로 만든다. 대문자 문장도 단어로 토크나이즈한 후 리스트로 만든다.\n",
    "    # 두 개의 리스트를 비교해서 같은 단어가 3개 혹은 5개 이상 나오면 대문자 문장의 원문을 매칭한다. 끝!\n",
    "    \n",
    "    #아래 메소드에 리스트로된 문장 삽입, set 함수로 처리된것을 다시 list로 변환해야 첫 글자를 대문자로 바꿀 수 있다.\n",
    "    lower_text_input = list(extract_only_sentences_include_setting_words_re)\n",
    "    #print('lower_text_input: ', lower_text_input[0])\n",
    "    \n",
    "    ######################################################################################\n",
    "    ###### 소문자 문장으로 대문자 포함원 원문 추출하는 함수 ########\n",
    "    # essay_input_corpus : 최초의 입력문자를 스트링으로 변환한 원본\n",
    "    def find_original_sentence(lower_text_input, essay_input_corpus):\n",
    "        \n",
    "        #1)원본 전체을 문장으로 토큰화\n",
    "        sentence_tokenized = sent_tokenize(essay_input_corpus)\n",
    "        #print(\"======================================\")\n",
    "        #print('sentence_tokenized:',sentence_tokenized)\n",
    "        #문장으로 토큰화한 것을 리스트로 묶어서 다시 단어로 토큰화한다. \n",
    "        word_tokenized = [] #입력에세이 원본의 토큰화된 리스트화!\n",
    "        for st_to in sentence_tokenized:\n",
    "            word_tokenized.append(word_tokenize(st_to))\n",
    "        #print(\"======================================\")\n",
    "        # 이렇게 되어 있을 것이다 -> 문장으로 구분되어 리스트로 나뉘고 다시 단어로 분할되어 리스트[['단어','단어', ...], ['단어','단어', ...]...]\n",
    "        #print('word_tokenized:', word_tokenized)\n",
    "        \n",
    "        \n",
    "        #2)다음으로 계산 추출된 소문자로 변환된 셋팅단어 포함 문장의 단어에 대해서 첫 글자를 대문자로 만든다.\n",
    "        capital_text = []\n",
    "        for lt in lower_text_input:  \n",
    "            capital_text.append(lt.capitalize())\n",
    "        #print(\"======================================\")    \n",
    "        #print('captal_text(첫글자 대문자로 변환되었는지 확인!!!!!!!!!!):', capital_text) # 잘됨!\n",
    "        \n",
    "        capital_token_text_list = []\n",
    "        for cpt_item in capital_text:\n",
    "            #단어로 분할해서 리스트에 담는다.\n",
    "            capital_token_text_list.append(word_tokenize(cpt_item))\n",
    "        #print(\"======================================\")\n",
    "        # 이렇게 되어 있을 것이다 -> 문장으로 구분되어 리스트로 나뉘고 다시 단어로 분할되어 리스트[['단어','단어', ...], ['단어','단어', ...]\n",
    "        #print('captal_token_text_list:',capital_token_text_list)\n",
    "        \n",
    "        \n",
    "        # 이제 아래 두개의 리스트를 비교해서 원본을 찾아야 한다.그리고 다시 찾은 원본토큰화된 단어 리스트를 문장으로 복원한다.\n",
    "        \n",
    "        # word_tokenized : 입력에세이 원본의 토큰화된 리스트화! (원본문장)\n",
    "        # capital_token_text_list : 추출된 에세이 결과물 토큰화 (입력문장)\n",
    "        \n",
    "        # print('word_tokenized:', word_tokenized)\n",
    "        # print(('capital_token_text_list:', capital_token_text_list))\n",
    "        \n",
    "        # 캐릭터가 표현이 포함된 최종 문장의 리트스 추출\n",
    "        count_ct_item = 0\n",
    "        included_character_exp = []\n",
    "        for ct in capital_token_text_list:\n",
    "            for wt in word_tokenized:\n",
    "                for ct_item in ct:\n",
    "                    if count_ct_item >= 5: # 겹치는 단어가 4개 이상이면 같은 문장이라고 판단하자 \n",
    "                        # 같은 문장이기 땜누에 원본 리스트의 단어들을 하나의 문장으로 만들어서 저장하자\n",
    "                        re_cpt = ' '.join(wt).capitalize()\n",
    "                        included_character_exp.append(re_cpt)\n",
    "                    elif ct_item in wt: # 리스트 안에 비교리스트가 있다면, 단어 수 카운트하고 for문 돌림\n",
    "                        count_ct_item += 1\n",
    "                        #print('count_ct_item:', count_ct_item)\n",
    "                    else: # 비교 후 겹치는 값이 없다면 패스\n",
    "                        pass\n",
    "                    \n",
    "        # 최종결과물 첫 글자 대문자로 복원\n",
    "        \n",
    "        # 최종 결과물 중복제거\n",
    "        result_origin = set(included_character_exp)\n",
    "        return result_origin\n",
    "    ####################################################################################\n",
    "    \n",
    "    \n",
    "    # 셋팅 단어가 포함된 모든 문장을 추출\n",
    "    \n",
    "    totalSettingSentences = find_original_sentence(lower_text_input, essay_input_corpus)\n",
    "    #print('totalSettingSentences:', totalSettingSentences)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in filtered_setting_text__:\n",
    "        ext_setting_sim_words_key = model.most_similar_cosmul(i) # 모델적용\n",
    "    \n",
    "    setting_total_count = len(filtered_setting_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 setting 표현 수\n",
    "    setting_count_ = len(filtered_setting_text__) # 중복제거된 setting표현 총 수\n",
    "        \n",
    "    result_setting_words_ratio = round(setting_total_count/total_words * 100, 2)\n",
    "    #return result_setting_words_ratio\n",
    "    \n",
    "    # 결과해석\n",
    "    # result_setting_words_ratio : 전체 문장에서 셋팅관련 단어의 사용비율(포함비율)\n",
    "    # total_sentences : 총 문장 수\n",
    "    # total_words : 총 단어 수\n",
    "    # setting_total_count : # 중복이 제거되지 않은 에세이 총 문장에 사용된 setting 표현 수\n",
    "    # setting_count_ : # 중복제거된 setting표현 총 수\n",
    "    # ext_setting_sim_words_key : 셋팅설정과 유사한 단어들 추출\n",
    "    # totalSettingSentences : 셋팅 단어가 포함된 모든 문장을 추출\n",
    "    \n",
    "    return result_setting_words_ratio, total_sentences, total_words, setting_total_count, setting_count_, ext_setting_sim_words_key, totalSettingSentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/gensim/models/base_any2vec.py:323: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n",
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:256: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12.34,\n",
       " 21,\n",
       " 705,\n",
       " 87,\n",
       " 22,\n",
       " [('we', 0.982723593711853),\n",
       "  ('connected', 0.9821943044662476),\n",
       "  ('long', 0.9813782572746277),\n",
       "  ('pursuit', 0.9811192154884338),\n",
       "  ('year', 0.9799692034721375),\n",
       "  ('junior', 0.975834846496582),\n",
       "  ('familiarity', 0.9729634523391724),\n",
       "  ('together', 0.9724130630493164),\n",
       "  ('their', 0.9688291549682617),\n",
       "  ('similarities', 0.9683848023414612)],\n",
       " {\"And , although i did not know what would be the 'best ' route for me to follow as a musician , the freedom to forge whatever path i felt was right seemed to be exactly what i needed ; there were no expectations for me to continue in any particular way—only the way that suited my own desires.while journeying this trail , i found myself at interlochen arts camp the summer before my junior year .\",\n",
       "  'Bloomington normal is almost laughably cliché for a midwestern city .',\n",
       "  'Her improvisatory language , comping style and even personal qualities loomed above me as something i had to live up to .',\n",
       "  'I began to explore different pedagogical methods , transcribe solos from the greats , and experiment with various approaches until my own unique sound began to develop .',\n",
       "  'I found that i grew because of , rather than in spite of , her presence ; i could find solace in our similarities and even a sense of comfort in an unfamiliar environment without being trapped by expectation .',\n",
       "  'I knew immediately that this would be a perfect opportunity to cultivate my sound , unbounded by the limits of confining tradition .',\n",
       "  'I ultimately found that i can embrace this warmth while still rejecting the pressure to succumb to expectations , and that , in the careful balance between these elements , i can grow in a way that feels both like discove',\n",
       "  'I was eager to play with her , but while i quickly recognized a slew of differences between us—different heights , guitars , and even playing styles—others seemed to have trouble making that distinction during performances .',\n",
       "  'In time , i learned to draw inspiration from her instead of feeling pressured to follow whatever precedent i thought she set .',\n",
       "  'Jazz guitar was not only evocative and creative , but also strangely liberating .',\n",
       "  'Like the admittedly trite conditions of my hometown , the resemblances between us provided comfort to me through their familiarity .',\n",
       "  \"Never before had i been immersed in an environment so conducive to musical growth : i was surrounded by people intensely passionate about pursuing all kinds of art with no regard for ideas of what art 'should ' be .\",\n",
       "  'Nevertheless , as francesca and i continued to play together , it was not long before we connected through our creative pursuit .',\n",
       "  'On the first day of camp , i found that my peer guitarist in big band was another filipino girl from illinois .',\n",
       "  \"Some even went as far as calling me 'other-francesca . '\",\n",
       "  \"Though the pressure to conform was still present—and will likely remain present in my life no matter what genre i 'm playing or what pursuits i engage in—i learned to eschew its corrosive influence and enjoy the rewards that it brings .\",\n",
       "  \"Thus , amidst the glittering lakes and musky pine needles of interlochen , i once again confronted bloomington 's frustrating expectations.after being mistaken for her several times , i could not help but view francesca as a standard of what the 'female filipino jazz guitarist ' should embody .\",\n",
       "  'Until that moment , my endeavors in jazz guitar had been a solitary effort ; i had no one with whom to collaborate and no one against whom i could compare myself , much less someone from a background mirroring my own .',\n",
       "  'Vast swathes of corn envelop winding roads and the heady smell of bbq smoke pervades the countryside every summer .',\n",
       "  'While my encounter with francesca at first sparked a feeling of pressure to conform in a setting where i never thought i would feel its presence , it also carried the warmth of finding someone with whom i could connect .',\n",
       "  \"Yet , underlying the trite norms of normal is the prescriptive force of tradition—the expectation to fulfill my role as a female filipino by playing debussy in the yearly piano festival and enrolling in multivariable calculus instead of political philosophy.so when i discovered the technical demand of bebop , the triplet groove , and the intricacies of chordal harmony after ten years of grueling classical piano , i was fascinated by the music 's novelty .\"})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = Setting_analysis(input_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>result_setting_words_ratio</th>\n",
       "      <td>12.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_sentences</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_words</th>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>setting_total_count</th>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>setting_count_</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ext_setting_sim_words_key</th>\n",
       "      <td>[(we, 0.982723593711853), (connected, 0.982194...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totalSettingSentences</th>\n",
       "      <td>{And , although i did not know what would be t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        Value\n",
       "result_setting_words_ratio                                              12.34\n",
       "total_sentences                                                            21\n",
       "total_words                                                               705\n",
       "setting_total_count                                                        87\n",
       "setting_count_                                                             22\n",
       "ext_setting_sim_words_key   [(we, 0.982723593711853), (connected, 0.982194...\n",
       "totalSettingSentences       {And , although i did not know what would be t..."
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.DataFrame(df, index = [\n",
    "                                'result_setting_words_ratio', 'total_sentences', 'total_words', 'setting_total_count',\n",
    "                                'setting_count_', 'ext_setting_sim_words_key', 'totalSettingSentences'\n",
    "                                ], columns = ['Value']\n",
    "                  )\n",
    "\n",
    "# 데이터프레임 출력해봄                     \n",
    "df_ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting PPT 13p\n",
    "# Emphasis on Setting\n",
    "\n",
    "# intended_setting_by_you 입력: Surroundings matter a lot : 'alot', Somewhat important: 'impt', Not a big factor : 'notBigFactor'\n",
    "# prompt_no : promt_1~7\n",
    "def EmphasisOnSetting(prompt_no, input_text, intended_setting_by_you):\n",
    "    #intended by you setting value\n",
    "    intended_re = intendedSetting(intended_setting_by_you)\n",
    "\n",
    "    ##########################################################\n",
    "    # 1000명의 평균값 셋팅 벨류(임의로 설정, 나중에 평균값 계산해서 적용할 것)\n",
    "    group_setting_mean_value = 15\n",
    "    ##########################################################\n",
    "    \n",
    "    #detected setting value\n",
    "    detected_setting_value_re = Setting_analysis(input_text)[4]\n",
    "    \n",
    "    # 조건판단, 오차를 +-20% 주자\n",
    "    if detected_setting_value_re > (group_setting_mean_value + round(group_setting_mean_value * 0.2)):\n",
    "        dct_result = 'Surroundings matter a lot'\n",
    "    elif detected_setting_value_re == group_setting_mean_value:\n",
    "        dct_result = 'Somewhat important'\n",
    "    elif detected_setting_value_re <= (group_setting_mean_value + round(group_setting_mean_value * 0.2)):\n",
    "        dct_result = 'Somewhat important'\n",
    "    elif detected_setting_value_re >= (group_setting_mean_value - round(group_setting_mean_value * 0.2)):\n",
    "        dct_result = 'Somewhat important'\n",
    "    else: # detected_setting_value_re < group_setting_mean_value:\n",
    "        dct_result = 'Not a big factor'\n",
    "        \n",
    "    # Setting Preferences by Admitted Students for 'Prompt #3'\n",
    "    selected_prompt_number = []\n",
    "    if prompt_no == \"promt_1\":\n",
    "        selected_prompt_number.append(\"prompt #.1\")\n",
    "    elif prompt_no == \"promt_2\":\n",
    "        selected_prompt_number.append(\"prompt #.2\")\n",
    "    elif prompt_no == \"promt_3\":\n",
    "        selected_prompt_number.append(\"prompt #.3\")\n",
    "    elif prompt_no == \"promt_4\":\n",
    "        selected_prompt_number.append(\"prompt #.4\")\n",
    "    elif prompt_no == \"promt_5\":\n",
    "        selected_prompt_number.append(\"prompt #.5\")\n",
    "    elif prompt_no == \"promt_6\":\n",
    "        selected_prompt_number.append(\"prompt #.6\")\n",
    "    elif prompt_no == \"promt_7\":\n",
    "        selected_prompt_number.append(\"prompt #.7\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # print('selected prompt number:', selected_prompt_number)\n",
    "    \n",
    "    # 문장 생성 부분 시작\n",
    "    # Sentence 1\n",
    "    if intended_re == 'Surroundings matter a lot':\n",
    "        Sentence_1 = 'You aimed to give high importance on setting in your personal statement.'\n",
    "    elif intended_re == 'Somewhat important':\n",
    "        Sentence_1 = 'You aimed to give moderate importance on setting in your personal statement.'\n",
    "    elif intended_re == 'Not a big factor':\n",
    "        Sentence_1 = 'You aimed to give low importance on setting in your personal statement.'\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # 문장 삽입 조건판단( 의도한 결과와 분석결과 비교)\n",
    "    def inLineWith_DifferentFrom(intended_re, dct_result):\n",
    "        if intended_re == dct_result:\n",
    "            in_di_re = 'in line with'\n",
    "        else:\n",
    "            in_di_re = 'different from'\n",
    "        return in_di_re\n",
    "    \n",
    "    in_di_result = inLineWith_DifferentFrom(intended_re, dct_result)\n",
    "        \n",
    "    # Sentence 2\n",
    "    if dct_result == 'Surroundings matter a lot':\n",
    "        Sentence_2 = ['It seems that the significance of setting is high in your writing, which is ', in_di_result,'your intentions.']\n",
    "    elif dct_result == 'Somewhat important':\n",
    "        Sentence_2 = ['It seems that the significance of setting is moderate in your writing, which is ', in_di_result, 'your intentions.']\n",
    "    elif dct_result == 'Not a big factor':\n",
    "        Sentence_2 =- ['It seems that the significance of setting is moderate in your writing, which is ', in_di_result, 'your intentions.']\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # 문장 삽입 조건 판단(heavy emphasis / moderate emphasis / minimal emphasis)\n",
    "    \n",
    "    \n",
    "    # Sentence 3\n",
    "    Sentence_3 = ['In addition, the admitted students tend to choose to display a', , 'on setting for this prompt.']\n",
    "    \n",
    "    # 결과해석\n",
    "    # intended_re : intended setting by you\n",
    "    # dct_result : detected setting value of personal essay\n",
    "    # selected_prompt_number : 선택한 프롬프트 질문\n",
    "    # Sentence_1 : 첫번째 문장\n",
    "    \n",
    "    return intended_re, dct_result, selected_prompt_number, Sentence_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:256: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Surroundings matter a lot', 'Surroundings matter a lot', ['prompt #.3'])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EmphasisOnSetting('promt_3', input_text, 'alot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000명의 에세이 평균과 비교해서 ideal, overboard, lacking 구분할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lackigIdealOverboard(group_mean, personal_value): # group_mean: 1000명 평균, personal_value|:개인값\n",
    "        ideal_mean = group_mean\n",
    "        one_ps_char_desc = personal_value\n",
    "        #최대, 최소값 기준으로 구간설정. 구간비율 30% => 0.3으로 설정\n",
    "        min_ = int(ideal_mean-ideal_mean*0.6)\n",
    "        # #print('min_', min_)\n",
    "        max_ = int(ideal_mean+ideal_mean*0.6)\n",
    "        # #print('max_: ', max_)\n",
    "        div_ = int(((ideal_mean+ideal_mean*0.6)-(ideal_mean-ideal_mean*0.6))/3)\n",
    "        # #print('div_:', div_)\n",
    "\n",
    "        #결과 판단 Lacking, Ideal, Overboard\n",
    "        cal_abs = abs(ideal_mean - one_ps_char_desc) # 개인 - 단체 값의 절대값계산\n",
    "\n",
    "        # #print('cal_abs 절대값 :', cal_abs)\n",
    "        compare7 = (one_ps_char_desc + ideal_mean)/6\n",
    "        compare6 = (one_ps_char_desc + ideal_mean)/5\n",
    "        compare5 = (one_ps_char_desc + ideal_mean)/4\n",
    "        compare4 = (one_ps_char_desc + ideal_mean)/3\n",
    "        compare3 = (one_ps_char_desc + ideal_mean)/2\n",
    "        # #print('compare7 :', compare7)\n",
    "        # #print('compare6 :', compare6)\n",
    "        # #print('compare5 :', compare5)\n",
    "        # #print('compare4 :', compare4)\n",
    "        # #print('compare3 :', compare3)\n",
    "\n",
    "\n",
    "\n",
    "        if one_ps_char_desc > ideal_mean: # 개인점수가 평균보다 클 경우는 overboard\n",
    "            if cal_abs > compare3: # 37 개인점수가 개인평균차의 절대값보다 클 경우, 즉 차이가 많이 날경우\n",
    "                # #print(\"Overboard: 2\")\n",
    "                result = 2 #overboard\n",
    "                #score = 1\n",
    "            elif cal_abs > compare4:\n",
    "                # #print(\"Overvoard: 2\")\n",
    "                result = 2\n",
    "                #score = 2\n",
    "            elif cal_abs > compare5:\n",
    "                # #print(\"Overvoard: 2\")\n",
    "                result = 2\n",
    "                #score = 3\n",
    "            elif cal_abs > compare6:\n",
    "                # #print(\"Overvoard: 2\")\n",
    "                result = 2\n",
    "                #score = 4\n",
    "            else:\n",
    "                # #print(\"Ideal: 1\")\n",
    "                result = 1\n",
    "                #score = 5\n",
    "        elif one_ps_char_desc < ideal_mean: # 개인점수가 평균보다 작을 경우 lacking\n",
    "            if cal_abs > compare3: # 37 개인점수가 개인평균차의 절대값보다 클 경우, 즉 차이가 많이 날경우\n",
    "                # #print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                #score = 1\n",
    "            elif cal_abs > compare4:\n",
    "                # #print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                #score = 2\n",
    "            elif cal_abs > compare5:\n",
    "                # #print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                #score = 3\n",
    "            elif cal_abs > compare6:\n",
    "                # #print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                #score = 4\n",
    "            else:\n",
    "                # #print(\"Ideal: 1\")\n",
    "                result = 1\n",
    "                #score = 5\n",
    "                \n",
    "        else:\n",
    "            # #print(\"Ideal: 1\")\n",
    "            result = 1\n",
    "            #score = 5\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ILO mean ideal lacking overboard\n",
    "# 1000명의 평균 값을 변경하려면 \"1000명의 평균값으로 int \"을 적절한 숫자(int)로 바꿔야함!!\n",
    "# re_setting_ILO = lackigIdealOverboard(\"1000명의 평균값으로 int \", \"입력한 계산 결과값 int\")\n",
    "# 0:lacking, 1:ideal, 2:overbaord\n",
    "re_setting_ILO = lackigIdealOverboard(4, result_setting[0])\n",
    "re_setting_ILO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(re_setting_ILO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
