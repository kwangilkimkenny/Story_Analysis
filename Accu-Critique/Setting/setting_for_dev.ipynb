{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아나콘다 가상환경 office:  py37TF2\n",
    "# home : py37Keras\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import multiprocessing\n",
    "import io\n",
    "from gensim.models import Phrases\n",
    "from textblob import TextBlob\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간, 공간, 장소를 알려주는 단어 추출하여 카운트\n",
    "def find_setting_words(text):\n",
    "    # Create Doc object\n",
    "    doc2 = nlp(text)\n",
    "    \n",
    "    setting_list = []\n",
    "    # Identify by label FAC(building etc), GPE(countries, cities..), LOC(locaton), TIME\n",
    "    fac_r = [ent.text for ent in doc2.ents if ent.label_ == 'FAC']\n",
    "    setting_list.append(fac_r)\n",
    "    \n",
    "    gpe_r = [ent.text for ent in doc2.ents if ent.label_ == 'GPE']\n",
    "    setting_list.append(gpe_r)\n",
    "    \n",
    "    loc_r = [ent.text for ent in doc2.ents if ent.label_ == 'LOC']\n",
    "    setting_list.append(loc_r)\n",
    "    \n",
    "    time_r = [ent.text for ent in doc2.ents if ent.label_ == 'TIME']\n",
    "    setting_list.append(time_r)\n",
    "    \n",
    "    #추출된 항목들\n",
    "    all_setting_words = sum(setting_list, [])\n",
    "    \n",
    "    #셋팅 추출 항목들의 총 수\n",
    "    get_setting_list = len(all_setting_words)\n",
    "    \n",
    "    # Return all setting words\n",
    "    return get_setting_list, all_setting_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Bloomington Normal is almost laughably cliché for a midwestern city. Vast swathes of corn envelop winding roads and the heady smell of BBQ smoke pervades the countryside every summer. Yet, underlying the trite norms of Normal is the prescriptive force of tradition—the expectation to fulfill my role as a female Filipino by playing Debussy in the yearly piano festival and enrolling in multivariable calculus instead of political philosophy.So when I discovered the technical demand of bebop, the triplet groove, and the intricacies of chordal harmony after ten years of grueling classical piano, I was fascinated by the music's novelty. Jazz guitar was not only evocative and creative, but also strangely liberating. I began to explore different pedagogical methods, transcribe solos from the greats, and experiment with various approaches until my own unique sound began to develop. And, although I did not know what would be the 'best' route for me to follow as a musician, the freedom to forge whatever path I felt was right seemed to be exactly what I needed; there were no expectations for me to continue in any particular way—only the way that suited my own desires.While journeying this trail, I found myself at Interlochen Arts Camp the summer before my junior year. Never before had I been immersed in an environment so conducive to musical growth: I was surrounded by people intensely passionate about pursuing all kinds of art with no regard for ideas of what art 'should' be. I knew immediately that this would be a perfect opportunity to cultivate my sound, unbounded by the limits of confining tradition. On the first day of camp, I found that my peer guitarist in big band was another Filipino girl from Illinois. Until that moment, my endeavors in jazz guitar had been a solitary effort; I had no one with whom to collaborate and no one against whom I could compare myself, much less someone from a background mirroring my own. I was eager to play with her, but while I quickly recognized a slew of differences between us—different heights, guitars, and even playing styles—others seemed to have trouble making that distinction during performances. Some even went as far as calling me 'other-Francesca.' Thus, amidst the glittering lakes and musky pine needles of Interlochen, I once again confronted Bloomington's frustrating expectations.After being mistaken for her several times, I could not help but view Francesca as a standard of what the 'female Filipino jazz guitarist' should embody. Her improvisatory language, comping style and even personal qualities loomed above me as something I had to live up to. Nevertheless, as Francesca and I continued to play together, it was not long before we connected through our creative pursuit. In time, I learned to draw inspiration from her instead of feeling pressured to follow whatever precedent I thought she set. I found that I grew because of, rather than in spite of, her presence; I could find solace in our similarities and even a sense of comfort in an unfamiliar environment without being trapped by expectation. Though the pressure to conform was still present—and will likely remain present in my life no matter what genre I'm playing or what pursuits I engage in—I learned to eschew its corrosive influence and enjoy the rewards that it brings. While my encounter with Francesca at first sparked a feeling of pressure to conform in a setting where I never thought I would feel its presence, it also carried the warmth of finding someone with whom I could connect. Like the admittedly trite conditions of my hometown, the resemblances between us provided comfort to me through their familiarity. I ultimately found that I can embrace this warmth while still rejecting the pressure to succumb to expectations, and that, in the careful balance between these elements, I can grow in a way that feels both like discove\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_setting = list(find_setting_words(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, ['Interlochen Arts Camp', 'Illinois', 'Interlochen', 'Bloomington']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intended Setting \n",
    "# 입력 : Surroundings matter a lot : 'alot', Somewhat important: 'impt', Not a big factor : 'notBigFactor'\n",
    "def intendedSetting(intended_setting_input):\n",
    "    if intended_setting_input == 'alot':\n",
    "        int_setting_result = 'Surroundings matter a lot'\n",
    "    elif intended_setting_input == 'impt':\n",
    "        int_setting_result = 'Somewhat important'\n",
    "    else: # not a big factor\n",
    "        int_setting_result = 'Not a big factor'\n",
    "    return int_setting_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Setting_analysis(text):\n",
    "\n",
    "    essay_input_corpus = str(text) #문장입력\n",
    "    essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "    #print('essay_input_corpus :', essay_input_corpus)\n",
    "    \n",
    "    sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화 > 문장으로 구분\n",
    "    total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "    total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "    split_sentences = []\n",
    "    for sentence in sentences:\n",
    "        processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "        words = processed.split()\n",
    "        split_sentences.append(words)\n",
    "\n",
    "    skip_gram = 1\n",
    "    workers = multiprocessing.cpu_count()\n",
    "    bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "    model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "    model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "    \n",
    "    #모델 설계 완료\n",
    "\n",
    "    #setting을 표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "    location_list = ['above', 'behind','below','beside','betweed','by','in','inside','near',\n",
    "                     'on','over','through']\n",
    "    time_list = ['after', 'before','by','during','from','on','past','since','through','to','until','upon']\n",
    "      \n",
    "    movement_list = ['against','along','down','from','into','off','on','onto','out of','toward','up','upon']\n",
    "    \n",
    "    palce_terrain_type_list = ['wood', 'forest', 'copse', 'bush', 'trees', 'stand',\n",
    "                                'swamp', 'marsh', 'wetland', 'fen', 'bog', 'moor', 'heath', 'fells', 'morass',\n",
    "                                'jungle', 'rainforest', 'cloud forest','plains', 'fields', 'grass', 'grassland', \n",
    "                                'savannah', 'flood plain', 'flats', 'prairie','tundra', 'iceberg', 'glacier', \n",
    "                                'snowfields','hills', 'highland,' 'heights', 'plateau', 'badland', 'kame', 'shield',\n",
    "                                'downs', 'downland', 'ridge', 'ridgeline','hollow,' 'valley',' vale','glen', 'dell',\n",
    "                                'mountain', 'peak', 'summit', 'rise', 'pass', 'notch', 'crown', 'mount', 'switchback',\n",
    "                                'furth','canyon', 'cliff', 'bluff,' 'ravine', 'gully', 'gulch', 'gorge',\n",
    "                                'desert', 'scrub', 'waste', 'wasteland', 'sands', 'dunes',\n",
    "                                'volcano', 'crater', 'cone', 'geyser', 'lava fields']\n",
    "    \n",
    "    water_list = ['ocean', 'sea', 'coast', 'beach', 'shore', 'strand','bay', 'port', 'harbour', 'fjord', 'vike',\n",
    "                  'cove', 'shoals', 'lagoon', 'firth', 'bight', 'sound', 'strait', 'gulf', 'inlet', 'loch', \n",
    "                  'bayou','dock', 'pier', 'anchorage', 'jetty', 'wharf', 'marina', 'landing', 'mooring', 'berth', \n",
    "                  'quay', 'staith','river', 'stream', 'creek', 'brook', 'waterway', 'rill','delta', 'bank', 'runoff',\n",
    "                  'channel', 'bend', 'meander', 'backwater','lake', 'pool', 'pond', 'dugout', 'fountain', 'spring', \n",
    "                  'watering-hole', 'oasis','well', 'cistern', 'reservoir','waterfall', 'falls', 'rapids', 'cataract', \n",
    "                  'cascade','bridge', 'crossing', 'causeway', 'viaduct', 'aquaduct', 'ford', 'ferry','dam', 'dike', \n",
    "                  'bar', 'canal', 'ditch','peninsula', 'isthmus', 'island', 'isle', 'sandbar', 'reef', 'atoll', \n",
    "                  'archipelago', 'cay','shipwreck', 'derelict']\n",
    "    \n",
    "    \n",
    "    outdoor_places_list = ['clearing', 'meadow', 'grove', 'glade', 'fairy ring','earldom', 'fief', 'shire',\n",
    "                            'ruin', 'acropolis', 'desolation', 'remnant', 'remains',\n",
    "                            'henge', 'cairn', 'circle', 'mound', 'barrow', 'earthworks', 'petroglyphs',\n",
    "                            'lookout', 'aerie', 'promontory', 'outcropping', 'ledge', 'overhang', 'mesa', 'butte',\n",
    "                            'outland', 'outback', 'territory', 'reaches', 'wild', 'wilderness', 'expanse',\n",
    "                            'view', 'vista', 'tableau', 'spectacle', 'landscape', 'seascape', 'aurora', 'landmark',\n",
    "                            'battlefield', 'trenches', 'gambit', 'folly', 'conquest', 'claim', 'muster', 'post',\n",
    "                            'path', 'road', 'track', 'route', 'highway', 'way', 'trail', 'lane', 'thoroughfare', 'pike',\n",
    "                            'alley', 'street', 'avenue', 'boulevard', 'promenade', 'esplande', 'boardwalk',\n",
    "                            'crossroad', 'junction', 'intersection', 'turn', 'corner','plaza', 'terrace', 'square', \n",
    "                            'courtyard', 'court', 'park', 'marketplace', 'bazaar', 'fairground','realm', 'land', 'country',\n",
    "                            'nation', 'state', 'protectorate', 'empire', 'kingdom', 'principality','domain', 'dominion',\n",
    "                            'demesne', 'province', 'county', 'duchy', 'barony', 'baronetcy', 'march', 'canton']\n",
    "\n",
    "    \n",
    "    underground_list = ['pit', 'hole', 'abyss', 'sinkhole', 'crack', 'chasm', 'scar', 'rift', 'trench', 'fissure',\n",
    "                        'cavern', 'cave', 'gallery', 'grotto', 'karst',\n",
    "                        'mine', 'quarry', 'shaft', 'vein','graveyard', 'cemetery',\n",
    "                        'darkness', 'shadow', 'depths', 'void','maze', 'labyrinth'\n",
    "                        'tomb', 'grave', 'crypt', 'sepulchre', 'mausoleum', 'ossuary', 'boneyard']\n",
    "                        \n",
    "    living_places_list = ['nest', 'burrow', 'lair', 'den', 'bolt-hole', 'warren', 'roost', 'rookery', 'hibernaculum',\n",
    "                         'home', 'rest', 'hideout', 'hideaway', 'retreat', 'resting-place', 'safehouse', 'sanctuary',\n",
    "                         'respite', 'lodge','slum', 'shantytown', 'ghetto','camp', 'meeting place,' 'bivouac', 'campsite', \n",
    "                         'encampment','tepee', 'tent', 'wigwam', 'shelter', 'lean-to', 'yurt','house', 'mansion', 'estate',\n",
    "                         'villa','hut', 'palace', 'outbuilding', 'shack tenement', 'hovel', 'manse', 'manor', 'longhouse',\n",
    "                         'cottage', 'cabin','parsonage', 'rectory', 'vicarge', 'friary', 'priory','abbey', 'monastery', \n",
    "                         'nunnery', 'cloister', 'convent', 'hermitage','castle', 'keep', 'fort', 'fortress', 'citadel', \n",
    "                         'bailey', 'motte', 'stronghold', 'hold', 'chateau', 'outpost', 'redoubt',\n",
    "                         'town', 'village', 'hamlet', 'city', 'metropolis','settlement', 'commune']\n",
    "\n",
    "    building_facilities_list = ['temple', 'shrine', 'church', 'cathedral', 'tabernacle', 'ark', 'sanctum', 'parish', \n",
    "                                'chapel', 'synagogue', 'mosque','pyramid', 'ziggurat', 'prison', 'jail', 'dungeon',\n",
    "                                'oubliette', 'hospital', 'hospice', 'stocks', 'gallows','asylum', 'madhouse', 'bedlam',\n",
    "                                'vault', 'treasury', 'warehouse', 'cellar', 'relicry', 'repository',\n",
    "                                'barracks', 'armoury','sewer', 'gutter', 'catacombs', 'dump', 'middens', 'pipes', 'baths', 'heap',\n",
    "                                'mill', 'windmill', 'sawmill', 'smithy', 'forge', 'workshop', 'brickyard', 'shipyard', 'forgeworks',\n",
    "                                'foundry','bakery', 'brewery', 'almshouse', 'counting house', 'courthouse', 'apothecary', 'haberdashery', 'cobbler',\n",
    "                                'garden', 'menagerie', 'zoo', 'aquarium', 'terrarium', 'conservatory', 'lawn', 'greenhouse',\n",
    "                                'farm', 'orchard', 'vineyard', 'ranch', 'apiary', 'farmstead', 'homestead',\n",
    "                                'pasture', 'commons', 'granary', 'silo', 'crop','barn', 'stable', 'pen', 'kennel', 'mews', 'hutch', \n",
    "                                'pound', 'coop', 'stockade', 'yard', 'lumber yard','tavern', 'inn', 'pub', 'brothel', 'whorehouse',\n",
    "                                'cathouse', 'discotheque','lighthouse', 'beacon','amphitheatre', 'colosseum', 'stadium', 'arena', \n",
    "                                'circus','academy', 'university', 'campus', 'college', 'library', 'scriptorium', 'laboratory', \n",
    "                                'observatory', 'museum']\n",
    "    \n",
    "    \n",
    "    architecture_list = ['hall', 'chamber', 'room','nave', 'aisle', 'vestibule',\n",
    "                        'antechamber', 'chantry', 'pulpit','dome', 'arch', 'colonnade',\n",
    "                        'stair', 'ladder', 'climb', 'ramp', 'steps',\n",
    "                        'portal', 'mouth', 'opening', 'door', 'gate', 'entrance', 'maw',\n",
    "                        'tunnel', 'passage', 'corridor', 'hallway', 'chute', 'slide', 'tube', 'trapdoor',\n",
    "                        'tower', 'turret', 'belfry','wall', 'fortifications', 'ramparts', 'pallisade', 'battlements',\n",
    "                        'portcullis', 'barbican','throne room', 'ballroom','roof', 'rooftops', 'chimney', 'attic',\n",
    "                        'loft', 'gable', 'eaves', 'belvedere','balcony', 'balustrade', 'parapet', 'walkway', 'catwalk',\n",
    "                        'pavillion', 'pagoda', 'gazebo','mirror', 'glass', 'mere','throne', 'seat', 'dais',\n",
    "                        'pillar', 'column', 'stone', 'spike', 'rock', 'megalith', 'menhir', 'dolmen', 'obelisk',\n",
    "                        'statue', 'giant', 'head', 'arm', 'leg', 'body', 'chest', 'body', 'face', 'visage', 'gargoyle', 'grotesque',\n",
    "                        'fire', 'flame', 'bonfire', 'hearth', 'fireplace', 'furnace', 'stove','window', 'grate', 'peephole', \n",
    "                        'arrowslit', 'slit', 'balistraria', 'lancet', 'aperture', 'dormerl']\n",
    "    \n",
    "    \n",
    "    setting_words_filter_list = location_list + time_list + movement_list + palce_terrain_type_list + water_list + outdoor_places_list + underground_list + underground_list + living_places_list + building_facilities_list + architecture_list\n",
    "\n",
    "    \n",
    "    ####문장에 setting_words_filter_list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "    #우선 토큰화한다.\n",
    "    retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "    token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "    # print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "    # 리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "    filtered_setting_text = []\n",
    "    for k in token_input_text:\n",
    "        for j in setting_words_filter_list:\n",
    "            if k == j:\n",
    "                filtered_setting_text.append(j)\n",
    "    \n",
    "    # print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "    \n",
    "    filtered_setting_text_ = set(filtered_setting_text) #중복제거\n",
    "    filtered_setting_text__ = list(filtered_setting_text_) #다시 리스트로 변환\n",
    "    # print (filtered_setting_text__) # 중복값 제거 확인\n",
    "    \n",
    "    # 셋팅의 장소관련 단어 추출\n",
    "    extract_setting_words = list(find_setting_words(text))\n",
    "    \n",
    "    # 문장내 모든 셋팅 단어 추출\n",
    "    tot_setting_words = extract_setting_words[1] + filtered_setting_text__\n",
    "    \n",
    "    # 셋팅단어가 포함된 문장을 찾아내서 추출하기\n",
    "    # if 셋팅단어가 문장에 있다면, 그 문장을 추출(.로 split한 문장 리스트)해서 리스트로 저장한다.\n",
    "    \n",
    "    # print('sentences: ', sentences) # .로 구분된 전체 문장\n",
    "    \n",
    "    sentence_to_words = word_tokenize(essay_input_corpus) # 총 문장을 단어 리스트로 변환\n",
    "    # print('sentence_to_words:', sentence_to_words)\n",
    "    \n",
    "    # 셋팅단어가 포함된 문장을 찾아내서 추출\n",
    "    extrace_sentence_and_setting_words = [] # 이것은 \"문장\", '셋팅단어' ... 합쳐서 리스트로 저장\n",
    "    extract_only_sentences_include_setting_words = [] # 셋팅 단어가 포함된 문장만 리스트로 저장\n",
    "    for sentence in sentences: # 문장을 하나씩 꺼내온다.\n",
    "        for item in tot_setting_words: # 셋팅 단어를 하나씩 꺼내온다.\n",
    "            if item in word_tokenize(sentence): # 꺼낸 문장을 단어로 나누고, 그 안에 셋팅 단어가 있다면\n",
    "                extrace_sentence_and_setting_words.append(sentence) # 셋팅 단어가 포함된 문장을 별도로 저장한다.\n",
    "                extrace_sentence_and_setting_words.append(item) # 셋팅 단어도 추가로 저장한다. \n",
    "                \n",
    "                extract_only_sentences_include_setting_words.append(sentence)\n",
    "                \n",
    "                \n",
    "                ## 찾는 단어 수 대로 문장을 모두 별도 저장하기때문에 문장이 중복 저장된다. 한번만 문장이 저장되도록 하자. \n",
    "                ## 문장. '단어' , '단어' 이런 식으로다가 수정해야함. 중복리스트를 제거하면 됨.\n",
    "    # 중복리스트를 제거한다.\n",
    "    extrace_sentence_with_setting_words_re = set(extrace_sentence_and_setting_words)\n",
    "    #print('extrace_sentence_and_setting_words(문장+단어)) :', extrace_sentence_with_setting_words_re)\n",
    "    \n",
    "    extract_only_sentences_include_setting_words_re = set(extract_only_sentences_include_setting_words) #중복제거\n",
    "    #print('extract_only_sentences_include_setting_words(오직 셋팅 포함 문장):', extract_only_sentences_include_setting_words_re)\n",
    "    \n",
    "    # 단, 소문자로 문장이 저장되어 있어서, 동일한 원문을 찾을 수 없다. 소문자로 되어있는 문장을 통해서 대문자가 섞여있는 원문을 찾자\n",
    "    # 방법) 소문자 문장을 단어로로 토크나이즈한 후 리스트로 만든다. 대문자 문장도 단어로 토크나이즈한 후 리스트로 만든다.\n",
    "    # 두 개의 리스트를 비교해서 같은 단어가 3개 혹은 5개 이상 나오면 대문자 문장의 원문을 매칭한다. 끝!\n",
    "    \n",
    "    #아래 메소드에 리스트로된 문장 삽입, set 함수로 처리된것을 다시 list로 변환해야 첫 글자를 대문자로 바꿀 수 있다.\n",
    "    lower_text_input = list(extract_only_sentences_include_setting_words_re)\n",
    "    #print('lower_text_input: ', lower_text_input[0])\n",
    "    \n",
    "    ######################################################################################\n",
    "    ###### 소문자 문장으로 대문자 포함원 원문 추출하는 함수 ########\n",
    "    # essay_input_corpus : 최초의 입력문자를 스트링으로 변환한 원본\n",
    "    def find_original_sentence(lower_text_input, essay_input_corpus):\n",
    "        \n",
    "        #1)원본 전체을 문장으로 토큰화\n",
    "        sentence_tokenized = sent_tokenize(essay_input_corpus)\n",
    "        #print(\"======================================\")\n",
    "        #print('sentence_tokenized:',sentence_tokenized)\n",
    "        #문장으로 토큰화한 것을 리스트로 묶어서 다시 단어로 토큰화한다. \n",
    "        word_tokenized = [] #입력에세이 원본의 토큰화된 리스트화!\n",
    "        for st_to in sentence_tokenized:\n",
    "            word_tokenized.append(word_tokenize(st_to))\n",
    "        #print(\"======================================\")\n",
    "        # 이렇게 되어 있을 것이다 -> 문장으로 구분되어 리스트로 나뉘고 다시 단어로 분할되어 리스트[['단어','단어', ...], ['단어','단어', ...]...]\n",
    "        #print('word_tokenized:', word_tokenized)\n",
    "        \n",
    "        \n",
    "        #2)다음으로 계산 추출된 소문자로 변환된 셋팅단어 포함 문장의 단어에 대해서 첫 글자를 대문자로 만든다.\n",
    "        capital_text = []\n",
    "        for lt in lower_text_input:  \n",
    "            capital_text.append(lt.capitalize())\n",
    "        #print(\"======================================\")    \n",
    "        #print('captal_text(첫글자 대문자로 변환되었는지 확인!!!!!!!!!!):', capital_text) # 잘됨!\n",
    "        \n",
    "        capital_token_text_list = []\n",
    "        for cpt_item in capital_text:\n",
    "            #단어로 분할해서 리스트에 담는다.\n",
    "            capital_token_text_list.append(word_tokenize(cpt_item))\n",
    "        #print(\"======================================\")\n",
    "        # 이렇게 되어 있을 것이다 -> 문장으로 구분되어 리스트로 나뉘고 다시 단어로 분할되어 리스트[['단어','단어', ...], ['단어','단어', ...]\n",
    "        #print('captal_token_text_list:',capital_token_text_list)\n",
    "        \n",
    "        \n",
    "        # 이제 아래 두개의 리스트를 비교해서 원본을 찾아야 한다.그리고 다시 찾은 원본토큰화된 단어 리스트를 문장으로 복원한다.\n",
    "        \n",
    "        # word_tokenized : 입력에세이 원본의 토큰화된 리스트화! (원본문장)\n",
    "        # capital_token_text_list : 추출된 에세이 결과물 토큰화 (입력문장)\n",
    "        \n",
    "        # print('word_tokenized:', word_tokenized)\n",
    "        # print(('capital_token_text_list:', capital_token_text_list))\n",
    "        \n",
    "        # 셋팅 표현이 포함된 최종 문장의 리트스 추출\n",
    "        count_ct_item = 0\n",
    "        included_character_exp = []\n",
    "        for ct in capital_token_text_list:\n",
    "            for wt in word_tokenized:\n",
    "                for ct_item in ct:\n",
    "                    if count_ct_item >= 5: # 겹치는 단어가 4개 이상이면 같은 문장이라고 판단하자 \n",
    "                        # 같은 문장이기 땜누에 원본 리스트의 단어들을 하나의 문장으로 만들어서 저장하자\n",
    "                        re_cpt = ' '.join(wt).capitalize()\n",
    "                        included_character_exp.append(re_cpt)\n",
    "                    elif ct_item in wt: # 리스트 안에 비교리스트가 있다면, 단어 수 카운트하고 for문 돌림\n",
    "                        count_ct_item += 1\n",
    "                        #print('count_ct_item:', count_ct_item)\n",
    "                    else: # 비교 후 겹치는 값이 없다면 패스\n",
    "                        pass\n",
    "                    \n",
    "        # 최종결과물 첫 글자 대문자로 복원\n",
    "        \n",
    "        # 최종 결과물 중복제거\n",
    "        result_origin = set(included_character_exp) #셋팅 단어를 사용한 총 문장을 리스트로 출력\n",
    "        setting_total_sentences_number = len(result_origin) # 셋팅 단어가 발견된 총 문장수를 구하라\n",
    "        return result_origin, setting_total_sentences_number\n",
    "    ####################################################################################\n",
    "    \n",
    "    \n",
    "    # 셋팅 단어가 포함된 모든 문장을 추출\n",
    "    find_origin_result = find_original_sentence(lower_text_input, essay_input_corpus)\n",
    "    totalSettingSentences = find_origin_result[0]\n",
    "    #print('totalSettingSentences:', totalSettingSentences)\n",
    "    \n",
    "    # 셋팅 단어가 포함된 총 문장 수\n",
    "    setting_total_sentences_number_re = find_origin_result[1]\n",
    "    ####################################################################################\n",
    "    # 합격자들의 평균 셋팅문장 사용 수(임의로 설정, 나중에 평균값 계산해서 적용할 것)\n",
    "    setting_total_sentences_number_of_admitted_student = 20\n",
    "    ####################################################################################\n",
    "    \n",
    "    \n",
    "    # 문장생성 부분  - Overall Emphasis on Setting의 첫 문장값 계산\n",
    "    \n",
    "    if setting_total_sentences_number_re > setting_total_sentences_number_of_admitted_student:\n",
    "        less_more_numb = abs(setting_total_sentences_number_re - setting_total_sentences_number_of_admitted_student)\n",
    "        over_all_sentence_1 = [less_more_numb, 'more']\n",
    "    elif setting_total_sentences_number_re < setting_total_sentences_number_of_admitted_student:\n",
    "        less_more_numb = abs(setting_total_sentences_number_re - setting_total_sentences_number_of_admitted_student)\n",
    "        over_all_sentence_1 = [less_more_numb, 'fewer']\n",
    "    elif setting_total_sentences_number_re == setting_total_sentences_number_of_admitted_student: # ??? 두값이 같을 경우\n",
    "        over_all_sentence_1 = ['a similar number of'] # ??????? 확인할 것\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        \n",
    "    for i in filtered_setting_text__:\n",
    "        ext_setting_sim_words_key = model.most_similar_cosmul(i) # 모델적용\n",
    "    \n",
    "    setting_total_count = len(filtered_setting_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 setting 표현 수\n",
    "    setting_count_ = len(filtered_setting_text__) # 중복제거된 setting표현 총 수\n",
    "        \n",
    "    result_setting_words_ratio = round(setting_total_count/total_words * 100, 2)\n",
    "    #return result_setting_words_ratio\n",
    "    \n",
    "    # 결과해석\n",
    "    # result_setting_words_ratio : 전체 문장에서 셋팅관련 단어의 사용비율(포함비율)\n",
    "    # total_sentences : 총 문장 수\n",
    "    # total_words : 총 단어 수\n",
    "    # setting_total_count : # 중복이 제거되지 않은 에세이 총 문장에 사용된 setting 표현 수\n",
    "    # setting_count_ : # 중복제거된 setting표현 총 수\n",
    "    # ext_setting_sim_words_key : 셋팅설정과 유사한 단어들 추출\n",
    "    # totalSettingSentences : 셋팅 단어가 포함된 모든 문장을 추출\n",
    "    # setting_total_sentences_number_re : 셋팅 단어가 포함된 총 문장 수\n",
    "    # over_all_sentence_1 :문장생성 \n",
    "    # tot_setting_words : 총 문장에서 셋팅 단어 추출\n",
    "    \n",
    "    return result_setting_words_ratio, total_sentences, total_words, setting_total_count, setting_count_, ext_setting_sim_words_key, totalSettingSentences, setting_total_sentences_number_re, over_all_sentence_1, tot_setting_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/gensim/models/base_any2vec.py:323: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n",
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:277: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12.34,\n",
       " 21,\n",
       " 705,\n",
       " 87,\n",
       " 22,\n",
       " [('freedom', 0.99934983253479),\n",
       "  ('path', 0.9984238147735596),\n",
       "  ('musician', 0.998212993144989),\n",
       "  ('felt', 0.9964879751205444),\n",
       "  ('right', 0.9937456846237183),\n",
       "  ('precedent', 0.9907154440879822),\n",
       "  ('follow', 0.9904447793960571),\n",
       "  ('whatever', 0.9899702072143555),\n",
       "  ('she', 0.989422082901001),\n",
       "  ('route', 0.988984227180481)],\n",
       " {\"And , although i did not know what would be the 'best ' route for me to follow as a musician , the freedom to forge whatever path i felt was right seemed to be exactly what i needed ; there were no expectations for me to continue in any particular way—only the way that suited my own desires.while journeying this trail , i found myself at interlochen arts camp the summer before my junior year .\",\n",
       "  'Bloomington normal is almost laughably cliché for a midwestern city .',\n",
       "  'Her improvisatory language , comping style and even personal qualities loomed above me as something i had to live up to .',\n",
       "  'I began to explore different pedagogical methods , transcribe solos from the greats , and experiment with various approaches until my own unique sound began to develop .',\n",
       "  'I found that i grew because of , rather than in spite of , her presence ; i could find solace in our similarities and even a sense of comfort in an unfamiliar environment without being trapped by expectation .',\n",
       "  'I knew immediately that this would be a perfect opportunity to cultivate my sound , unbounded by the limits of confining tradition .',\n",
       "  'I ultimately found that i can embrace this warmth while still rejecting the pressure to succumb to expectations , and that , in the careful balance between these elements , i can grow in a way that feels both like discove',\n",
       "  'I was eager to play with her , but while i quickly recognized a slew of differences between us—different heights , guitars , and even playing styles—others seemed to have trouble making that distinction during performances .',\n",
       "  'In time , i learned to draw inspiration from her instead of feeling pressured to follow whatever precedent i thought she set .',\n",
       "  'Jazz guitar was not only evocative and creative , but also strangely liberating .',\n",
       "  'Like the admittedly trite conditions of my hometown , the resemblances between us provided comfort to me through their familiarity .',\n",
       "  \"Never before had i been immersed in an environment so conducive to musical growth : i was surrounded by people intensely passionate about pursuing all kinds of art with no regard for ideas of what art 'should ' be .\",\n",
       "  'Nevertheless , as francesca and i continued to play together , it was not long before we connected through our creative pursuit .',\n",
       "  'On the first day of camp , i found that my peer guitarist in big band was another filipino girl from illinois .',\n",
       "  \"Some even went as far as calling me 'other-francesca . '\",\n",
       "  \"Though the pressure to conform was still present—and will likely remain present in my life no matter what genre i 'm playing or what pursuits i engage in—i learned to eschew its corrosive influence and enjoy the rewards that it brings .\",\n",
       "  \"Thus , amidst the glittering lakes and musky pine needles of interlochen , i once again confronted bloomington 's frustrating expectations.after being mistaken for her several times , i could not help but view francesca as a standard of what the 'female filipino jazz guitarist ' should embody .\",\n",
       "  'Until that moment , my endeavors in jazz guitar had been a solitary effort ; i had no one with whom to collaborate and no one against whom i could compare myself , much less someone from a background mirroring my own .',\n",
       "  'Vast swathes of corn envelop winding roads and the heady smell of bbq smoke pervades the countryside every summer .',\n",
       "  'While my encounter with francesca at first sparked a feeling of pressure to conform in a setting where i never thought i would feel its presence , it also carried the warmth of finding someone with whom i could connect .',\n",
       "  \"Yet , underlying the trite norms of normal is the prescriptive force of tradition—the expectation to fulfill my role as a female filipino by playing debussy in the yearly piano festival and enrolling in multivariable calculus instead of political philosophy.so when i discovered the technical demand of bebop , the triplet groove , and the intricacies of chordal harmony after ten years of grueling classical piano , i was fascinated by the music 's novelty .\"},\n",
       " 21,\n",
       " [1, 'more'],\n",
       " ['Interlochen Arts Camp',\n",
       "  'Illinois',\n",
       "  'Interlochen',\n",
       "  'Bloomington',\n",
       "  'way',\n",
       "  'through',\n",
       "  'view',\n",
       "  'route',\n",
       "  'camp',\n",
       "  'before',\n",
       "  'after',\n",
       "  'to',\n",
       "  'sound',\n",
       "  'above',\n",
       "  'on',\n",
       "  'city',\n",
       "  'in',\n",
       "  'up',\n",
       "  'during',\n",
       "  'until',\n",
       "  'by',\n",
       "  'trail',\n",
       "  'from',\n",
       "  'path',\n",
       "  'against',\n",
       "  'forge'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = Setting_analysis(input_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>result_setting_words_ratio</th>\n",
       "      <td>12.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_sentences</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_words</th>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>setting_total_count</th>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>setting_count_</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ext_setting_sim_words_key</th>\n",
       "      <td>[(freedom, 0.99934983253479), (path, 0.9984238...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totalSettingSentences</th>\n",
       "      <td>{I knew immediately that this would be a perfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>setting_total_sentences_number_re</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>over_all_sentence_1</th>\n",
       "      <td>[1, more]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tot_setting_words</th>\n",
       "      <td>[Interlochen Arts Camp, Illinois, Interlochen,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               Value\n",
       "result_setting_words_ratio                                                     12.34\n",
       "total_sentences                                                                   21\n",
       "total_words                                                                      705\n",
       "setting_total_count                                                               87\n",
       "setting_count_                                                                    22\n",
       "ext_setting_sim_words_key          [(freedom, 0.99934983253479), (path, 0.9984238...\n",
       "totalSettingSentences              {I knew immediately that this would be a perfe...\n",
       "setting_total_sentences_number_re                                                 21\n",
       "over_all_sentence_1                                                        [1, more]\n",
       "tot_setting_words                  [Interlochen Arts Camp, Illinois, Interlochen,..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.DataFrame(df, index = [\n",
    "                                'result_setting_words_ratio', 'total_sentences', 'total_words', \n",
    "                                'setting_total_count', 'setting_count_', 'ext_setting_sim_words_key', \n",
    "                                'totalSettingSentences', 'setting_total_sentences_number_re', 'over_all_sentence_1',\n",
    "                                'tot_setting_words'\n",
    "                                ], columns = ['Value']\n",
    "                  )\n",
    "\n",
    "# 데이터프레임 출력해봄                     \n",
    "df_ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# 650단어에서 또는 전체 단어에서 단락별 셋팅단어 활용 수 분석\n",
    "# 20% intro, 60% body1,2,3 20% conclusion\n",
    "##########################################################\n",
    "def paragraph_divide_ratio(text):\n",
    "\n",
    "    essay_input_corpus = str(text) #문장입력\n",
    "    essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "    sentences  = word_tokenize(essay_input_corpus) #문장 토큰화\n",
    "    # print('sentences:',sentences)\n",
    "\n",
    "    # 총 문장수 계산\n",
    "    total_sentences = len(sentences) # 토큰으로 처리된 총 문장 수\n",
    "    total_sentences = float(total_sentences)\n",
    "    #print('total_sentences:', total_sentences)\n",
    "\n",
    "    # 비율계산 시작\n",
    "    intro_n = round(total_sentences*0.2) # 20% 만 계산하기, 소수점이하는 반올림\n",
    "    body_1 = round(total_sentences*0.2) # 20% 만 계산하기, 소수점이하는 반올림\n",
    "    body_2 = round(total_sentences*0.2)\n",
    "    body_3 = round(total_sentences*0.2)\n",
    "    conclusion_n = round(total_sentences*0.2) # 20% 만 계산하기, 소수점이하는 반올림\n",
    "\n",
    "    #데이터셋 비율분할 완료\n",
    "    intro = sentences[:intro_n]\n",
    "    #print('intro :', intro)\n",
    "    body_1_ = sentences[intro_n:intro_n + body_1]\n",
    "    #print('body 1 :', body_1_)\n",
    "    body_2_ = sentences[intro_n + body_1:intro_n + body_1 + body_2]\n",
    "    #print('body 2 :', body_2_)\n",
    "    body_3_ = sentences[intro_n + body_1 + body_2:intro_n + body_1 + body_2 + body_3]\n",
    "    # print('body_3_ :', body_3_)\n",
    "    conclusion = sentences[intro_n + body_1 + body_2 + body_3 + 1 :]\n",
    "    # print('conclusion :', conclusion)\n",
    "    \n",
    "    #print('sentences:', sentences)\n",
    "    #데이터프레임으로 변환\n",
    "    df_sentences = pd.DataFrame(sentences,columns=['words'])\n",
    "    #print('sentences:',df_sentences)\n",
    "    \n",
    "    ######### setting 관련 단어 추출 #########\n",
    "    s_a_re = Setting_analysis(input_text)\n",
    "    tot_setting_words = s_a_re[9]\n",
    "\n",
    "    # 구간별 셋팅 단어가 몇개씩 포함되어 있는지 계산 method\n",
    "    def set_wd_conunter_each_parts(st_wd, each_parts_):\n",
    "        if each_parts_ == intro:\n",
    "            part_section = 'intro'\n",
    "        elif each_parts_ == body_1_:\n",
    "            part_section = 'body #1'\n",
    "        elif each_parts_ == body_2_:\n",
    "            part_section = 'body #2'\n",
    "        elif each_parts_ == body_3_:\n",
    "            part_section = 'body #3'\n",
    "        else: #conclusion\n",
    "            part_section = 'conclusion'\n",
    "        counter = 0\n",
    "        for set_itm in st_wd:\n",
    "            if set_itm in each_parts_:\n",
    "                counter += 1\n",
    "            else:\n",
    "                pass\n",
    "        return counter, part_section\n",
    "\n",
    "    # 구간별 셋팅 단어가 몇개씩 포함되어 있는지 계산 \n",
    "    intro_s_num = set_wd_conunter_each_parts(tot_setting_words, intro)\n",
    "    print('intor:', intro_s_num)\n",
    "    body_1_s_num = set_wd_conunter_each_parts(tot_setting_words, body_1_)\n",
    "    print('body1:', body_1_s_num)\n",
    "    body_2_s_num = set_wd_conunter_each_parts(tot_setting_words, body_2_)\n",
    "    print('body2:', body_2_s_num)\n",
    "    body_3_s_num = set_wd_conunter_each_parts(tot_setting_words, body_3_)\n",
    "    print('body3',body_3_s_num)\n",
    "    conclusion_s_num = set_wd_conunter_each_parts(tot_setting_words, conclusion)\n",
    "    print('conclusion:',conclusion_s_num)\n",
    "\n",
    "    \n",
    "    # 가장 많이 포함된 구간을 순서대로 추출\n",
    "    compare_parts_grup_nums = [] # 숫자와 항복명을 모두 저장(튜플을 리스트로)\n",
    "    compare_parts_grup_nums_and_parts = [] # 숫자만 리스트로\n",
    "    \n",
    "    compare_parts_grup_nums.append(intro_s_num[0])\n",
    "    compare_parts_grup_nums.append(intro_s_num[1])\n",
    "    compare_parts_grup_nums_and_parts.append(intro_s_num[0])\n",
    "\n",
    "    \n",
    "    compare_parts_grup_nums.append(body_1_s_num[0])\n",
    "    compare_parts_grup_nums.append(body_1_s_num[1])\n",
    "    compare_parts_grup_nums_and_parts.append(body_1_s_num[0])\n",
    "    \n",
    "    compare_parts_grup_nums.append(body_2_s_num[0])\n",
    "    compare_parts_grup_nums.append(body_2_s_num[1])\n",
    "    compare_parts_grup_nums_and_parts.append(body_2_s_num[0])\n",
    "    \n",
    "    compare_parts_grup_nums.append(body_3_s_num[0])\n",
    "    compare_parts_grup_nums.append(body_3_s_num[1])\n",
    "    compare_parts_grup_nums_and_parts.append(body_3_s_num[0])\n",
    "    \n",
    "    compare_parts_grup_nums.append(conclusion_s_num[0])\n",
    "    compare_parts_grup_nums.append(conclusion_s_num[1])\n",
    "    compare_parts_grup_nums_and_parts.append(conclusion_s_num[0])\n",
    "    \n",
    "    #compare_parts_grup_nums_and_parts =compare_parts_grup_nums_and_parts.sort(reverse=True)\n",
    "    \n",
    "    print('compare_parts_grup: ', compare_parts_grup_nums) # [7, 'intro', 11, 'body #1', 9, 'body #2', 9, 'body #3', 4, 'conclusion']\n",
    "    \n",
    "    #순서정렬\n",
    "    compare_parts_grup_nums_and_parts_sorted = sorted(compare_parts_grup_nums_and_parts, reverse=True)\n",
    "    print('compare_parts_grup_nums_and_parts(sorted)', compare_parts_grup_nums_and_parts_sorted) # [11, 9, 9, 7, 4]\n",
    "    print('compare_parts_grup_nums_and_parts :',compare_parts_grup_nums_and_parts)\n",
    "    \n",
    "    first_result = compare_parts_grup_nums_and_parts_sorted[0]\n",
    "    second_result = compare_parts_grup_nums_and_parts_sorted[1]\n",
    "    \n",
    "    get_first_re = compare_parts_grup_nums.index(first_result) #인덱스 위치찾기\n",
    "    print('get_firtst_re:',get_first_re)\n",
    "    #가장 많은 표현이 들어간 부분 추출(최종값)\n",
    "    first_snt_part = compare_parts_grup_nums[get_first_re + 1]\n",
    "    \n",
    "    get_second_re = compare_parts_grup_nums.index(second_result)\n",
    "    print('get_second_re:',get_second_re)\n",
    "    second_snt_part = compare_parts_grup_nums[get_second_re + 1] # 인덱스 다음 항목이 최종값\n",
    "\n",
    "    # 결과해석\n",
    "    # df_sentences: 모든 단어를 데이터프레임으로 변환\n",
    "    # tot_setting_words: : 추출한 셋팅 관련 단어 리스트로 변환\n",
    "    # first_snt_part: 문단중 가장 셋팅 관련 단어가 많은 부분 -> overall emphasis on setting의 3번째 문장으로 표현\n",
    "    # second_snt_part: 문잔중  셋팅 관련 단어가 두번째고 많은 부분 -> overall emphasis on setting의 3번째 문장으로 표현\n",
    "    # compare_parts_grup_nums_and_parts : intro body_1 body_2 body_3 conclusion 의 개인 에세이 계산 값\n",
    "    \n",
    "    return df_sentences, tot_setting_words, first_snt_part, second_snt_part, compare_parts_grup_nums_and_parts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intor: (7, 'intro')\n",
      "body1: (11, 'body #1')\n",
      "body2: (9, 'body #2')\n",
      "body3 (9, 'body #3')\n",
      "conclusion: (4, 'conclusion')\n",
      "compare_parts_grup:  [7, 'intro', 11, 'body #1', 9, 'body #2', 9, 'body #3', 4, 'conclusion']\n",
      "compare_parts_grup_nums_and_parts(sorted) [11, 9, 9, 7, 4]\n",
      "compare_parts_grup_nums_and_parts : [7, 11, 9, 9, 4]\n",
      "get_firtst_re: 2\n",
      "get_second_re: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:277: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           words\n",
       " 0    bloomington\n",
       " 1         normal\n",
       " 2             is\n",
       " 3         almost\n",
       " 4      laughably\n",
       " ..           ...\n",
       " 700         that\n",
       " 701        feels\n",
       " 702         both\n",
       " 703         like\n",
       " 704      discove\n",
       " \n",
       " [705 rows x 1 columns],\n",
       " ['Interlochen Arts Camp',\n",
       "  'Illinois',\n",
       "  'Interlochen',\n",
       "  'Bloomington',\n",
       "  'way',\n",
       "  'through',\n",
       "  'view',\n",
       "  'route',\n",
       "  'camp',\n",
       "  'before',\n",
       "  'after',\n",
       "  'to',\n",
       "  'sound',\n",
       "  'above',\n",
       "  'on',\n",
       "  'city',\n",
       "  'in',\n",
       "  'up',\n",
       "  'during',\n",
       "  'until',\n",
       "  'by',\n",
       "  'trail',\n",
       "  'from',\n",
       "  'path',\n",
       "  'against',\n",
       "  'forge'],\n",
       " 'body #1',\n",
       " 'body #2',\n",
       " [7, 11, 9, 9, 4])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = paragraph_divide_ratio(input_text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting PPT 13p\n",
    "# Emphasis on Setting\n",
    "\n",
    "# intended_setting_by_you 입력: Surroundings matter a lot : 'alot', Somewhat important: 'impt', Not a big factor : 'notBigFactor'\n",
    "# prompt_no : promt_1~7\n",
    "def EmphasisOnSetting(prompt_no, input_text, intended_setting_by_you):\n",
    "    #intended by you setting value\n",
    "    intended_re = intendedSetting(intended_setting_by_you)\n",
    "    \n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    # 1000명의 평균값 셋팅 벨류(임의로 설정, 나중에 평균값 계산해서 적용할 것)\n",
    "    group_setting_mean_value = 15\n",
    "    # 3번 문항에 대한 내용 입력부분 - 합격한 학생들의 평균값 적용\n",
    "    group_setting_mean_value_for_prompt = 'moderate emphasis' # 'heavy emphasis', 'moderate emphasis', 'minimal emphasis' 중 1개 서택\n",
    "    \n",
    "    # 각 구간의 셋팅 관련 표현의 합격자 평균값\n",
    "    group_setting_parts_mean_value = [9, 8, 9, 10, 3]\n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    #detected setting value\n",
    "    detected_setting_value_re = Setting_analysis(input_text)[4]\n",
    "    \n",
    "    # 조건판단, 오차를 +-20% 주자\n",
    "    if detected_setting_value_re > (group_setting_mean_value + round(group_setting_mean_value * 0.2)):\n",
    "        dct_result = 'Surroundings matter a lot'\n",
    "        less_more_number_re = abs(detected_setting_value_re - group_setting_mean_value) # 평균값보다 디텍팅한 값이 많을 경우 몇단어가 많은지\n",
    "        less_more_re = 'more' # overall Emphasis sentence 1 번째 문장 생성 부분 \n",
    "    elif detected_setting_value_re == group_setting_mean_value:\n",
    "        dct_result = 'Somewhat important'\n",
    "        less_more_number_re = abs(detected_setting_value_re - group_setting_mean_value) # 평균값보다 디텍팅한 값이 많을 경우 몇단어가 많은지\n",
    "        less_more_re = 'a similar number of' # overall Emphasis sentence 1 번째 문장 생성 부분 \n",
    "    elif detected_setting_value_re <= (group_setting_mean_value + round(group_setting_mean_value * 0.2)):\n",
    "        dct_result = 'Somewhat important'\n",
    "        less_more_number_re = abs(detected_setting_value_re - group_setting_mean_value) # 평균값보다 디텍팅한 값이 많을 경우 몇단어가 많은지\n",
    "        less_more_re = 'a similar number of' # overall Emphasis sentence 1 번째 문장 생성 부분 \n",
    "    elif detected_setting_value_re >= (group_setting_mean_value - round(group_setting_mean_value * 0.2)):\n",
    "        dct_result = 'Somewhat important'\n",
    "        less_more_re = 'a similar number of' # overall Emphasis sentence 1 번째 문장 생성 부분 \n",
    "    else: # detected_setting_value_re < group_setting_mean_value:\n",
    "        dct_result = 'Not a big factor'\n",
    "        less_more_number_re = abs(detected_setting_value_re - group_setting_mean_value) # 평균값보다 디텍팅한 값이 많을 경우 몇단어가 많은지\n",
    "        less_more_re = 'fewer' # overall Emphasis sentence 1 번째 문장 생성 부분 \n",
    "        \n",
    "        \n",
    "    # Setting Preferences by Admitted Students for 'Prompt #3'\n",
    "    selected_prompt_number = []\n",
    "    if prompt_no == \"promt_1\":\n",
    "        selected_prompt_number.append(\"prompt #.1\")\n",
    "    elif prompt_no == \"promt_2\":\n",
    "        selected_prompt_number.append(\"prompt #.2\")\n",
    "    elif prompt_no == \"promt_3\":\n",
    "        selected_prompt_number.append(\"prompt #.3\")\n",
    "    elif prompt_no == \"promt_4\":\n",
    "        selected_prompt_number.append(\"prompt #.4\")\n",
    "    elif prompt_no == \"promt_5\":\n",
    "        selected_prompt_number.append(\"prompt #.5\")\n",
    "    elif prompt_no == \"promt_6\":\n",
    "        selected_prompt_number.append(\"prompt #.6\")\n",
    "    elif prompt_no == \"promt_7\":\n",
    "        selected_prompt_number.append(\"prompt #.7\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # print('selected prompt number:', selected_prompt_number)\n",
    "    \n",
    "    # 문장 생성 부분 시작\n",
    "    # Sentence 1\n",
    "    if intended_re == 'Surroundings matter a lot':\n",
    "        Sentence_1 = 'You aimed to give high importance on setting in your personal statement.'\n",
    "    elif intended_re == 'Somewhat important':\n",
    "        Sentence_1 = 'You aimed to give moderate importance on setting in your personal statement.'\n",
    "    elif intended_re == 'Not a big factor':\n",
    "        Sentence_1 = 'You aimed to give low importance on setting in your personal statement.'\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # 문장 삽입 조건판단( 의도한 결과와 분석결과 비교)\n",
    "    def inLineWith_DifferentFrom(intended_re, dct_result):\n",
    "        if intended_re == dct_result:\n",
    "            in_di_re = 'in line with' # 2번 문항 생성 부분\n",
    "            compare_match_re = 'matches well' # 4번 문항 생성 부분\n",
    "            conicide_re = 'coincides' # 4번 문항 생성 부분\n",
    "        else:\n",
    "            in_di_re = 'different from' # 2번 문항 생성 부분\n",
    "            compare_match_re = 'does not match' # 4번 문항 생성 부분\n",
    "            conicide_re = 'does not coincide' # 4번 문항 생성 부분\n",
    "        return in_di_re, compare_match_re, conicide_re\n",
    "    \n",
    "    in_di_result = inLineWith_DifferentFrom(intended_re, dct_result)\n",
    "        \n",
    "    # Sentence 2\n",
    "    if dct_result == 'Surroundings matter a lot':\n",
    "        Sentence_2 = ['It seems that the significance of setting is high in your writing, which is ', in_di_result[0],'your intentions.']\n",
    "    elif dct_result == 'Somewhat important':\n",
    "        Sentence_2 = ['It seems that the significance of setting is moderate in your writing, which is ', in_di_result[0], 'your intentions.']\n",
    "    elif dct_result == 'Not a big factor':\n",
    "        Sentence_2 =- ['It seems that the significance of setting is moderate in your writing, which is ', in_di_result[0], 'your intentions.']\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # 문장 삽입 조건 판단(heavy emphasis / moderate emphasis / minimal emphasis)\n",
    "    \n",
    "    \n",
    "    # Sentence 3  - 이 부분은 합격한 학생의 평균값을 적용하는 부분임(group_setting_mean_value_for_prompt 값 설정한것 참고할 것)\n",
    "    Sentence_3 = ['In addition, the admitted students tend to choose to display a', group_setting_mean_value_for_prompt, 'on setting for this prompt.']\n",
    "    \n",
    "    \n",
    "    # Sentence 4 \n",
    "    Sentence_4 = ['It', in_di_result[1], 'with your intended direction for setting while it', in_di_result[2],'with the level of emphasis shown in your essay.']\n",
    "    \n",
    "    ####### Overall Emphasis on Setting ######\n",
    "    \n",
    "    sa_re = Setting_analysis(input_text)\n",
    "    words_desp_re = sa_re[8]\n",
    "    \n",
    "    overall_sentence_1 = ['Compared to the accepted case average for this prompt, you have spent', less_more_number_re, less_more_re,'setting indicators and', words_desp_re,'words to describe the setting.']\n",
    "    \n",
    "    adding_more_using_less_person = detected_setting_value_re + sa_re[7] # 8번째 리스트값이 셋팅단어가 포함된 문장임\n",
    "    print('adding_more_using_less_person :',adding_more_using_less_person)\n",
    "    ############### 합격자 평균값(계산해서 적용할 것, 현재값은 임의로 넣었음)\n",
    "    adding_more_using_less_group = 27\n",
    "    # 비교하여 overall_sentence_2 를 계산하기\n",
    "    if adding_more_using_less_person > adding_more_using_less_group:\n",
    "        overall_sent_2 = 'adding more'\n",
    "        overall_sentence_2 = ['You may consider', overall_sent_2, 'words to describe the setting in your story.']\n",
    "    elif adding_more_using_less_person < adding_more_using_less_group:\n",
    "        overall_sent_2 = 'using less'\n",
    "        overall_sentence_2 = ['You may consider', overall_sent_2, 'words to describe the setting in your story.']\n",
    "    else: #adding_more_using_less_person =  adding_more_using_less_group:\n",
    "        overall_sentence_2 = 'Overall, your setting description looks good compared with the accepted average.'\n",
    "        \n",
    "    first_2nd_parts = paragraph_divide_ratio(input_text) # 3, 4번째의 리스트 값이 아래 들어갈 문장님\n",
    "        \n",
    "    over_sentence_3 = ['Dividing up the personal statement in 5 equal parts by the word count, the accepted case average indicated that most number of setting descriptors are concentrated in the',  first_2nd_parts[2], 'and',  first_2nd_parts[3],'.']\n",
    "    \n",
    "    # 합격자 평규값\n",
    "    # group_setting_parts_mean_value\n",
    "    \n",
    "    first_2nd_parts[4] #함수 계산 값중에서 5번째 리스트 값\n",
    "    print('개인 구간별 계산 값: ', first_2nd_parts[4]) # [7, 11, 9, 9, 4]\n",
    "    \n",
    "    # 각각의 값을 비교하고, 0.3 의 오차범위에서 같으면 True \n",
    "    def compart(val_1, val_2):\n",
    "        if val_1 < (val_2 + val_2 * 0.3) and val_1 > (val_2 - val_2 * 0.3):\n",
    "            result_compart = True\n",
    "        else:\n",
    "            result_compart = False\n",
    "        return result_compart\n",
    "    \n",
    "    # 구간별 두개의 값(그룹, 개인) 비교 함수\n",
    "    def comp_each_parts(personal, group):\n",
    "        if personal == group: # 각 구간의 값이 일치하면\n",
    "            over_sentence_4 = ['Comparing this with your essay, we see a very similar pattern.']  \n",
    "        elif compart(personal[0], group[0]) and compart(personal[1], group[1]) and compart(personal[2], group[2]) and compart(personal[3], group[3]): # 30% 범위 내로 각 값이 같다면       \n",
    "            over_sentence_4 = ['Comparing this with your essay, we see a very similar pattern.'] \n",
    "            \n",
    "        elif personal[1] + personal[2] == group[1] + group[2]: # body1 + body 2 로 개인과 그릅울 비교\n",
    "            over_sentence_4 = ['Comparing this with your essay, we see some similarities in the pattern.']\n",
    "        elif personal[1] + personal[2] < (group[1] + group[2]) + (group[1] + group[2]) * 0.3:\n",
    "            over_sentence_4 = ['Comparing this with your essay, we see some similarities in the pattern.']\n",
    "        elif ersonal[1] + personal[2] > (group[1] + group[2]) - (group[1] + group[2]) * 0.3:\n",
    "            over_sentence_4 = ['Comparing this with your essay, we see some similarities in the pattern.']\n",
    "        else: # 각 구간들이 불일치\n",
    "            over_sentence_4 = ['Comparing this with your essay, we see a different pattern.']\n",
    "        return over_sentence_4\n",
    "                \n",
    "    over_sentence_4 = comp_each_parts(first_2nd_parts[4], group_setting_parts_mean_value)\n",
    "    \n",
    "    # 결과해석\n",
    "    # intended_re : intended setting by you\n",
    "    # dct_result : detected setting value of personal essay\n",
    "    # selected_prompt_number : 선택한 프롬프트 질문\n",
    "    # Sentence_1 ~ 4: 1~4번째 문장\n",
    "    \n",
    "    return intended_re, dct_result, selected_prompt_number, Sentence_1, Sentence_2, Sentence_3, Sentence_4, overall_sentence_1, overall_sentence_2, over_sentence_3, over_sentence_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:277: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding_more_using_less_person : 43\n",
      "intor: (7, 'intro')\n",
      "body1: (11, 'body #1')\n",
      "body2: (9, 'body #2')\n",
      "body3 (9, 'body #3')\n",
      "conclusion: (4, 'conclusion')\n",
      "compare_parts_grup:  [7, 'intro', 11, 'body #1', 9, 'body #2', 9, 'body #3', 4, 'conclusion']\n",
      "compare_parts_grup_nums_and_parts(sorted) [11, 9, 9, 7, 4]\n",
      "compare_parts_grup_nums_and_parts : [7, 11, 9, 9, 4]\n",
      "get_firtst_re: 2\n",
      "get_second_re: 4\n",
      "개인 구간별 계산 값:  [7, 11, 9, 9, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Surroundings matter a lot',\n",
       " 'Surroundings matter a lot',\n",
       " ['prompt #.3'],\n",
       " 'You aimed to give high importance on setting in your personal statement.',\n",
       " ['It seems that the significance of setting is high in your writing, which is ',\n",
       "  'in line with',\n",
       "  'your intentions.'],\n",
       " ['In addition, the admitted students tend to choose to display a',\n",
       "  'moderate emphasis',\n",
       "  'on setting for this prompt.'],\n",
       " ['It',\n",
       "  'matches well',\n",
       "  'with your intended direction for setting while it',\n",
       "  'coincides',\n",
       "  'with the level of emphasis shown in your essay.'],\n",
       " ['Compared to the accepted case average for this prompt, you have spent',\n",
       "  7,\n",
       "  'more',\n",
       "  'setting indicators and',\n",
       "  [1, 'more'],\n",
       "  'words to describe the setting.'],\n",
       " ['You may consider',\n",
       "  'adding more',\n",
       "  'words to describe the setting in your story.'],\n",
       " ['Dividing up the personal statement in 5 equal parts by the word count, the accepted case average indicated that most number of setting descriptors are concentrated in the',\n",
       "  'body #1',\n",
       "  'and',\n",
       "  'body #2',\n",
       "  '.'],\n",
       " ['Comparing this with your essay, we see some similarities in the pattern.'])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EmphasisOnSetting('promt_3', input_text, 'alot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 끝!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000명의 에세이 평균과 비교해서 ideal, overboard, lacking 구분할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lackigIdealOverboard(group_mean, personal_value): # group_mean: 1000명 평균, personal_value|:개인값\n",
    "        ideal_mean = group_mean\n",
    "        one_ps_char_desc = personal_value\n",
    "        #최대, 최소값 기준으로 구간설정. 구간비율 30% => 0.3으로 설정\n",
    "        min_ = int(ideal_mean-ideal_mean*0.6)\n",
    "        # #print('min_', min_)\n",
    "        max_ = int(ideal_mean+ideal_mean*0.6)\n",
    "        # #print('max_: ', max_)\n",
    "        div_ = int(((ideal_mean+ideal_mean*0.6)-(ideal_mean-ideal_mean*0.6))/3)\n",
    "        # #print('div_:', div_)\n",
    "\n",
    "        #결과 판단 Lacking, Ideal, Overboard\n",
    "        cal_abs = abs(ideal_mean - one_ps_char_desc) # 개인 - 단체 값의 절대값계산\n",
    "\n",
    "        # #print('cal_abs 절대값 :', cal_abs)\n",
    "        compare7 = (one_ps_char_desc + ideal_mean)/6\n",
    "        compare6 = (one_ps_char_desc + ideal_mean)/5\n",
    "        compare5 = (one_ps_char_desc + ideal_mean)/4\n",
    "        compare4 = (one_ps_char_desc + ideal_mean)/3\n",
    "        compare3 = (one_ps_char_desc + ideal_mean)/2\n",
    "        # #print('compare7 :', compare7)\n",
    "        # #print('compare6 :', compare6)\n",
    "        # #print('compare5 :', compare5)\n",
    "        # #print('compare4 :', compare4)\n",
    "        # #print('compare3 :', compare3)\n",
    "\n",
    "\n",
    "\n",
    "        if one_ps_char_desc > ideal_mean: # 개인점수가 평균보다 클 경우는 overboard\n",
    "            if cal_abs > compare3: # 37 개인점수가 개인평균차의 절대값보다 클 경우, 즉 차이가 많이 날경우\n",
    "                # #print(\"Overboard: 2\")\n",
    "                result = 2 #overboard\n",
    "                #score = 1\n",
    "            elif cal_abs > compare4:\n",
    "                # #print(\"Overvoard: 2\")\n",
    "                result = 2\n",
    "                #score = 2\n",
    "            elif cal_abs > compare5:\n",
    "                # #print(\"Overvoard: 2\")\n",
    "                result = 2\n",
    "                #score = 3\n",
    "            elif cal_abs > compare6:\n",
    "                # #print(\"Overvoard: 2\")\n",
    "                result = 2\n",
    "                #score = 4\n",
    "            else:\n",
    "                # #print(\"Ideal: 1\")\n",
    "                result = 1\n",
    "                #score = 5\n",
    "        elif one_ps_char_desc < ideal_mean: # 개인점수가 평균보다 작을 경우 lacking\n",
    "            if cal_abs > compare3: # 37 개인점수가 개인평균차의 절대값보다 클 경우, 즉 차이가 많이 날경우\n",
    "                # #print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                #score = 1\n",
    "            elif cal_abs > compare4:\n",
    "                # #print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                #score = 2\n",
    "            elif cal_abs > compare5:\n",
    "                # #print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                #score = 3\n",
    "            elif cal_abs > compare6:\n",
    "                # #print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                #score = 4\n",
    "            else:\n",
    "                # #print(\"Ideal: 1\")\n",
    "                result = 1\n",
    "                #score = 5\n",
    "                \n",
    "        else:\n",
    "            # #print(\"Ideal: 1\")\n",
    "            result = 1\n",
    "            #score = 5\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ILO mean ideal lacking overboard\n",
    "# 1000명의 평균 값을 변경하려면 \"1000명의 평균값으로 int \"을 적절한 숫자(int)로 바꿔야함!!\n",
    "# re_setting_ILO = lackigIdealOverboard(\"1000명의 평균값으로 int \", \"입력한 계산 결과값 int\")\n",
    "# 0:lacking, 1:ideal, 2:overbaord\n",
    "re_setting_ILO = lackigIdealOverboard(4, result_setting[0])\n",
    "re_setting_ILO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(re_setting_ILO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
