{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kimkwangil/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import multiprocessing\n",
    "import os\n",
    "from pathlib import Path\n",
    "import io\n",
    "from gensim.models import Phrases\n",
    "from textblob import TextBlob\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_text= \"\"\"A window into the soul.For most people, this would be the eyes. The eyes cannot lie; they often tell more about a person's emotions than their words. What distinguishes a fake smile from a genuine one? The eyes. What shows sadness? The eyes. What gives away a liar? The eyes.But are the eyes the only window into the soul?Recently, I began painting with watercolors. With watercolors, there is no turning back: if one section is too dark, it is nearly impossible to lighten the area again. Every stroke must be done purposefully, every color mixed to its exact value.I laid my materials before me, preparing myself for the worst. I checked my list of supplies, making sure my setup was perfect.I wet my brush, dipped it into some yellow ochre, and dabbed off the excess paint. Too little water on my brush. I dipped my brush back into my trusty water jar; the colors swirled beautifully, forming an abstract art piece before my eyes. \\u2014It's a shame that I couldn't appreciate it.I continued mixing colors to their exact value. More alizarin crimson. More water. More yellow ochre. Less water. More phthalo blue. The cycle continued. Eventually, I was satisfied. The colors looked good, there was enough contrast between facial features, and the watercolors stayed inside the lines.Craving feedback, I posted my art to Snapchat. I got a few messages such as 'wow' and 'pretty,' but one message stood out. 'You were anxious with this one, huh? Anyways, love the hair!'I was caught off guard. Was it a lucky guess? Did they know something I didn't? I immediately responded: 'Haha, how could you tell?' No response.What I didn't know at the time was that my response would come a few months later while babysitting. Since the girl I was babysitting loved art, I took out some Crayola watercolors and some watercolor paper for her to play with. After I went to the bathroom and came back, the watercolors were doused with water. 'You were impatient with this one, huh? Anyways, love the little dog you drew!'The little girl looked up at me, confused. 'How could you tell?' 'You used a lot of water for a brighter color, but you couldn't wait for it to slowly soak in.''Oh.'Now, I would be lying if I said I realized the connection between the two events immediately.Instead, I made the connection when I decided to sit down one day and objectively critique my art. The piece that I once loved now seemed like a nervous wreck: the paper was overworked, the brushstrokes were undecided, the facial features blended together, and each drop of water was bound inside the lines as if it was a prisoner in a cage.From then on, I started noticing pieces of personality in additional creations surrounding me: website designs, solutions to math problems, code written for class, and even the preparation of a meal.When I peer around at people's projects during Code Club, I notice the clear differences between their code. Some people break it up by commenting in every possible section. Others breeze through the project, not caring to comment or organize their code. I could also see clear differences in personalities when our club members began coding the Arduino for the first time. Some followed the tutorials to the letter, while others immediately started experimenting with different colored LEDs and ways of wiring the circuit.It became clear to me that, as humans, we leave pieces of our souls in everything we do, more than we intend to. If we entertain this thought, perhaps the key to better understanding others around us is simply noticing the subtler clues under our noses?Perhaps there are endless windows to the soul, and we simply need to peer through them. I shakily rose my hand. 'We should create workshops of our own,' I suggested.I got a few strange looks. 'It's a good idea, but it's too much work.' 'We just don't have enough free time to make it work.' 'Maybe we could, but I don't know how to make workshops.' My suggestion was shot down. I shuffled in my seat. 'I could make them.' A few people stared at me in disbelief. I glanced over at the club advisor, Mr. C, nervous to hear his response.'If you're willing to take on the work, we can try it.' Mr. C replied. And so I embarked on my quest. I researched different workshops on the internet, learning the information myself at first. Then, I transitioned into creating workshops of my own, making sure that the information was easy to understand for even a beginner. I was exhausted; my first workshop took 16 cumulative hours to create.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def award_analysis(text):\n",
    "\n",
    "    essay_input_corpus = str(text) #문장입력\n",
    "    essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "    sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "    total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "    total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "    split_sentences = []\n",
    "    for sentence in sentences:\n",
    "        processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "        words = processed.split()\n",
    "        split_sentences.append(words)\n",
    "\n",
    "    skip_gram = 1\n",
    "    workers = multiprocessing.cpu_count()\n",
    "    bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "    model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "    model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "    \n",
    "    #모델 설계 완료\n",
    "\n",
    "    #단어들을 리스트에 넣어서 필터로 만들고\n",
    "    one_hundred_list = ['First', 'First Place', 'First Prize', 'Top', '1st place',\n",
    "                        '1st prize', 'Champion', 'Grand', 'Grand winner', 'Grand Prize'\n",
    "                        'Grand champion', 'Gold', 'Gold Medal', 'Platinum']\n",
    "    \n",
    "    ninety_list = ['Second', 'Second Place', 'Second Prize', 'Finalist'\n",
    "                '2nd Place', '2nd Prize', 'Silver', 'Silver Medal']\n",
    "      \n",
    "    eighty_list = ['Third', 'Third Place', 'Third Prize', 'Semi-finalist',\n",
    "                    'Semi finalist', 'Semi final', 'Bronze', '3rd Place', '3rd Prize']\n",
    "    \n",
    "    sixty_list = ['Honorable mention', 'Commendation of honor', 'Honourable mention']\n",
    "   \n",
    "    \n",
    "    #setting_words_filter_list = location_list + time_list + movement_list + palce_terrain_type_list + water_list + outdoor_places_list + underground_list + underground_list + living_places_list + building_facilities_list + architecture_list\n",
    "\n",
    "    \n",
    "    ####문장에 setting_words_filter_list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "    #우선 토큰화한다.\n",
    "    retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "    token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "    #print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "    #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "    filtered_one_hundred_text = []\n",
    "    for k in token_input_text:\n",
    "        for j in one_hundred_list:\n",
    "            if k == j:\n",
    "                filtered_setting_text.append(j)\n",
    "    \n",
    "    print (filtered_one_hundred_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "    \n",
    "    \n",
    "    filtered_ninety_text = []\n",
    "    for k in token_input_text:\n",
    "        for j in ninety_list:\n",
    "            if k == j:\n",
    "                filtered_ninety_text.append(j)\n",
    "    \n",
    "    print (filtered_ninety_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "    \n",
    "    \n",
    "    filtered_eighty_text = []\n",
    "    for k in token_input_text:\n",
    "        for j in eighty_list:\n",
    "            if k == j:\n",
    "                filtered_eighty_text.append(j)\n",
    "    \n",
    "    print (filtered_eighty_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "    \n",
    "    \n",
    "    filtered_sixty_text = []\n",
    "    for k in token_input_text:\n",
    "        for j in sixty_list:\n",
    "            if k == j:\n",
    "                filtered_sixty_text.append(j)\n",
    "    \n",
    "    print (filtered_sixty_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "    \n",
    "    \n",
    "    filtered_setting_text_ = set(filtered_one_hundred_text) #중복제거\n",
    "    filtered_setting_text__ = list(filtered_setting_text_) #다시 리스트로 변환\n",
    "    print (filtered_setting_text__) # 중복값 제거 확인\n",
    "    \n",
    "    for i in filtered_setting_text__:\n",
    "        ext_setting_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "    \n",
    "    setting_total_count = len(filtered_ninety_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 setting 표현 수\n",
    "    setting_count_ = len(filtered_setting_text__) #중복제거된 setting표현 총 수\n",
    "        \n",
    "    result_setting_words_ratio = round(setting_total_count/total_words * 100, 2)\n",
    "    return result_setting_words_ratio\n",
    "    #return result_setting_words_ratio, total_sentences, total_words, setting_total_count, setting_count_, ext_setting_sim_words_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setting ratio</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total sentences</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toral words</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>setting_total_count</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ext_setting_words_no</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ext_setting_sim_words_samples</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Value\n",
       "setting ratio                    0.0\n",
       "total sentences                  0.0\n",
       "toral words                      0.0\n",
       "setting_total_count              0.0\n",
       "ext_setting_words_no             0.0\n",
       "ext_setting_sim_words_samples    0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### 이하 코드는 개별코드 테스트를 위해서 작성한 것임. 서버에 적용할때는 주석처리\n",
    "#award_analysis(input_text)\n",
    "\n",
    "df = award_analysis(input_text)\n",
    "df_ = pd.DataFrame(df, index = [\n",
    "                                'setting ratio','total sentences', 'toral words', 'setting_total_count',\n",
    "                                'ext_setting_words_no', 'ext_setting_sim_words_samples'\n",
    "                                ], columns = ['Value']\n",
    "                  )\n",
    "                     \n",
    "df_ # 이 겨래과에서는 92라는 숫자만 의미가 있지 >>> 총 문장에 setting를 표현한 수가 총 몇개인지 추출한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
