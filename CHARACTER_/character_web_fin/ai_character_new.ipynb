{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kimkwangil/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import multiprocessing\n",
    "import os\n",
    "from pathlib import Path\n",
    "import io\n",
    "from gensim.models import Phrases\n",
    "from textblob import TextBlob\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from pandas import DataFrame\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#캐릭터 수 카운트하기\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def character_all_section(text):\n",
    "\n",
    "    # Number of Characters\n",
    "    def NumberofCharacters(text):\n",
    "\n",
    "        essay_input_corpus = str(text) #문장입력\n",
    "        essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "        sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "        total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "        total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "        split_sentences = []\n",
    "        for sentence in sentences:\n",
    "            processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "            words = processed.split()\n",
    "            split_sentences.append(words)\n",
    "\n",
    "        skip_gram = 1\n",
    "        workers = multiprocessing.cpu_count()\n",
    "        bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "        model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "        model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "        \n",
    "        #모델 설계 완료\n",
    "\n",
    "        #캐릭터 표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "        character_list = ['i', 'my', 'me', 'mine', 'you', 'your', 'they','them',\n",
    "                        'yours', 'he','him','his' 'she','her','it','someone','their', 'myself', 'aunt',\n",
    "                        'brother','cousin','daughter','father','grandchild','granddaughter','granddson','grandfather',\n",
    "                        'grandmother','great-grandchild','husband','ex-husband','son-in-law', 'daughter-in-law','mother',\n",
    "                        'niece','nephew','parents','sister','son','stepfather','stepmother','stepdaughter', 'stepson',\n",
    "                        'twin','uncle','widow','widower','wife','ex-wife','aunt',\n",
    "                        'baby', 'beget', 'brother', 'buddy', 'conserve', 'counterpart', 'cousin', 'daughter', 'duplicate', 'ex',\n",
    "                        'father', 'forefather', 'founder', 'gemini', 'grandchild', 'granddaughter', 'grandfather', 'grandma', 'he', 'helium',\n",
    "                        'husband', 'i', 'in', 'iodine', 'law', 'maine', 'match', 'mine', 'mother', 'nephew', 'niece', 'one', 'parent', 'person',\n",
    "                        'rear', 'sister', 'son', 'stepdaughter', 'stepfather', 'stepmother', 'stepson', 'twin', 'uncle', 'widow', 'widower', 'wife']\n",
    "        \n",
    "        ####문장에 char_list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "        #우선 토큰화한다.\n",
    "        retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "        token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "        ##print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "        #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "        filtered_chr_text = []\n",
    "        for k in token_input_text:\n",
    "            for j in character_list:\n",
    "                if k == j:\n",
    "                    filtered_chr_text.append(j)\n",
    "        \n",
    "        ##print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "        \n",
    "        filtered_chr_text_ = set(filtered_chr_text) #중복제거\n",
    "        filtered_chr_text__ = list(filtered_chr_text_) #다시 리스트로 변환\n",
    "        #print (filtered_chr_text__) # 중복값 제거 확인\n",
    "        \n",
    "        # for i in filtered_chr_text__:\n",
    "        #     ext_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "        \n",
    "        char_total_count = len(filtered_chr_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 캐릭터 표현 수\n",
    "        char_count_ = len(filtered_chr_text__) #중복제거된 캐릭터 표현 총 수\n",
    "        \n",
    "        print(\"중복제거된 캐릭터 표현 수:\", char_count_)\n",
    "        \n",
    "        #전체 단어중에서 캐릭터 관련 단어가 얼마나 포함되었는지 비율 계산, 그 비율에 맞게 결과값 조정\n",
    "        result_char_ratio = round(char_total_count/total_words * 100, 2)\n",
    "        print(\"전체 단어중에서 캐릭터 관련 단어가 얼마나 포함되었는지 비율:\", result_char_ratio)\n",
    "        # 전체 단어중에서 캐릭터 관련 단어가 얼마나 포함되었는지 비율: 12.36\n",
    "        \n",
    "        ###################################\n",
    "        ########### 추가한 코드 ##############\n",
    "        ###################################\n",
    "        #이름을 인식하여 캐릭터 수 카운트하기\n",
    "        doc = nlp(text)\n",
    "        ext_label_char =[(X.text, X.label_) for X in doc.ents] #레이블 추출\n",
    "        ext_label_char = set(ext_label_char) #같은 이름 중복 제거\n",
    "        print(\"레이블 추출:\", ext_label_char)\n",
    "        \n",
    "        cont = 0 #캐릭터 수 초기화\n",
    "        for i in ext_label_char:\n",
    "            'PERSON' in i # PERSON이 튜플 안에 있다면 캐릭터니까 카운트를 한다. \n",
    "            cont += 1\n",
    "            \n",
    "        print(\"에세이에 포함된 이름 수:\", cont)\n",
    "        #총 캐릭터수 계산(character_list + cont) 합치면\n",
    "        char_count_ = char_count_ + cont\n",
    "        char_count_ = round(char_count_ / total_words * 100, 2) #전체문장길이 캐릭터 비율 적용\n",
    "        \n",
    "        print(\"1명 에세이 총 캐릭터수:\", char_count_) \n",
    "        return char_count_\n",
    "\n",
    "\n",
    "\n",
    "    # number_of_characters = NumberofCharacters(input_text) # 문장에서 키워드와 관련된 단어을 모두 추출하면 이런 결과가 나옴, 이 결과를 모두 합쳐서 캐릭터 총 값 계산해서 숫자로 출력\n",
    "    # number_of_characters\n",
    "    # #print ('=============================================')\n",
    "    # #print ('Number of Characters :', number_of_characters)\n",
    "    # #print ('=============================================')\n",
    "\n",
    "    ####################################\n",
    "    #### Character Descriptiveness #####\n",
    "    ####################################\n",
    "\n",
    "    def character_descrip(text):\n",
    "\n",
    "        input_sentence = text\n",
    "\n",
    "        def findSentence(input_sentence):\n",
    "            result = []\n",
    "\n",
    "            data = str(input_sentence)\n",
    "            #data = input_sentence.splitlines()\n",
    "            \n",
    "            findText = ['i', 'my', 'me', 'mine', 'you', 'your', 'they','them',\n",
    "                        'yours', 'he','him','his' 'she','her','it','someone','their', 'myself', 'aunt',\n",
    "                        'brother','cousin','daughter','father','grandchild','granddaughter','granddson','grandfather',\n",
    "                        'grandmother','great-grandchild','husband','ex-husband','son-in-law', 'daughter-in-law','mother',\n",
    "                        'niece','nephew','parents','sister','son','stepfather','stepmother','stepdaughter', 'stepson',\n",
    "                        'twin','uncle','widow','widower','wife','ex-wife','aunt',\n",
    "                        'baby', 'beget', 'brother', 'buddy', 'conserve', 'counterpart', 'cousin', 'daughter', 'duplicate', 'ex',\n",
    "                        'father', 'forefather', 'founder', 'gemini', 'grandchild', 'granddaughter', 'grandfather', 'grandma', 'he', 'helium',\n",
    "                        'husband', 'i', 'in', 'iodine', 'law', 'maine', 'match', 'mine', 'mother', 'nephew', 'niece', 'one', 'parent', 'person',\n",
    "                        'rear', 'sister', 'son', 'stepdaughter', 'stepfather', 'stepmother', 'stepson', 'twin', 'uncle', 'widow', 'widower', 'wife']\n",
    "\n",
    "\n",
    "            sentences = data.split(\".\")\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                for item in findText:\n",
    "                    if item in sentence:\n",
    "                        result.append(sentence)\n",
    "\n",
    "            return result\n",
    "\n",
    "        input_sent_included_character = findSentence(text) \n",
    "        input_sent_chr = set(input_sent_included_character) #중복값을 제거해보자\n",
    "        input_sent_chr = '.'.join(input_sent_chr) #하나의 문자열로 합쳐야 원본 문장처럼 변환되고, 이것을 show/tell 분석코드에 넣게됨\n",
    "\n",
    "\n",
    "\n",
    "        #입력된 전체 문장을 개별문장으로 분리하여 전처리 처리함\n",
    "        def sentence_to_df(input_sentence):\n",
    "\n",
    "            input_text_df = nltk.tokenize.sent_tokenize(input_sentence)\n",
    "            test = []\n",
    "\n",
    "            for i in range(0,len(input_text_df)):\n",
    "                new_label = np.random.randint(0,2)  # 개별문장(input_text_df) 수만큼 0 또는 1 난수 생성\n",
    "                data = [new_label, input_text_df[i]]\n",
    "                test.append(data)\n",
    "\n",
    "            ##print(test)\n",
    "            dataf = pd.DataFrame(test, columns=['label', 'text'])\n",
    "            ##print(dataf)\n",
    "            return dataf\n",
    "\n",
    "\n",
    "        class STDataset(Dataset):\n",
    "            ''' Showing Telling Corpus Dataset '''\n",
    "            def __init__(self, df):\n",
    "                self.df = df\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.df)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                text = self.df.iloc[idx, 1]\n",
    "                label = int(self.df.iloc[idx, 0])\n",
    "                return text, label\n",
    "\n",
    "\n",
    "        ###########입력받은 데이터 처리 실행하는 메소드 showtell_classfy() ###############\n",
    "        #result_all.html에서 입력받을 text를 contents에 넣고 전처리 후 데이터프레임에 넣어줌\n",
    "        def showtell_classfy(text):\n",
    "            contents = str(text)\n",
    "            preprossed_contents_df = sentence_to_df(contents)\n",
    "\n",
    "            preprossed_contents_df.dropna(inplace=True)\n",
    "            #전처리된 데이터를 확인(데이터프레임으로 가공됨)\n",
    "            preprossed_contents_df__ = preprossed_contents_df.sample(frac=1, random_state=999)\n",
    "            \n",
    "\n",
    "            #파이토치에 입력하기 위해서 로딩...\n",
    "            ST_test_dataset = STDataset(preprossed_contents_df__)\n",
    "            test_loader = DataLoader(ST_test_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "            #로딩되는지 확인\n",
    "            ST_test_dataset.__getitem__(1)\n",
    "\n",
    "            #time.sleep(1)\n",
    "\n",
    "\n",
    "\n",
    "            #check whether cuda is available\n",
    "            #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "            device = torch.device(\"cpu\")  \n",
    "            #device = torch.device(\"cuda\")\n",
    "            #tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "            model = BertForSequenceClassification.from_pretrained('bert-large-cased')\n",
    "            model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "            # for text, label in test_loader :\n",
    "            #     #print(\"text:\",text)\n",
    "            #     #print(\"label:\",label)\n",
    "\n",
    "\n",
    "            #저장된 모델을 불러온다.\n",
    "            #J:\\Django\\EssayFit_Django\\essayfitaiproject\\essayfitapp\\model.pt\n",
    "            #time.sleep(1)\n",
    "            #model = torch.load(\"model.pt\", map_location=torch.device('cpu'))\n",
    "            model = torch.load(\"/Users/kimkwangil/Documents/001_ESSAYFITAI/EssayFitAI_2021-01-08/ai_server_backup/essayfit/essayai/data/model.pt\", map_location=torch.device('cpu'))\n",
    "            # #print(\"model loadling~\")\n",
    "            model.eval()\n",
    "\n",
    "\n",
    "            pred_loader = test_loader\n",
    "            # #print(\"pred_loader:\", pred_loader)\n",
    "            total_loss = 0\n",
    "            total_len = 0\n",
    "            total_showing__ = 0\n",
    "            total_telling__ = 0\n",
    "\n",
    "            showing_conunter = [] #문장에 해당하는 SHOWING을 계산한다.\n",
    "            \n",
    "            # #print(\"check!\")\n",
    "            for text, label in pred_loader:\n",
    "                # #print(\"text:\",text)\n",
    "                ##print(\"label:\",label)\n",
    "                encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text] #text to tokenize\n",
    "                padded_list =  [e + [0] * (512-len(e)) for e in encoded_list] #padding\n",
    "                sample = torch.tensor(padded_list) #torch tensor로 변환\n",
    "                sample, label = sample.to(device), label.to(device) #tokenized text에 label을 넣어서 Device(gpu/cpu)에 넣기 위해 준비\n",
    "                labels = torch.tensor(label) #레이블을 텐서로 변환\n",
    "                #time.sleep(1)\n",
    "                outputs = model(sample,labels=labels) #모델을 통해서 샘플텍스트와 레이블 입력데이터를 출력 output에 넣음\n",
    "                #시간 딜레이를 주자\n",
    "                #time.sleep(1)\n",
    "                _, logits = outputs #outputs를 로짓에 넣음 이것을 softmax에 넣으면 0~1 사이로 결과가 출력됨\n",
    "                \n",
    "                pred = torch.argmax(F.softmax(logits), dim=1) #드디어 예측한다. argmax는 리스트(계산된 값)에서 가장 큰 값을 추출하여 pred에 넣는다. 0 ~1 사이의 값이 나올거임\n",
    "                # #print('pred :', pred)\n",
    "                # correct = pred.eq(labels) \n",
    "                showing__ = pred.eq(1) # 예측한 결과가 1과 같으면 showing이다   >> TRUE   SHOWING을 추출하려면 이것만 카운드하면 된다. \n",
    "                telling__ = pred.eq(0) # 예측한 결과가 0과 같으면 telling이다   >> FALSE\n",
    "                \n",
    "                ##print('showing : ', showing__)\n",
    "                ##print('telling : ', telling__)\n",
    "                \n",
    "                \n",
    "                showing_conunter.append(text)        \n",
    "                #pred_ = round(float(pred))\n",
    "                showing_conunter.append(pred)\n",
    "\n",
    "\n",
    "            return showing_conunter \n",
    "\n",
    "\n",
    "        st_re = showtell_classfy(str(input_sent_chr)) # 캐릭터거 포함된 문장(전처리 완료된) 입력\n",
    "\n",
    "        df = DataFrame(st_re)\n",
    "        df_ = df[0::2] # 글만 추출\n",
    "        df_label = df[1::2] # 레이블만 추출\n",
    "\n",
    "        df_.reset_index(drop=True, inplace=True) #데이터를 합치기 위해서 초기화\n",
    "        df_label.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        df_result = pd.concat([df_,df_label],axis=1) #합치기\n",
    "\n",
    "        df_result.columns = ['sentence','show/tell']\n",
    "\n",
    "        df_fin = df_result['show/tell'].value_counts(normalize=True)\n",
    "        list(df_fin)\n",
    "        showing_sentence_with_char = max(round(df_fin*100))\n",
    "\n",
    "        # #print(\"===============================================================\")\n",
    "        # #print ('Character Descriptiveness : ', showing_sentence_with_char)\n",
    "        # #print(\"===============================================================\")\n",
    "\n",
    "        return showing_sentence_with_char\n",
    "\n",
    "\n",
    "\n",
    "    ################################################\n",
    "    #############  Emphasis on YOU  ################\n",
    "    ################################################\n",
    "    def EmphasisOnYou(text):\n",
    "\n",
    "        essay_input_corpus = str(text) #문장입력\n",
    "        essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "        sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "        total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "        total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "        split_sentences = []\n",
    "        for sentence in sentences:\n",
    "            processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "            words = processed.split()\n",
    "            split_sentences.append(words)\n",
    "\n",
    "        skip_gram = 1\n",
    "        workers = multiprocessing.cpu_count()\n",
    "        bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "        model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "        model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "        \n",
    "        #모델 설계 완료\n",
    "\n",
    "        #캐릭터 표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "        character_list = ['i', 'I', 'my', 'me', 'mine', 'one']\n",
    "        \n",
    "        ####문장에 char_list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "        #우선 토큰화한다.\n",
    "        retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "        token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "        ##print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "        #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "        filtered_chr_text = []\n",
    "        for k in token_input_text:\n",
    "            for j in character_list:\n",
    "                if k == j:\n",
    "                    filtered_chr_text.append(j)\n",
    "        \n",
    "        ##print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "        \n",
    "        #filtered_chr_text_ = set(filtered_chr_text) #중복제거\n",
    "        #filtered_chr_text__ = list(filtered_chr_text_) #다시 리스트로 변환\n",
    "        # #print (filtered_chr_text__) # 중복값 제거 확인\n",
    "        \n",
    "        # for i in filtered_chr_text__:\n",
    "        #     ext_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "        \n",
    "        char_total_count = len(filtered_chr_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 캐릭터 표현 수\n",
    "\n",
    "        # char_count_ = len(filtered_chr_text__) #중복제거된 캐릭터 표현 총 수\n",
    "\n",
    "        result_char_ratio = round(char_total_count/total_words * 100, 2)\n",
    "        print(\"전체 문장에서 캐릭터 단어 비율::::::\", result_char_ratio)\n",
    "\n",
    "        print(\"emphasis on you\", char_total_count)\n",
    "        \n",
    "        \n",
    "        char_total_count = round(char_total_count / total_words * 100, 2) #전체문장길이 캐릭터 비율 적용\n",
    "            \n",
    "            \n",
    "        return char_total_count\n",
    "\n",
    "\n",
    "    # EmphasisOnYou_ = EmphasisOnYou(input_text) # 문장에서 키워드와 관련된 단어을 모두 추출하면 이런 결과가 나옴, 이 결과를 모두 합쳐서 캐릭터 총 값 계산해서 숫자로 출력\n",
    "    # EmphasisOnYou_\n",
    "    # #print ('=============================================')\n",
    "    # #print ('Emphasis on You :', EmphasisOnYou_)\n",
    "    # #print ('=============================================')\n",
    "\n",
    "\n",
    "\n",
    "    #########################################\n",
    "    ######### Emphasis on others  ###########\n",
    "    #########################################\n",
    "    def EmphasisOnOthers(text):\n",
    "\n",
    "        essay_input_corpus = str(text) #문장입력\n",
    "        essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "        sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "        total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "        total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "        split_sentences = []\n",
    "        for sentence in sentences:\n",
    "            processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "            words = processed.split()\n",
    "            split_sentences.append(words)\n",
    "\n",
    "        skip_gram = 1\n",
    "        workers = multiprocessing.cpu_count()\n",
    "        bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "        model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "        model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "        \n",
    "        #모델 설계 완료\n",
    "\n",
    "        #캐릭터 표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "        character_list = ['they','them','he','him','his' 'she','her','someone','their', 'myself', 'aunt',\n",
    "                        'brother','cousin','daughter','father','grandchild','granddaughter','granddson','grandfather',\n",
    "                        'grandmother','great-grandchild','husband','ex-husband','son-in-law', 'daughter-in-law','mother',\n",
    "                        'niece','nephew','parents','sister','son','stepfather','stepmother','stepdaughter', 'stepson',\n",
    "                        'twin','uncle','widow','widower','wife','ex-wife','aunt',\n",
    "                        'baby', 'beget', 'brother', 'buddy', 'conserve', 'counterpart', 'cousin',\n",
    "                        'daughter', 'duplicate', 'ex', 'father', 'forefather', 'founder', 'gemini',\n",
    "                        'grandchild', 'granddaughter', 'grandfather', 'grandma', 'he', 'helium', 'husband',\n",
    "                        'match', 'mother', 'nephew', 'niece', 'parent', 'person', 'rear',\n",
    "                        'sister', 'son', 'stepdaughter', 'stepfather', 'stepmother', 'stepson', 'twin', 'uncle', 'widow',\n",
    "                        'widower', 'wife']\n",
    "        \n",
    "        ####문장에 char_list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "        #우선 토큰화한다.\n",
    "        retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "        token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "        ##print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "        #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "        filtered_chr_text = []\n",
    "        for k in token_input_text:\n",
    "            for j in character_list:\n",
    "                if k == j:\n",
    "                    filtered_chr_text.append(j)\n",
    "        \n",
    "        ##print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "        \n",
    "        # filtered_chr_text_ = set(filtered_chr_text) #중복제거\n",
    "        # filtered_chr_text__ = list(filtered_chr_text_) #다시 리스트로 변환\n",
    "        #print (filtered_chr_text__) # 중복값 제거 확인\n",
    "        \n",
    "        # for i in filtered_chr_text__:\n",
    "        #     ext_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "        print(\"Emphasis on others 수집 단어들:\",filtered_chr_text)\n",
    "        char_total_count = len(filtered_chr_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 캐릭터 표현 수\n",
    "        print(\"Emphasis on others:\", char_total_count)\n",
    "    \n",
    "        char_total_count = round(char_total_count / total_words * 100, 2) #전체문장길이 캐릭터 비율 적용\n",
    "    \n",
    "        return char_total_count\n",
    "\n",
    "\n",
    "    # EmphasisOnOthers_ = EmphasisOnOthers(input_text) # 문장에서 키워드와 관련된 단어을 모두 추출하면 이런 결과가 나옴, 이 결과를 모두 합쳐서 캐릭터 총 값 계산해서 숫자로 출력\n",
    "    # EmphasisOnOthers_\n",
    "    # #print ('=============================================')\n",
    "    # #print ('Emphasis on Others :', EmphasisOnOthers_)\n",
    "    # #print ('=============================================')\n",
    "\n",
    "\n",
    "    character_descriptiveness = character_descrip(text)\n",
    "    # #print(\"===============================================================\")\n",
    "    # #print ('Character Descriptiveness : ' , character_descriptiveness)\n",
    "    # #print(\"===============================================================\")\n",
    "\n",
    "\n",
    "    number_of_characters = NumberofCharacters(text) \n",
    "    # #print ('=============================================')\n",
    "    # #print ('Number of Characters :' , number_of_characters)\n",
    "    # #print ('=============================================')\n",
    "\n",
    "\n",
    "    EmphasisOnYou_ = EmphasisOnYou(text)\n",
    "    # #print ('=============================================')\n",
    "    # #print ('Emphasis on You :' , EmphasisOnYou_)\n",
    "    # #print ('=============================================')\n",
    "\n",
    "\n",
    "    EmphasisOnOthers_ = EmphasisOnOthers(text) \n",
    "    # #print ('=============================================')\n",
    "    # #print ('Emphasis on Others :' , EmphasisOnOthers_)\n",
    "    # #print ('=============================================')\n",
    "\n",
    "\n",
    "    return character_descriptiveness, number_of_characters, EmphasisOnYou_, EmphasisOnOthers_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 학생 #1\n",
    "# -성적/활동 높음\n",
    "# -캐릭터 수 엄청 많음 (15명) >>>>>>>>> result_number_of_chracters': 0 >>>>> 2로 나와야 정상\n",
    "# -Setting 갯수 엄청 많음 / Setting 설명 엄청 많음\n",
    "# -Conflict 엄청 많음 (Body에 집중)\n",
    "# -Emotion (negative 한게 엄청 많음... 약간의 positive)\n",
    "# -Essay Prompt #2 (Obstacle) - Theme 맞는지 (각 prompt 별로 emotion / keyword 조건 확인)\n",
    "\n",
    "\n",
    "text= \"\"\"Jennifer walked into the dark and creepy bedroom, filled with old furniture and dust. She felt uncomfortable because she knew that this old house belonged to John. John was the cousin of Tom, who used to abuse her very cruelly.The wooden house was built with cherry wood. It was previously owned by criminals, named Joe and Cyndi. They were grandparents of John. The basement was filled with screams and sobbing, as the angry captives, such as Roy, Eugene, and Peter, who were miserably tortured in the wet, cold floor of the isolated cell. They felt so threatened as the tension heightened. The pressure was high as they violently hit each other and bled in the faces. The cement floor of the bathroom was filled with a red pungent smell of blood and vivid marks of ripped flesh splashed everywhere. They ran like turkey without heads but began to brutally crash into the steel walls and gates, collapsing into the pit. There were fierce yelling and fighting but Bill and Ted didn't seem to care.\n",
    "Finally, the room quieted down as people calmed down too. It was quiet and peaceful. Everyone seemed to be sleeping comfortably, with a smile on their faces. I am happy now because I am no longer angry.\"\"\"\n",
    "\n",
    "\n",
    "# 중복제거된 캐릭터 표현 수: 7\n",
    "# 전체 단어중에서 캐릭터 관련 단어가 얼마나 포함되었는지 비율: 6.69\n",
    "# 레이블 추출: {('Bill', 'PERSON'), ('John', 'PERSON'), ('Tom', 'PERSON'), ('Eugene', 'WORK_OF_ART'), ('Peter', 'PERSON'), ('Jennifer', 'PERSON'), ('turkey', 'GPE'), ('Ted', 'PERSON'), ('Roy', 'PERSON'), ('Joe', 'PERSON'), ('Cyndi', 'GPE')}\n",
    "# 에세이에 포함된 이름 수: 11\n",
    "# 1명 에세이 총 캐릭터수: 7.53\n",
    "# 전체 문장에서 캐릭터 단어 비율:::::: 0.84\n",
    "# emphasis on you 2\n",
    "# Emphasis on others 수집 단어들: ['cousin', 'cousin', 'her', 'they', 'they', 'they', 'they', 'their']\n",
    "# Emphasis on others: 8\n",
    "# 한개의 에세이 캐릭터 수: 7.53\n",
    "# 1000명 평균 캐릭터수_입력문장길이비율적용: 7\n",
    "# 캐릭터 분석 결과 (1, 5)\n",
    "# 캐랙터 상대적 점수 1\n",
    "# 최종결과 :  {'number_of_chracters': 7.53, 'character_description': 36.0, 'emaphasis_on_you': 0.84, \n",
    "#          'emaphasis_on_others': 3.35, 'result_number_of_chracters': 1, 'result_character_description': 1,\n",
    "#          'result_emaphasis_on_you': 0, 'result_emaphasis_on_others': 0, 'avg_character': 2.5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) 학생 #2\n",
    "# -성적/활동 낮음\n",
    "# -캐릭더 수 없음 (나만, I)   -------> 'result_number_of_chracters': 0 으로 나와야 함\n",
    "# -Setting 갯수 별로 없음 / Setting 설명 없음\n",
    "# -Conflict 거의 없음 (neutral)\n",
    "# -Emotion (거의 realization 쪽으로... intellectual/academic 하게)\n",
    "# -Essay Prompt #3 (Intellectual) - Theme 맞는지  (각 prompt 별로 emotion / keyword 조건 확인)\n",
    "\n",
    "# text = \"\"\"I always studied physics because it's science. When objects meet each other, there starts some type of force. And forces make things to move, hence resulting in perpetual movements using gravity.\n",
    "# Gravity is a unique concept because I think it applies to everything around us. Objects are definitely affected by gravity and I understood this from a long time ago. This is why physics deals with gravity and objects. We can now see that physics is a study of relationships, more physical than philosophical, because it is proven to us through research. Therefore, the lesson in physics is something I discovered and I realize that it is important for engineering as well.\n",
    "# In terms of engineering, physics is used as the foundation. Furthermore, the foundation for physics is math. Therefore, math is very relevant with engineering. This is a lesson I found to be true and I will remember this forever.\n",
    "# \"\"\"\n",
    "\n",
    "# 중복제거된 캐릭터 표현 수: 3\n",
    "# 전체 단어중에서 캐릭터 관련 단어가 얼마나 포함되었는지 비율: 12.21\n",
    "# 레이블 추출: set()\n",
    "# 에세이에 포함된 이름 수: 0\n",
    "# 1명 에세이 총 캐릭터수: 1.74\n",
    "# 전체 문장에서 캐릭터 단어 비율:::::: 4.07\n",
    "# emphasis on you 7\n",
    "# Emphasis on others 수집 단어들: []\n",
    "# Emphasis on others: 0\n",
    "# 한개의 에세이 캐릭터 수: 1.74\n",
    "# 1000명 평균 캐릭터수_입력문장길이비율적용: 7\n",
    "# 캐릭터 분석 결과 (0, 1)\n",
    "# 캐랙터 상대적 점수 0\n",
    "# 최종결과 :  {'number_of_chracters': 1.74, 'character_description': 45.0, 'emaphasis_on_you': 4.07, \n",
    "#          'emaphasis_on_others': 0.0, 'result_number_of_chracters': 0, 'result_character_description': 0, \n",
    "#          'result_emaphasis_on_you': 1, 'result_emaphasis_on_others': 0, 'avg_character': 2.5}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학생 #3 에세이\n",
    "\n",
    "# -성적/활동 중간\n",
    "# -캐릭터 수 4명 (중간)   >>>>> 결과: result_number_of_chracters': 1  \n",
    "# -Setting 갯수 설명 중간 (3개 장소)\n",
    "# -Conflict 중간\n",
    "# -Emotion (positive & realization 쪽으로)\n",
    "# -Essay Prompt #1 (Identity) - Theme 맞는지  (각 prompt 별로 emotion / keyword 조건 확인)\n",
    "\n",
    "# text = \"\"\"Meeting a friend is always a joy in my life. Yesterday, I left to see Thomas and Sammy. They are my best friends whom I treasure dearly. The paved road was so straight that it was easy and pleasant for me to walk on.\n",
    "# When I saw them at the bright red restaurant, I knew the waiter would serve us ketchup. I love ketchup because it goes perfectly with french fries, which was made in France. Thomas and Sammy brought another friend named Kim. All of us spent a wonderful time, sitting in the hall and listening to music. The large hall at the building has very beautiful windows and we loved it.\n",
    "# As we came back to the steep hill where the blazing sun was setting, we were grateful for the lesson we found. The hill was connected to a mountain which told us that friendship is the pathway to the high grounds. It was a beautiful day with friends.\"\"\"\n",
    "\n",
    "\n",
    "# 중복제거된 캐릭터 표현 수: 7\n",
    "# 전체 단어중에서 캐릭터 관련 단어가 얼마나 포함되었는지 비율: 12.36\n",
    "# 레이블 추출: {('Yesterday', 'DATE'), ('Thomas', 'PERSON'), ('french', 'NORP'), ('Sammy', 'PERSON'), ('a beautiful day', 'DATE'), ('Kim', 'PERSON'), ('France', 'GPE')}\n",
    "# 에세이에 포함된 이름 수: 7\n",
    "# 1명 에세이 총 캐릭터수: 7.87\n",
    "# 전체 문장에서 캐릭터 단어 비율:::::: 4.49\n",
    "# emphasis on you 8\n",
    "# Emphasis on others 수집 단어들: ['they', 'them']\n",
    "# Emphasis on others: 2\n",
    "# 한개의 에세이 캐릭터 수: 7.87\n",
    "# 1000명 평균 캐릭터수_입력문장길이비율적용: 7\n",
    "# 캐릭터 분석 결과 (1, 5)\n",
    "# 캐랙터 상대적 점수 1\n",
    "# 최종결과 :  {'number_of_chracters': 7.87, 'character_description': 64.0, 'emaphasis_on_you': 4.49,\n",
    "#          'emaphasis_on_others': 1.12, 'result_number_of_chracters': 1, 'result_character_description': 1, \n",
    "#          'result_emaphasis_on_you': 1, 'result_emaphasis_on_others': 0, 'avg_character': 4.0}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_total_analysis(text):\n",
    "\n",
    "    ##### 주석 처리 ##########################\n",
    "    ########################################\n",
    "    char_sec_re = character_all_section(text)\n",
    "\n",
    "    # #print(\"1명의 에세이 결과 계산점수 :\", char_sec_re)\n",
    "\n",
    "    # 위에서 계산한 총 4개의 값을 개인, 그룹의 값과 비교하여 lacking, ideal, overboard 계산\n",
    "    \n",
    "    # 개인에세이 값 계산 4가지 결과 추출 >>>>> personal_value 로 입력됨\n",
    "    one_ps_char_desc = char_sec_re[0] # 튜플에서 첫번재 인댁스 값 가져오기 : Character Descriptiveness\n",
    "    one_ps_num_of_char = char_sec_re[1]\n",
    "    print('한개의 에세이 캐릭터 수:',one_ps_num_of_char)\n",
    "    one_ps_emp_on_you = char_sec_re[2]\n",
    "    one_ps_emp_on_others = char_sec_re[3]\n",
    "\n",
    "    ##############################################################################################################################\n",
    "    ## 1000명 데이터의 각 값(char_desc_mean)의 평균 값 전달. >>>> group_mean 으로 입력됨\n",
    "    char_desc_mean = [77, 7, 3, 10] # 현재 계산 완료한 1000명의 평균 값(고정값)  1명 에세이 총 캐릭터수: 173.04\n",
    "    group_db_fin_result = [5.0] #레이다차트의 1000명 평균값 기준설정\n",
    "    ##############################################################################################################################\n",
    "\n",
    "\n",
    "    char_desc_ideal_mean = char_desc_mean[0] #첫번째 값을 가져옴, Character Descriptiveness\n",
    "    num_of_char_ideal_mean = char_desc_mean[1] #Number of Characters\n",
    "    print('1000명 평균 캐릭터수_입력문장길이비율적용:', num_of_char_ideal_mean)\n",
    "    emp_on_you_ideal_mean = char_desc_mean[2] #Emphasis on You \n",
    "    emp_on_others_ideal_mean = char_desc_mean[3] #Emphasis on Others\n",
    "\n",
    "\n",
    "    def lackigIdealOverboard(group_mean, personal_value): # group_mean: 1000명 평균, personal_value|:개인값\n",
    "        ideal_mean = group_mean\n",
    "        one_ps_char_desc = personal_value\n",
    "        #최대, 최소값 기준으로 구간설정. 구간비율 30% => 0.3으로 설정\n",
    "        min_ = int(ideal_mean-ideal_mean*0.5)\n",
    "        # #print('min_', min_)\n",
    "        max_ = int(ideal_mean+ideal_mean*0.5)\n",
    "        # #print('max_: ', max_)\n",
    "        div_ = int(((ideal_mean+ideal_mean*0.5)-(ideal_mean-ideal_mean*0.5))/3)\n",
    "        # #print('div_:', div_)\n",
    "\n",
    "        #결과 판단 Lacking, Ideal, Overboard\n",
    "        cal_abs = abs(ideal_mean - one_ps_char_desc) # 개인 - 단체 값의 절대값계산\n",
    "\n",
    "        # #print('cal_abs 절대값 :', cal_abs)\n",
    "        compare7 = (one_ps_char_desc + ideal_mean)/6\n",
    "        compare6 = (one_ps_char_desc + ideal_mean)/5\n",
    "        compare5 = (one_ps_char_desc + ideal_mean)/4\n",
    "        compare4 = (one_ps_char_desc + ideal_mean)/3\n",
    "        compare3 = (one_ps_char_desc + ideal_mean)/2\n",
    "        # #print('compare7 :', compare7)\n",
    "        # #print('compare6 :', compare6)\n",
    "        # #print('compare5 :', compare5)\n",
    "        # #print('compare4 :', compare4)\n",
    "        # #print('compare3 :', compare3)\n",
    "\n",
    "\n",
    "\n",
    "        if one_ps_char_desc > ideal_mean: # 개인점수가 평균보다 클 경우는 overboard\n",
    "            if cal_abs > compare3: # 37 개인점수가 개인평균차의 절대값보다 클 경우, 즉 차이가 많이 날경우\n",
    "                # #print(\"Overboard: 2\")\n",
    "                result = 2 #overboard\n",
    "                score = 1\n",
    "            elif cal_abs > compare4: # 28\n",
    "                # #print(\"Overvoard: 2\")\n",
    "                result = 2\n",
    "                score = 2\n",
    "            elif cal_abs > compare5: # 22\n",
    "                # #print(\"Overvoard: 2\")\n",
    "                result = 2\n",
    "                score = 3\n",
    "            elif cal_abs > compare6: # 18\n",
    "                # #print(\"Overvoard: 2\")\n",
    "                result = 2\n",
    "                score = 4\n",
    "            else:\n",
    "                # #print(\"Ideal: 1\")\n",
    "                result = 1\n",
    "                score = 5\n",
    "        elif one_ps_char_desc < ideal_mean: # 개인점수가 평균보다 작을 경우 lacking\n",
    "            if cal_abs > compare3: # 37 개인점수가 개인평균차의 절대값보다 클 경우, 즉 차이가 많이 날경우\n",
    "                # #print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                score = 1\n",
    "            elif cal_abs > compare4: # 28\n",
    "                # #print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                score = 2\n",
    "            elif cal_abs > compare5: # 22\n",
    "                # #print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                score = 3\n",
    "            elif cal_abs > compare6: # 18\n",
    "                # #print(\"Lacking: 2\")\n",
    "                result = 0\n",
    "                score = 4\n",
    "            else:\n",
    "                # #print(\"Ideal: 1\")\n",
    "                result = 1\n",
    "                score = 5\n",
    "                \n",
    "        else:\n",
    "            # #print(\"Ideal: 1\")\n",
    "            result = 1\n",
    "            score = 5\n",
    "\n",
    "        return result, score\n",
    "\n",
    "\n",
    "    #종합계산시작 lackigIdealOverboard(group_mean, personal_value)\n",
    "    character_desc_result = lackigIdealOverboard(char_desc_ideal_mean, one_ps_char_desc)\n",
    "    number_of_char_result = lackigIdealOverboard(num_of_char_ideal_mean, one_ps_num_of_char)\n",
    "    emp_on_you_result = lackigIdealOverboard(emp_on_you_ideal_mean, one_ps_emp_on_you)\n",
    "    emp_on_others_result = lackigIdealOverboard(emp_on_others_ideal_mean, one_ps_emp_on_others)\n",
    "\n",
    "    fin_result = [character_desc_result, number_of_char_result, emp_on_you_result, emp_on_others_result]\n",
    "\n",
    "    print('캐릭터 분석 결과', number_of_char_result)\n",
    "    print('캐랙터 상대적 점수', fin_result[1][0])\n",
    "    \n",
    "    # each_fin_result의 결과는 순서대로 [Character Descriptiveness, Number of Characters, Emphasis on You, Emphasis on Others] 이다. 0: lacking, 1:ideal, 2:overbaord \n",
    "    each_fin_result = [fin_result[0][0], fin_result[1][0], fin_result[2][0], fin_result[3][0]]\n",
    "    # 최종 character  전체 점수 계산\n",
    "    overall_character_rating = [(fin_result[0][1]+ fin_result[1][1] + fin_result[2][1]+ fin_result[3][1])/4]\n",
    "\n",
    "    result_final = each_fin_result + overall_character_rating + group_db_fin_result\n",
    "    \n",
    "    data = {\n",
    "        # 이게 순서가 바뀌었음\n",
    "        \"number_of_chracters\":one_ps_num_of_char, \n",
    "        \"character_description\":one_ps_char_desc,\n",
    "        \"emaphasis_on_you\":one_ps_emp_on_you,\n",
    "        \"emaphasis_on_others\":one_ps_emp_on_others,\n",
    "        \n",
    "        \"result_number_of_chracters\": fin_result[1][0],\n",
    "        \"result_character_description\": result_final[1],\n",
    "        \"result_emaphasis_on_you\" : result_final[2],\n",
    "        \"result_emaphasis_on_others\" : result_final[3],\n",
    "        \n",
    "        \"avg_character\": result_final[4]\n",
    "    }\n",
    "    \n",
    "\n",
    "    return data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:243: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/gensim/models/base_any2vec.py:323: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복제거된 캐릭터 표현 수: 7\n",
      "전체 단어중에서 캐릭터 관련 단어가 얼마나 포함되었는지 비율: 6.69\n",
      "레이블 추출: {('Bill', 'PERSON'), ('John', 'PERSON'), ('Tom', 'PERSON'), ('Eugene', 'WORK_OF_ART'), ('Peter', 'PERSON'), ('Jennifer', 'PERSON'), ('turkey', 'GPE'), ('Ted', 'PERSON'), ('Roy', 'PERSON'), ('Joe', 'PERSON'), ('Cyndi', 'GPE')}\n",
      "에세이에 포함된 이름 수: 11\n",
      "1명 에세이 총 캐릭터수: 7.53\n",
      "전체 문장에서 캐릭터 단어 비율:::::: 0.84\n",
      "emphasis on you 2\n",
      "Emphasis on others 수집 단어들: ['cousin', 'cousin', 'her', 'they', 'they', 'they', 'they', 'their']\n",
      "Emphasis on others: 8\n",
      "한개의 에세이 캐릭터 수: 7.53\n",
      "1000명 평균 캐릭터수_입력문장길이비율적용: 7\n",
      "캐릭터 분석 결과 (1, 5)\n",
      "캐랙터 상대적 점수 1\n",
      "최종결과 :  {'number_of_chracters': 7.53, 'character_description': 36.0, 'emaphasis_on_you': 0.84, 'emaphasis_on_others': 3.35, 'result_number_of_chracters': 1, 'result_character_description': 1, 'result_emaphasis_on_you': 0, 'result_emaphasis_on_others': 0, 'avg_character': 2.5}\n"
     ]
    }
   ],
   "source": [
    "print(\"최종결과 : \", character_total_analysis(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
