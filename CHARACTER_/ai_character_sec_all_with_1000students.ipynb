{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import multiprocessing\n",
    "import os\n",
    "from pathlib import Path\n",
    "import io\n",
    "from gensim.models import Phrases\n",
    "from textblob import TextBlob\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from pandas import DataFrame\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORDNET 이용한 유의어 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aunt',\n",
       " 'baby',\n",
       " 'beget',\n",
       " 'brother',\n",
       " 'buddy',\n",
       " 'conserve',\n",
       " 'counterpart',\n",
       " 'cousin',\n",
       " 'daughter',\n",
       " 'duplicate',\n",
       " 'ex',\n",
       " 'gemini',\n",
       " 'grandchild',\n",
       " 'granddaughter',\n",
       " 'grandfather',\n",
       " 'grandma',\n",
       " 'he',\n",
       " 'helium',\n",
       " 'husband',\n",
       " 'in',\n",
       " 'law',\n",
       " 'match',\n",
       " 'mother',\n",
       " 'nephew',\n",
       " 'niece',\n",
       " 'parent',\n",
       " 'person',\n",
       " 'rear',\n",
       " 'sister',\n",
       " 'son',\n",
       " 'stepdaughter',\n",
       " 'stepfather',\n",
       " 'stepmother',\n",
       " 'stepson',\n",
       " 'twin',\n",
       " 'uncle',\n",
       " 'widow',\n",
       " 'widower',\n",
       " 'wife'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "search_keywords_list = ['you', 'your', 'they','them',\n",
    "                        'yours', 'he','him','his' 'she','her','it','someone','their', 'myself', 'aunt',\n",
    "                        'brother','cousin','daughter','grandchild','granddaughter','granddson','grandfather',\n",
    "                        'grandmother','great-grandchild','husband','ex-husband','son-in-law', 'daughter-in-law','mother',\n",
    "                        'niece','nephew','parents','sister','son','stepfather','stepmother','stepdaughter', 'stepson',\n",
    "                        'twin','uncle','widow','widower','wife','ex-wife']\n",
    "\n",
    "sim_words_list = []\n",
    "for conf_word in search_keywords_list:\n",
    "    for synset in wn.synsets(conf_word):\n",
    "        sim_words_list.append(synset.name()) #동의, 유의어\n",
    "        #sim_words_list.append(synset.hypernyms()) # 상위어\n",
    "        #sim_words_list.append(synset.hyponyms()) # 하위어\n",
    "        \n",
    "conf_words_flttend_list = ''.join(str(sim_words_list)) #문자열로 변환\n",
    "result = re.findall(r\"(?i)\\b[a-z]+\\b\",conf_words_flttend_list)\n",
    "result_ =[]\n",
    "for i in result:\n",
    "    if i != 'n' and i != 'v' and i != 's' and i != 'a':\n",
    "        result_.append(i)\n",
    "        \n",
    "search_keywords_list_re = set(result_)\n",
    "search_keywords_list_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_all_section(text):\n",
    "\n",
    "\n",
    "    # Number of Characters\n",
    "    def NumberofCharacters(text):\n",
    "\n",
    "        essay_input_corpus = str(text) #문장입력\n",
    "        essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "        sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "        total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "        total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "        split_sentences = []\n",
    "        for sentence in sentences:\n",
    "            processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "            words = processed.split()\n",
    "            split_sentences.append(words)\n",
    "\n",
    "        skip_gram = 1\n",
    "        workers = multiprocessing.cpu_count()\n",
    "        bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "        model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "        model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "        \n",
    "        #모델 설계 완료\n",
    "\n",
    "        #캐릭터 표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "        character_list = ['i', 'my', 'me', 'mine', 'you', 'your', 'they','them',\n",
    "                        'yours', 'he','him','his' 'she','her','it','someone','their', 'myself', 'aunt',\n",
    "                        'brother','cousin','daughter','grandchild','granddaughter','granddson','grandfather',\n",
    "                        'grandmother','great-grandchild','husband','ex-husband','son-in-law', 'daughter-in-law','mother',\n",
    "                        'niece','nephew','parents','sister','son','stepfather','stepmother','stepdaughter', 'stepson',\n",
    "                        'twin','uncle','widow','widower','wife','ex-wife','aunt',\n",
    "                        'baby', 'beget', 'brother', 'buddy', 'conserve', 'counterpart', 'cousin', 'daughter', 'duplicate', 'ex',\n",
    "                        'forefather', 'founder', 'gemini', 'grandchild', 'granddaughter', 'grandfather', 'grandma', 'he', 'helium',\n",
    "                        'husband', 'i', 'in', 'iodine', 'law', 'maine', 'match', 'mine', 'mother', 'nephew', 'niece', 'one', 'parent', 'person',\n",
    "                        'rear', 'sister', 'son', 'stepdaughter', 'stepfather', 'stepmother', 'stepson', 'twin', 'uncle', 'widow', 'widower', 'wife']\n",
    "        \n",
    "        ####문장에 char_list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "        #우선 토큰화한다.\n",
    "        retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "        token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "        #print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "        #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "        filtered_chr_text = []\n",
    "        for k in token_input_text:\n",
    "            for j in character_list:\n",
    "                if k == j:\n",
    "                    filtered_chr_text.append(j)\n",
    "        \n",
    "        #print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "        \n",
    "        filtered_chr_text_ = set(filtered_chr_text) #중복제거\n",
    "        filtered_chr_text__ = list(filtered_chr_text_) #다시 리스트로 변환\n",
    "        print (filtered_chr_text__) # 중복값 제거 확인\n",
    "        \n",
    "#         for i in filtered_chr_text__:\n",
    "#             ext_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "        \n",
    "        char_total_count = len(filtered_chr_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 캐릭터 표현 수\n",
    "        char_count_ = len(filtered_chr_text__) #중복제거된 캐릭터 표현 총 수\n",
    "            \n",
    "        result_char_ratio = round(char_total_count/total_words * 100, 2)\n",
    "        return char_total_count\n",
    "\n",
    "\n",
    "\n",
    "    # number_of_characters = NumberofCharacters(input_text) # 문장에서 키워드와 관련된 단어을 모두 추출하면 이런 결과가 나옴, 이 결과를 모두 합쳐서 캐릭터 총 값 계산해서 숫자로 출력\n",
    "    # number_of_characters\n",
    "    # print ('=============================================')\n",
    "    # print ('Number of Characters :', number_of_characters)\n",
    "    # print ('=============================================')\n",
    "\n",
    "    ####################################\n",
    "    #### Character Descriptiveness #####\n",
    "    ####################################\n",
    "\n",
    "    def character_descrip(text):\n",
    "\n",
    "        input_sentence = text\n",
    "\n",
    "        def findSentence(input_sentence):\n",
    "            result = []\n",
    "\n",
    "            data = str(input_sentence)\n",
    "            #data = input_sentence.splitlines()\n",
    "            \n",
    "            findText = ['i', 'my', 'me', 'mine', 'you', 'your', 'they','them',\n",
    "                        'yours', 'he','him','his' 'she','her','it','someone','their', 'myself', 'aunt',\n",
    "                        'brother','cousin','daughter','father','grandchild','granddaughter','granddson','grandfather',\n",
    "                        'grandmother','great-grandchild','husband','ex-husband','son-in-law', 'daughter-in-law','mother',\n",
    "                        'niece','nephew','parents','sister','son','stepfather','stepmother','stepdaughter', 'stepson',\n",
    "                        'twin','uncle','widow','widower','wife','ex-wife','aunt',\n",
    "                        'baby', 'beget', 'brother', 'buddy', 'conserve', 'counterpart', 'cousin', 'daughter', 'duplicate', 'ex',\n",
    "                        'father', 'forefather', 'founder', 'gemini', 'grandchild', 'granddaughter', 'grandfather', 'grandma', 'he', 'helium',\n",
    "                        'husband', 'i', 'in', 'iodine', 'law', 'maine', 'match', 'mine', 'mother', 'nephew', 'niece', 'one', 'parent', 'person',\n",
    "                        'rear', 'sister', 'son', 'stepdaughter', 'stepfather', 'stepmother', 'stepson', 'twin', 'uncle', 'widow', 'widower', 'wife']\n",
    "\n",
    "            sentences = data.split(\".\")\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                for item in findText:\n",
    "                    if item in sentence:\n",
    "                        result.append(sentence)\n",
    "\n",
    "            return result\n",
    "\n",
    "        input_sent_included_character = findSentence(text) \n",
    "        input_sent_chr = set(input_sent_included_character) #중복값을 제거해보자\n",
    "        input_sent_chr = '.'.join(input_sent_chr) #하나의 문자열로 합쳐야 원본 문장처럼 변환되고, 이것을 show/tell 분석코드에 넣게됨\n",
    "\n",
    "\n",
    "\n",
    "        #입력된 전체 문장을 개별문장으로 분리하여 전처리 처리함\n",
    "        def sentence_to_df(input_sentence):\n",
    "\n",
    "            input_text_df = nltk.tokenize.sent_tokenize(input_sentence)\n",
    "            test = []\n",
    "\n",
    "            for i in range(0,len(input_text_df)):\n",
    "                new_label = np.random.randint(0,2)  # 개별문장(input_text_df) 수만큼 0 또는 1 난수 생성\n",
    "                data = [new_label, input_text_df[i]]\n",
    "                test.append(data)\n",
    "\n",
    "            #print(test)\n",
    "            dataf = pd.DataFrame(test, columns=['label', 'text'])\n",
    "            #print(dataf)\n",
    "            return dataf\n",
    "\n",
    "\n",
    "        class STDataset(Dataset):\n",
    "            ''' Showing Telling Corpus Dataset '''\n",
    "            def __init__(self, df):\n",
    "                self.df = df\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.df)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                text = self.df.iloc[idx, 1]\n",
    "                label = int(self.df.iloc[idx, 0])\n",
    "                return text, label\n",
    "\n",
    "\n",
    "        ###########입력받은 데이터 처리 실행하는 메소드 showtell_classfy() ###############\n",
    "        #result_all.html에서 입력받을 text를 contents에 넣고 전처리 후 데이터프레임에 넣어줌\n",
    "        def showtell_classfy(text):\n",
    "            contents = str(text)\n",
    "            preprossed_contents_df = sentence_to_df(contents)\n",
    "\n",
    "            preprossed_contents_df.dropna(inplace=True)\n",
    "            #전처리된 데이터를 확인(데이터프레임으로 가공됨)\n",
    "            preprossed_contents_df__ = preprossed_contents_df.sample(frac=1, random_state=999)\n",
    "            \n",
    "\n",
    "            #파이토치에 입력하기 위해서 로딩...\n",
    "            ST_test_dataset = STDataset(preprossed_contents_df__)\n",
    "            test_loader = DataLoader(ST_test_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "            #로딩되는지 확인\n",
    "            ST_test_dataset.__getitem__(1)\n",
    "\n",
    "            #time.sleep(1)\n",
    "\n",
    "\n",
    "\n",
    "            #check whether cuda is available\n",
    "            #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "            device = torch.device(\"cpu\")  \n",
    "            #device = torch.device(\"cuda\")\n",
    "            #tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "            model = BertForSequenceClassification.from_pretrained('bert-large-cased')\n",
    "            model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "            # for text, label in test_loader :\n",
    "            #     print(\"text:\",text)\n",
    "            #     print(\"label:\",label)\n",
    "\n",
    "\n",
    "            #저장된 모델을 불러온다.\n",
    "            #J:\\Django\\EssayFit_Django\\essayfitaiproject\\essayfitapp\\model.pt\n",
    "            #time.sleep(1)\n",
    "            #model = torch.load(\"/Users/jongphilkim/Desktop/Django_WEB/essayfitaiproject/essayai/model.pt\", map_location=torch.device('cpu'))\n",
    "            model = torch.load(\"model.pt\", map_location=torch.device('cpu'))\n",
    "            print(\"model loadling~\")\n",
    "            model.eval()\n",
    "\n",
    "\n",
    "            pred_loader = test_loader\n",
    "            print(\"pred_loader:\", pred_loader)\n",
    "            total_loss = 0\n",
    "            total_len = 0\n",
    "            total_showing__ = 0\n",
    "            total_telling__ = 0\n",
    "\n",
    "            showing_conunter = [] #문장에 해당하는 SHOWING을 계산한다.\n",
    "            \n",
    "            print(\"check!\")\n",
    "            for text, label in pred_loader:\n",
    "                print(\"text:\",text)\n",
    "                #print(\"label:\",label)\n",
    "                encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text] #text to tokenize\n",
    "                padded_list =  [e + [0] * (512-len(e)) for e in encoded_list] #padding\n",
    "                sample = torch.tensor(padded_list) #torch tensor로 변환\n",
    "                sample, label = sample.to(device), label.to(device) #tokenized text에 label을 넣어서 Device(gpu/cpu)에 넣기 위해 준비\n",
    "                labels = torch.tensor(label) #레이블을 텐서로 변환\n",
    "                #time.sleep(1)\n",
    "                outputs = model(sample,labels=labels) #모델을 통해서 샘플텍스트와 레이블 입력데이터를 출력 output에 넣음\n",
    "                #시간 딜레이를 주자\n",
    "                #time.sleep(1)\n",
    "                _, logits = outputs #outputs를 로짓에 넣음 이것을 softmax에 넣으면 0~1 사이로 결과가 출력됨\n",
    "                \n",
    "                pred = torch.argmax(F.softmax(logits), dim=1) #드디어 예측한다. argmax는 리스트(계산된 값)에서 가장 큰 값을 추출하여 pred에 넣는다. 0 ~1 사이의 값이 나올거임\n",
    "                print('pred :', pred)\n",
    "                # correct = pred.eq(labels) \n",
    "                showing__ = pred.eq(1) # 예측한 결과가 1과 같으면 showing이다   >> TRUE   SHOWING을 추출하려면 이것만 카운드하면 된다. \n",
    "                telling__ = pred.eq(0) # 예측한 결과가 0과 같으면 telling이다   >> FALSE\n",
    "                \n",
    "                #print('showing : ', showing__)\n",
    "                #print('telling : ', telling__)\n",
    "                \n",
    "                \n",
    "                showing_conunter.append(text)        \n",
    "                #pred_ = round(float(pred))\n",
    "                showing_conunter.append(pred)\n",
    "\n",
    "\n",
    "\n",
    "            return showing_conunter \n",
    "\n",
    "\n",
    "        st_re = showtell_classfy(str(input_sent_chr)) # 캐릭터거 포함된 문장(전처리 완료된) 입력\n",
    "\n",
    "        df = DataFrame(st_re)\n",
    "        df_ = df[0::2] # 글만 추출\n",
    "        df_label = df[1::2] # 레이블만 추출\n",
    "\n",
    "        df_.reset_index(drop=True, inplace=True) #데이터를 합치기 위해서 초기화\n",
    "        df_label.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        df_result = pd.concat([df_,df_label],axis=1) #합치기\n",
    "\n",
    "        df_result.columns = ['sentence','show/tell']\n",
    "\n",
    "        df_fin = df_result['show/tell'].value_counts(normalize=True)\n",
    "        list(df_fin)\n",
    "        showing_sentence_with_char = max(round(df_fin*100))\n",
    "\n",
    "        # print(\"===============================================================\")\n",
    "        # print ('Character Descriptiveness : ', showing_sentence_with_char)\n",
    "        # print(\"===============================================================\")\n",
    "\n",
    "        return showing_sentence_with_char\n",
    "\n",
    "\n",
    "\n",
    "    ################################################\n",
    "    #############  Emphasis on YOU  ################\n",
    "    ################################################\n",
    "    def EmphasisOnYou(text):\n",
    "\n",
    "        essay_input_corpus = str(text) #문장입력\n",
    "        essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "        sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "        total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "        total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "        split_sentences = []\n",
    "        for sentence in sentences:\n",
    "            processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "            words = processed.split()\n",
    "            split_sentences.append(words)\n",
    "\n",
    "        skip_gram = 1\n",
    "        workers = multiprocessing.cpu_count()\n",
    "        bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "        model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "        model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "        \n",
    "        #모델 설계 완료\n",
    "\n",
    "        #캐릭터 표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "        character_list = ['i', 'I', 'my', 'me', 'mine', 'one']\n",
    "        \n",
    "        ####문장에 char_list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "        #우선 토큰화한다.\n",
    "        retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "        token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "        #print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "        #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "        filtered_chr_text = []\n",
    "        for k in token_input_text:\n",
    "            for j in character_list:\n",
    "                if k == j:\n",
    "                    filtered_chr_text.append(j)\n",
    "        \n",
    "        #print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "        \n",
    "        filtered_chr_text_ = set(filtered_chr_text) #중복제거\n",
    "        filtered_chr_text__ = list(filtered_chr_text_) #다시 리스트로 변환\n",
    "        print (filtered_chr_text__) # 중복값 제거 확인\n",
    "        \n",
    "#         for i in filtered_chr_text__:\n",
    "#             ext_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "        \n",
    "        char_total_count = len(filtered_chr_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 캐릭터 표현 수\n",
    "        char_count_ = len(filtered_chr_text__) #중복제거된 캐릭터 표현 총 수\n",
    "            \n",
    "        result_char_ratio = round(char_total_count/total_words * 100, 2)\n",
    "        return char_total_count\n",
    "\n",
    "\n",
    "    # EmphasisOnYou_ = EmphasisOnYou(input_text) # 문장에서 키워드와 관련된 단어을 모두 추출하면 이런 결과가 나옴, 이 결과를 모두 합쳐서 캐릭터 총 값 계산해서 숫자로 출력\n",
    "    # EmphasisOnYou_\n",
    "    # print ('=============================================')\n",
    "    # print ('Emphasis on You :', EmphasisOnYou_)\n",
    "    # print ('=============================================')\n",
    "\n",
    "\n",
    "\n",
    "    #########################################\n",
    "    ######### Emphasis on others  ###########\n",
    "    #########################################\n",
    "    def EmphasisOnOthers(text):\n",
    "\n",
    "        essay_input_corpus = str(text) #문장입력\n",
    "        essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "        sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "        total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "        total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "        split_sentences = []\n",
    "        for sentence in sentences:\n",
    "            processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "            words = processed.split()\n",
    "            split_sentences.append(words)\n",
    "\n",
    "        skip_gram = 1\n",
    "        workers = multiprocessing.cpu_count()\n",
    "        bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "        model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "        model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "        \n",
    "        #모델 설계 완료\n",
    "\n",
    "        #캐릭터 표현하는 단어들을 리스트에 넣어서 필터로 만들고\n",
    "        character_list = ['you', 'your', 'they','them',\n",
    "                        'yours', 'he','him','his' 'she','her','it','someone','their', 'myself', 'aunt',\n",
    "                        'brother','cousin','daughter','grandchild','granddaughter','granddson','grandfather',\n",
    "                        'grandmother','great-grandchild','husband','ex-husband','son-in-law', 'daughter-in-law','mother',\n",
    "                        'niece','nephew','parents','sister','son','stepfather','stepmother','stepdaughter', 'stepson',\n",
    "                        'twin','uncle','widow','widower','wife','ex-wife','aunt',\n",
    "                        'baby', 'beget', 'brother', 'buddy', 'conserve', 'counterpart', 'cousin',\n",
    "                        'daughter', 'duplicate', 'ex', 'forefather', 'founder', 'gemini',\n",
    "                        'grandchild', 'granddaughter', 'grandfather', 'grandma', 'he', 'helium', 'husband',\n",
    "                        'in', 'law', 'match', 'mother', 'nephew', 'niece', 'parent', 'person', 'rear',\n",
    "                        'sister', 'son', 'stepdaughter', 'stepfather', 'stepmother', 'stepson', 'twin', 'uncle', 'widow',\n",
    "                        'widower', 'wife']\n",
    "        \n",
    "        ####문장에 char_list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "        #우선 토큰화한다.\n",
    "        retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "        token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "        #print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "        #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "        filtered_chr_text = []\n",
    "        for k in token_input_text:\n",
    "            for j in character_list:\n",
    "                if k == j:\n",
    "                    filtered_chr_text.append(j)\n",
    "        \n",
    "        #print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "        \n",
    "        filtered_chr_text_ = set(filtered_chr_text) #중복제거\n",
    "        filtered_chr_text__ = list(filtered_chr_text_) #다시 리스트로 변환\n",
    "        print (filtered_chr_text__) # 중복값 제거 확인\n",
    "        \n",
    "#         for i in filtered_chr_text__:\n",
    "#             ext_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "        \n",
    "        char_total_count = len(filtered_chr_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 캐릭터 표현 수\n",
    "        char_count_ = len(filtered_chr_text__) #중복제거된 캐릭터 표현 총 수\n",
    "            \n",
    "        result_char_ratio = round(char_total_count/total_words * 100, 2)\n",
    "    \n",
    "    \n",
    "        return char_total_count\n",
    "\n",
    "\n",
    "    # EmphasisOnOthers_ = EmphasisOnOthers(input_text) # 문장에서 키워드와 관련된 단어을 모두 추출하면 이런 결과가 나옴, 이 결과를 모두 합쳐서 캐릭터 총 값 계산해서 숫자로 출력\n",
    "    # EmphasisOnOthers_\n",
    "    # print ('=============================================')\n",
    "    # print ('Emphasis on Others :', EmphasisOnOthers_)\n",
    "    # print ('=============================================')\n",
    "\n",
    "\n",
    "    character_descriptiveness = character_descrip(text)\n",
    "    print(\"===============================================================\")\n",
    "    print ('Character Descriptiveness : ' , character_descriptiveness)\n",
    "    print(\"===============================================================\")\n",
    "\n",
    "\n",
    "    number_of_characters = NumberofCharacters(text) \n",
    "    print ('=============================================')\n",
    "    print ('Number of Characters :' , number_of_characters)\n",
    "    print ('=============================================')\n",
    "\n",
    "\n",
    "    EmphasisOnYou_ = EmphasisOnYou(text)\n",
    "    print ('=============================================')\n",
    "    print ('Emphasis on You :' , EmphasisOnYou_)\n",
    "    print ('=============================================')\n",
    "\n",
    "\n",
    "    EmphasisOnOthers_ = EmphasisOnOthers(text) \n",
    "    print ('=============================================')\n",
    "    print ('Emphasis on Others :' , EmphasisOnOthers_)\n",
    "    print ('=============================================')\n",
    "\n",
    "\n",
    "    return character_descriptiveness, number_of_characters, EmphasisOnYou_, EmphasisOnOthers_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1000명의 개별 에세이 값을 각각 불러와서, 데이터프레임으로 결과값을 저장하고, 표준편차 계산하여 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kimkwangil/Documents/008_AI/Story_Analysis-master 8/CHARACTER_'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['each_personal_essay_538.json',\n",
       " 'each_personal_essay_492.json',\n",
       " 'each_personal_essay_168.json',\n",
       " 'each_personal_essay_884.json',\n",
       " 'each_personal_essay_187.json',\n",
       " 'each_personal_essay_50.json',\n",
       " 'each_personal_essay_304.json',\n",
       " 'each_personal_essay_754.json',\n",
       " 'each_personal_essay_241.json',\n",
       " 'each_personal_essay_611.json',\n",
       " 'each_personal_essay_580.json',\n",
       " 'each_personal_essay_979.json',\n",
       " 'each_personal_essay_703.json',\n",
       " 'each_personal_essay_353.json',\n",
       " 'each_personal_essay_646.json',\n",
       " 'each_personal_essay_216.json',\n",
       " 'each_personal_essay_579.json',\n",
       " 'each_personal_essay_129.json',\n",
       " 'each_personal_essay_980.json',\n",
       " 'each_personal_essay_596.json',\n",
       " 'each_personal_essay_11.json',\n",
       " 'each_personal_essay_345.json',\n",
       " 'each_personal_essay_715.json',\n",
       " 'each_personal_essay_200.json',\n",
       " 'each_personal_essay_650.json',\n",
       " 'each_personal_essay_484.json',\n",
       " 'each_personal_essay_191.json',\n",
       " 'each_personal_essay_938.json',\n",
       " 'each_personal_essay_892.json',\n",
       " 'each_personal_essay_742.json',\n",
       " 'each_personal_essay_312.json',\n",
       " 'each_personal_essay_46.json',\n",
       " 'each_personal_essay_607.json',\n",
       " 'each_personal_essay_257.json',\n",
       " 'each_personal_essay_410.json',\n",
       " 'each_personal_essay_943.json',\n",
       " 'each_personal_essay_806.json',\n",
       " 'each_personal_essay_105.json',\n",
       " 'each_personal_essay_555.json',\n",
       " 'each_personal_essay_6.json',\n",
       " 'each_personal_essay_386.json',\n",
       " 'each_personal_essay_739.json',\n",
       " 'each_personal_essay_693.json',\n",
       " 'each_personal_essay_369.json',\n",
       " 'each_personal_essay_914.json',\n",
       " 'each_personal_essay_447.json',\n",
       " 'each_personal_essay_502.json',\n",
       " 'each_personal_essay_152.json',\n",
       " 'each_personal_essay_851.json',\n",
       " 'each_personal_essay_781.json',\n",
       " 'each_personal_essay_85.json',\n",
       " 'each_personal_essay_294.json',\n",
       " 'each_personal_essay_451.json',\n",
       " 'each_personal_essay_902.json',\n",
       " 'each_personal_essay_847.json',\n",
       " 'each_personal_essay_144.json',\n",
       " 'each_personal_essay_514.json',\n",
       " 'each_personal_essay_93.json',\n",
       " 'each_personal_essay_797.json',\n",
       " 'each_personal_essay_282.json',\n",
       " 'each_personal_essay_778.json',\n",
       " 'each_personal_essay_328.json',\n",
       " 'each_personal_essay_955.json',\n",
       " 'each_personal_essay_406.json',\n",
       " 'each_personal_essay_543.json',\n",
       " 'each_personal_essay_113.json',\n",
       " 'each_personal_essay_810.json',\n",
       " 'each_personal_essay_390.json',\n",
       " 'each_personal_essay_685.json',\n",
       " 'each_personal_essay_758.json',\n",
       " 'each_personal_essay_308.json',\n",
       " 'each_personal_essay_867.json',\n",
       " 'each_personal_essay_164.json',\n",
       " 'each_personal_essay_534.json',\n",
       " 'each_personal_essay_471.json',\n",
       " 'each_personal_essay_922.json',\n",
       " 'each_personal_essay_888.json',\n",
       " 'each_personal_essay_563.json',\n",
       " 'each_personal_essay_133.json',\n",
       " 'each_personal_essay_830.json',\n",
       " 'each_personal_essay_975.json',\n",
       " 'each_personal_essay_426.json',\n",
       " 'each_personal_essay_719.json',\n",
       " 'each_personal_essay_349.json',\n",
       " 'each_personal_essay_826.json',\n",
       " 'each_personal_essay_125.json',\n",
       " 'each_personal_essay_575.json',\n",
       " 'each_personal_essay_430.json',\n",
       " 'each_personal_essay_963.json',\n",
       " 'each_personal_essay_522.json',\n",
       " 'each_personal_essay_488.json',\n",
       " 'each_personal_essay_172.json',\n",
       " 'each_personal_essay_871.json',\n",
       " 'each_personal_essay_934.json',\n",
       " 'each_personal_essay_467.json',\n",
       " 'each_personal_essay_220.json',\n",
       " 'each_personal_essay_670.json',\n",
       " 'each_personal_essay_31.json',\n",
       " 'each_personal_essay_365.json',\n",
       " 'each_personal_essay_735.json',\n",
       " 'each_personal_essay_559.json',\n",
       " 'each_personal_essay_109.json',\n",
       " 'each_personal_essay_89.json',\n",
       " 'each_personal_essay_627.json',\n",
       " 'each_personal_essay_277.json',\n",
       " 'each_personal_essay_298.json',\n",
       " 'each_personal_essay_762.json',\n",
       " 'each_personal_essay_332.json',\n",
       " 'each_personal_essay_66.json',\n",
       " 'each_personal_essay_918.json',\n",
       " 'each_personal_essay_261.json',\n",
       " 'each_personal_essay_631.json',\n",
       " 'each_personal_essay_70.json',\n",
       " 'each_personal_essay_324.json',\n",
       " 'each_personal_essay_774.json',\n",
       " 'each_personal_essay_518.json',\n",
       " 'each_personal_essay_148.json',\n",
       " 'each_personal_essay_666.json',\n",
       " 'each_personal_essay_236.json',\n",
       " 'each_personal_essay_723.json',\n",
       " 'each_personal_essay_689.json',\n",
       " 'each_personal_essay_373.json',\n",
       " 'each_personal_essay_27.json',\n",
       " 'each_personal_essay_959.json',\n",
       " 'each_personal_essay_958.json',\n",
       " 'each_personal_essay_667.json',\n",
       " 'each_personal_essay_237.json',\n",
       " 'each_personal_essay_722.json',\n",
       " 'each_personal_essay_688.json',\n",
       " 'each_personal_essay_372.json',\n",
       " 'each_personal_essay_26.json',\n",
       " 'each_personal_essay_519.json',\n",
       " 'each_personal_essay_149.json',\n",
       " 'each_personal_essay_260.json',\n",
       " 'each_personal_essay_630.json',\n",
       " 'each_personal_essay_71.json',\n",
       " 'each_personal_essay_325.json',\n",
       " 'each_personal_essay_775.json',\n",
       " 'each_personal_essay_919.json',\n",
       " 'each_personal_essay_88.json',\n",
       " 'each_personal_essay_626.json',\n",
       " 'each_personal_essay_276.json',\n",
       " 'each_personal_essay_299.json',\n",
       " 'each_personal_essay_763.json',\n",
       " 'each_personal_essay_333.json',\n",
       " 'each_personal_essay_67.json',\n",
       " 'each_personal_essay_558.json',\n",
       " 'each_personal_essay_108.json',\n",
       " 'each_personal_essay_221.json',\n",
       " 'each_personal_essay_671.json',\n",
       " 'each_personal_essay_30.json',\n",
       " 'each_personal_essay_364.json',\n",
       " 'each_personal_essay_734.json',\n",
       " 'each_personal_essay_523.json',\n",
       " 'each_personal_essay_489.json',\n",
       " 'each_personal_essay_173.json',\n",
       " 'each_personal_essay_870.json',\n",
       " 'each_personal_essay_935.json',\n",
       " 'each_personal_essay_466.json',\n",
       " 'each_personal_essay_827.json',\n",
       " 'each_personal_essay_124.json',\n",
       " 'each_personal_essay_574.json',\n",
       " 'each_personal_essay_431.json',\n",
       " 'each_personal_essay_962.json',\n",
       " 'each_personal_essay_718.json',\n",
       " 'each_personal_essay_348.json',\n",
       " 'each_personal_essay_562.json',\n",
       " 'each_personal_essay_132.json',\n",
       " 'each_personal_essay_831.json',\n",
       " 'each_personal_essay_974.json',\n",
       " 'each_personal_essay_427.json',\n",
       " 'each_personal_essay_866.json',\n",
       " 'each_personal_essay_165.json',\n",
       " 'each_personal_essay_535.json',\n",
       " 'each_personal_essay_470.json',\n",
       " 'each_personal_essay_923.json',\n",
       " 'each_personal_essay_889.json',\n",
       " 'each_personal_essay_759.json',\n",
       " 'each_personal_essay_309.json',\n",
       " 'each_personal_essay_391.json',\n",
       " 'each_personal_essay_684.json',\n",
       " 'each_personal_essay_954.json',\n",
       " 'each_personal_essay_407.json',\n",
       " 'each_personal_essay_542.json',\n",
       " 'each_personal_essay_112.json',\n",
       " 'each_personal_essay_811.json',\n",
       " 'each_personal_essay_92.json',\n",
       " 'each_personal_essay_796.json',\n",
       " 'each_personal_essay_283.json',\n",
       " 'each_personal_essay_779.json',\n",
       " 'each_personal_essay_329.json',\n",
       " 'each_personal_essay_450.json',\n",
       " 'each_personal_essay_903.json',\n",
       " 'each_personal_essay_846.json',\n",
       " 'each_personal_essay_145.json',\n",
       " 'each_personal_essay_515.json',\n",
       " 'each_personal_essay_780.json',\n",
       " 'each_personal_essay_84.json',\n",
       " 'each_personal_essay_295.json',\n",
       " 'each_personal_essay_915.json',\n",
       " 'each_personal_essay_446.json',\n",
       " 'each_personal_essay_503.json',\n",
       " 'each_personal_essay_153.json',\n",
       " 'each_personal_essay_850.json',\n",
       " 'each_personal_essay_387.json',\n",
       " 'each_personal_essay_7.json',\n",
       " 'each_personal_essay_738.json',\n",
       " 'each_personal_essay_692.json',\n",
       " 'each_personal_essay_368.json',\n",
       " 'each_personal_essay_411.json',\n",
       " 'each_personal_essay_942.json',\n",
       " 'each_personal_essay_807.json',\n",
       " 'each_personal_essay_104.json',\n",
       " 'each_personal_essay_554.json',\n",
       " 'each_personal_essay_743.json',\n",
       " 'each_personal_essay_313.json',\n",
       " 'each_personal_essay_47.json',\n",
       " 'each_personal_essay_606.json',\n",
       " 'each_personal_essay_256.json',\n",
       " 'each_personal_essay_485.json',\n",
       " 'each_personal_essay_190.json',\n",
       " 'each_personal_essay_939.json',\n",
       " 'each_personal_essay_893.json',\n",
       " 'each_personal_essay_10.json',\n",
       " 'each_personal_essay_344.json',\n",
       " 'each_personal_essay_714.json',\n",
       " 'each_personal_essay_201.json',\n",
       " 'each_personal_essay_651.json',\n",
       " 'each_personal_essay_578.json',\n",
       " 'each_personal_essay_128.json',\n",
       " 'each_personal_essay_597.json',\n",
       " 'each_personal_essay_702.json',\n",
       " 'each_personal_essay_352.json',\n",
       " 'each_personal_essay_647.json',\n",
       " 'each_personal_essay_217.json',\n",
       " 'each_personal_essay_581.json',\n",
       " 'each_personal_essay_978.json',\n",
       " 'each_personal_essay_51.json',\n",
       " 'each_personal_essay_305.json',\n",
       " 'each_personal_essay_755.json',\n",
       " 'each_personal_essay_240.json',\n",
       " 'each_personal_essay_610.json',\n",
       " 'each_personal_essay_539.json',\n",
       " 'each_personal_essay_493.json',\n",
       " 'each_personal_essay_169.json',\n",
       " 'each_personal_essay_885.json',\n",
       " 'each_personal_essay_186.json',\n",
       " 'each_personal_essay_498.json',\n",
       " 'each_personal_essay_162.json',\n",
       " 'each_personal_essay_532.json',\n",
       " 'each_personal_essay_861.json',\n",
       " 'each_personal_essay_924.json',\n",
       " 'each_personal_essay_477.json',\n",
       " 'each_personal_essay_359.json',\n",
       " 'each_personal_essay_709.json',\n",
       " 'each_personal_essay_836.json',\n",
       " 'each_personal_essay_565.json',\n",
       " 'each_personal_essay_135.json',\n",
       " 'each_personal_essay_420.json',\n",
       " 'each_personal_essay_973.json',\n",
       " 'each_personal_essay_123.json',\n",
       " 'each_personal_essay_573.json',\n",
       " 'each_personal_essay_820.json',\n",
       " 'each_personal_essay_965.json',\n",
       " 'each_personal_essay_436.json',\n",
       " 'each_personal_essay_318.json',\n",
       " 'each_personal_essay_748.json',\n",
       " 'each_personal_essay_877.json',\n",
       " 'each_personal_essay_524.json',\n",
       " 'each_personal_essay_174.json',\n",
       " 'each_personal_essay_461.json',\n",
       " 'each_personal_essay_898.json',\n",
       " 'each_personal_essay_932.json',\n",
       " 'each_personal_essay_226.json',\n",
       " 'each_personal_essay_676.json',\n",
       " 'each_personal_essay_699.json',\n",
       " 'each_personal_essay_363.json',\n",
       " 'each_personal_essay_733.json',\n",
       " 'each_personal_essay_37.json',\n",
       " 'each_personal_essay_949.json',\n",
       " 'each_personal_essay_621.json',\n",
       " 'each_personal_essay_271.json',\n",
       " 'each_personal_essay_60.json',\n",
       " 'each_personal_essay_764.json',\n",
       " 'each_personal_essay_334.json',\n",
       " 'each_personal_essay_158.json',\n",
       " 'each_personal_essay_508.json',\n",
       " 'each_personal_essay_99.json',\n",
       " 'each_personal_essay_267.json',\n",
       " 'each_personal_essay_637.json',\n",
       " 'each_personal_essay_322.json',\n",
       " 'each_personal_essay_288.json',\n",
       " 'each_personal_essay_772.json',\n",
       " 'each_personal_essay_76.json',\n",
       " 'each_personal_essay_908.json',\n",
       " 'each_personal_essay_660.json',\n",
       " 'each_personal_essay_230.json',\n",
       " 'each_personal_essay_21.json',\n",
       " 'each_personal_essay_725.json',\n",
       " 'each_personal_essay_375.json',\n",
       " 'each_personal_essay_119.json',\n",
       " 'each_personal_essay_549.json',\n",
       " 'each_personal_essay_494.json',\n",
       " 'each_personal_essay_181.json',\n",
       " 'each_personal_essay_882.json',\n",
       " 'each_personal_essay_928.json',\n",
       " 'each_personal_essay_302.json',\n",
       " 'each_personal_essay_752.json',\n",
       " 'each_personal_essay_56.json',\n",
       " 'each_personal_essay_247.json',\n",
       " 'each_personal_essay_617.json',\n",
       " 'each_personal_essay_139.json',\n",
       " 'each_personal_essay_569.json',\n",
       " 'each_personal_essay_586.json',\n",
       " 'each_personal_essay_705.json',\n",
       " 'each_personal_essay_355.json',\n",
       " 'each_personal_essay_640.json',\n",
       " 'each_personal_essay_210.json',\n",
       " 'each_personal_essay_590.json',\n",
       " 'each_personal_essay_969.json',\n",
       " 'each_personal_essay_343.json',\n",
       " 'each_personal_essay_713.json',\n",
       " 'each_personal_essay_17.json',\n",
       " 'each_personal_essay_206.json',\n",
       " 'each_personal_essay_656.json',\n",
       " 'each_personal_essay_482.json',\n",
       " 'each_personal_essay_178.json',\n",
       " 'each_personal_essay_528.json',\n",
       " 'each_personal_essay_894.json',\n",
       " 'each_personal_essay_197.json',\n",
       " 'each_personal_essay_40.json',\n",
       " 'each_personal_essay_744.json',\n",
       " 'each_personal_essay_314.json',\n",
       " 'each_personal_essay_601.json',\n",
       " 'each_personal_essay_251.json',\n",
       " 'each_personal_essay_945.json',\n",
       " 'each_personal_essay_416.json',\n",
       " 'each_personal_essay_103.json',\n",
       " 'each_personal_essay_553.json',\n",
       " 'each_personal_essay_800.json',\n",
       " 'each_personal_essay_0.json',\n",
       " 'each_personal_essay_380.json',\n",
       " 'each_personal_essay_695.json',\n",
       " 'each_personal_essay_441.json',\n",
       " 'each_personal_essay_912.json',\n",
       " 'each_personal_essay_857.json',\n",
       " 'each_personal_essay_504.json',\n",
       " 'each_personal_essay_154.json',\n",
       " 'each_personal_essay_83.json',\n",
       " 'each_personal_essay_787.json',\n",
       " 'each_personal_essay_338.json',\n",
       " 'each_personal_essay_292.json',\n",
       " 'each_personal_essay_768.json',\n",
       " 'each_personal_essay_904.json',\n",
       " 'each_personal_essay_457.json',\n",
       " 'each_personal_essay_142.json',\n",
       " 'each_personal_essay_512.json',\n",
       " 'each_personal_essay_841.json',\n",
       " 'each_personal_essay_791.json',\n",
       " 'each_personal_essay_95.json',\n",
       " 'each_personal_essay_284.json',\n",
       " 'each_personal_essay_400.json',\n",
       " 'each_personal_essay_953.json',\n",
       " 'each_personal_essay_816.json',\n",
       " 'each_personal_essay_545.json',\n",
       " 'each_personal_essay_115.json',\n",
       " 'each_personal_essay_396.json',\n",
       " 'each_personal_essay_683.json',\n",
       " 'each_personal_essay_379.json',\n",
       " 'each_personal_essay_729.json',\n",
       " 'each_personal_essay_397.json',\n",
       " 'each_personal_essay_682.json',\n",
       " 'each_personal_essay_378.json',\n",
       " 'each_personal_essay_728.json',\n",
       " 'each_personal_essay_401.json',\n",
       " 'each_personal_essay_952.json',\n",
       " 'each_personal_essay_817.json',\n",
       " 'each_personal_essay_544.json',\n",
       " 'each_personal_essay_114.json',\n",
       " 'each_personal_essay_790.json',\n",
       " 'each_personal_essay_94.json',\n",
       " 'each_personal_essay_285.json',\n",
       " 'each_personal_essay_905.json',\n",
       " 'each_personal_essay_456.json',\n",
       " 'each_personal_essay_143.json',\n",
       " 'each_personal_essay_513.json',\n",
       " 'each_personal_essay_840.json',\n",
       " 'each_personal_essay_82.json',\n",
       " 'each_personal_essay_786.json',\n",
       " 'each_personal_essay_339.json',\n",
       " 'each_personal_essay_293.json',\n",
       " 'each_personal_essay_769.json',\n",
       " 'each_personal_essay_440.json',\n",
       " 'each_personal_essay_913.json',\n",
       " 'each_personal_essay_856.json',\n",
       " 'each_personal_essay_505.json',\n",
       " 'each_personal_essay_155.json',\n",
       " 'each_personal_essay_381.json',\n",
       " 'each_personal_essay_1.json',\n",
       " 'each_personal_essay_694.json',\n",
       " 'each_personal_essay_944.json',\n",
       " 'each_personal_essay_417.json',\n",
       " 'each_personal_essay_102.json',\n",
       " 'each_personal_essay_552.json',\n",
       " 'each_personal_essay_801.json',\n",
       " 'each_personal_essay_41.json',\n",
       " 'each_personal_essay_745.json',\n",
       " 'each_personal_essay_315.json',\n",
       " 'each_personal_essay_600.json',\n",
       " 'each_personal_essay_250.json',\n",
       " 'each_personal_essay_483.json',\n",
       " 'each_personal_essay_179.json',\n",
       " 'each_personal_essay_529.json',\n",
       " 'each_personal_essay_895.json',\n",
       " 'each_personal_essay_196.json',\n",
       " 'each_personal_essay_342.json',\n",
       " 'each_personal_essay_712.json',\n",
       " 'each_personal_essay_16.json',\n",
       " 'each_personal_essay_207.json',\n",
       " 'each_personal_essay_657.json',\n",
       " 'each_personal_essay_591.json',\n",
       " 'each_personal_essay_968.json',\n",
       " 'each_personal_essay_704.json',\n",
       " 'each_personal_essay_354.json',\n",
       " 'each_personal_essay_641.json',\n",
       " 'each_personal_essay_211.json',\n",
       " 'each_personal_essay_138.json',\n",
       " 'each_personal_essay_568.json',\n",
       " 'each_personal_essay_587.json',\n",
       " 'each_personal_essay_303.json',\n",
       " 'each_personal_essay_753.json',\n",
       " 'each_personal_essay_57.json',\n",
       " 'each_personal_essay_246.json',\n",
       " 'each_personal_essay_616.json',\n",
       " 'each_personal_essay_495.json',\n",
       " 'each_personal_essay_180.json',\n",
       " 'each_personal_essay_883.json',\n",
       " 'each_personal_essay_929.json',\n",
       " 'each_personal_essay_118.json',\n",
       " 'each_personal_essay_548.json',\n",
       " 'each_personal_essay_661.json',\n",
       " 'each_personal_essay_231.json',\n",
       " 'each_personal_essay_20.json',\n",
       " 'each_personal_essay_724.json',\n",
       " 'each_personal_essay_374.json',\n",
       " 'each_personal_essay_909.json',\n",
       " 'each_personal_essay_98.json',\n",
       " 'each_personal_essay_266.json',\n",
       " 'each_personal_essay_636.json',\n",
       " 'each_personal_essay_323.json',\n",
       " 'each_personal_essay_289.json',\n",
       " 'each_personal_essay_773.json',\n",
       " 'each_personal_essay_77.json',\n",
       " 'each_personal_essay_159.json',\n",
       " 'each_personal_essay_509.json',\n",
       " 'each_personal_essay_620.json',\n",
       " 'each_personal_essay_270.json',\n",
       " 'each_personal_essay_61.json',\n",
       " 'each_personal_essay_765.json',\n",
       " 'each_personal_essay_335.json',\n",
       " 'each_personal_essay_948.json',\n",
       " 'each_personal_essay_227.json',\n",
       " 'each_personal_essay_677.json',\n",
       " 'each_personal_essay_698.json',\n",
       " 'each_personal_essay_362.json',\n",
       " 'each_personal_essay_732.json',\n",
       " 'each_personal_essay_36.json',\n",
       " 'each_personal_essay_876.json',\n",
       " 'each_personal_essay_525.json',\n",
       " 'each_personal_essay_175.json',\n",
       " 'each_personal_essay_460.json',\n",
       " 'each_personal_essay_899.json',\n",
       " 'each_personal_essay_933.json',\n",
       " 'each_personal_essay_319.json',\n",
       " 'each_personal_essay_749.json',\n",
       " 'each_personal_essay_122.json',\n",
       " 'each_personal_essay_572.json',\n",
       " 'each_personal_essay_821.json',\n",
       " 'each_personal_essay_964.json',\n",
       " 'each_personal_essay_437.json',\n",
       " 'each_personal_essay_837.json',\n",
       " 'each_personal_essay_564.json',\n",
       " 'each_personal_essay_134.json',\n",
       " 'each_personal_essay_421.json',\n",
       " 'each_personal_essay_972.json',\n",
       " 'each_personal_essay_358.json',\n",
       " 'each_personal_essay_708.json',\n",
       " 'each_personal_essay_499.json',\n",
       " 'each_personal_essay_163.json',\n",
       " 'each_personal_essay_533.json',\n",
       " 'each_personal_essay_860.json',\n",
       " 'each_personal_essay_925.json',\n",
       " 'each_personal_essay_476.json',\n",
       " 'each_personal_essay_818.json',\n",
       " 'each_personal_essay_727.json',\n",
       " 'each_personal_essay_377.json',\n",
       " 'each_personal_essay_23.json',\n",
       " 'each_personal_essay_398.json',\n",
       " 'each_personal_essay_662.json',\n",
       " 'each_personal_essay_232.json',\n",
       " 'each_personal_essay_459.json',\n",
       " 'each_personal_essay_74.json',\n",
       " 'each_personal_essay_320.json',\n",
       " 'each_personal_essay_770.json',\n",
       " 'each_personal_essay_265.json',\n",
       " 'each_personal_essay_635.json',\n",
       " 'each_personal_essay_859.json',\n",
       " 'each_personal_essay_766.json',\n",
       " 'each_personal_essay_336.json',\n",
       " 'each_personal_essay_62.json',\n",
       " 'each_personal_essay_623.json',\n",
       " 'each_personal_essay_789.json',\n",
       " 'each_personal_essay_273.json',\n",
       " 'each_personal_essay_418.json',\n",
       " 'each_personal_essay_35.json',\n",
       " 'each_personal_essay_361.json',\n",
       " 'each_personal_essay_731.json',\n",
       " 'each_personal_essay_224.json',\n",
       " 'each_personal_essay_674.json',\n",
       " 'each_personal_essay_930.json',\n",
       " 'each_personal_essay_199.json',\n",
       " 'each_personal_essay_463.json',\n",
       " 'each_personal_essay_526.json',\n",
       " 'each_personal_essay_176.json',\n",
       " 'each_personal_essay_875.json',\n",
       " 'each_personal_essay_434.json',\n",
       " 'each_personal_essay_967.json',\n",
       " 'each_personal_essay_822.json',\n",
       " 'each_personal_essay_121.json',\n",
       " 'each_personal_essay_571.json',\n",
       " 'each_personal_essay_658.json',\n",
       " 'each_personal_essay_208.json',\n",
       " 'each_personal_essay_19.json',\n",
       " 'each_personal_essay_971.json',\n",
       " 'each_personal_essay_422.json',\n",
       " 'each_personal_essay_588.json',\n",
       " 'each_personal_essay_567.json',\n",
       " 'each_personal_essay_137.json',\n",
       " 'each_personal_essay_834.json',\n",
       " 'each_personal_essay_475.json',\n",
       " 'each_personal_essay_926.json',\n",
       " 'each_personal_essay_863.json',\n",
       " 'each_personal_essay_160.json',\n",
       " 'each_personal_essay_530.json',\n",
       " 'each_personal_essay_619.json',\n",
       " 'each_personal_essay_249.json',\n",
       " 'each_personal_essay_58.json',\n",
       " 'each_personal_essay_681.json',\n",
       " 'each_personal_essay_394.json',\n",
       " 'each_personal_essay_547.json',\n",
       " 'each_personal_essay_117.json',\n",
       " 'each_personal_essay_814.json',\n",
       " 'each_personal_essay_951.json',\n",
       " 'each_personal_essay_402.json',\n",
       " 'each_personal_essay_286.json',\n",
       " 'each_personal_essay_78.json',\n",
       " 'each_personal_essay_97.json',\n",
       " 'each_personal_essay_639.json',\n",
       " 'each_personal_essay_793.json',\n",
       " 'each_personal_essay_269.json',\n",
       " 'each_personal_essay_843.json',\n",
       " 'each_personal_essay_140.json',\n",
       " 'each_personal_essay_510.json',\n",
       " 'each_personal_essay_455.json',\n",
       " 'each_personal_essay_906.json',\n",
       " 'each_personal_essay_290.json',\n",
       " 'each_personal_essay_785.json',\n",
       " 'each_personal_essay_81.json',\n",
       " 'each_personal_essay_506.json',\n",
       " 'each_personal_essay_156.json',\n",
       " 'each_personal_essay_855.json',\n",
       " 'each_personal_essay_910.json',\n",
       " 'each_personal_essay_443.json',\n",
       " 'each_personal_essay_697.json',\n",
       " 'each_personal_essay_39.json',\n",
       " 'each_personal_essay_382.json',\n",
       " 'each_personal_essay_2.json',\n",
       " 'each_personal_essay_678.json',\n",
       " 'each_personal_essay_228.json',\n",
       " 'each_personal_essay_802.json',\n",
       " 'each_personal_essay_101.json',\n",
       " 'each_personal_essay_551.json',\n",
       " 'each_personal_essay_414.json',\n",
       " 'each_personal_essay_947.json',\n",
       " 'each_personal_essay_603.json',\n",
       " 'each_personal_essay_253.json',\n",
       " 'each_personal_essay_746.json',\n",
       " 'each_personal_essay_316.json',\n",
       " 'each_personal_essay_42.json',\n",
       " 'each_personal_essay_195.json',\n",
       " 'each_personal_essay_896.json',\n",
       " 'each_personal_essay_879.json',\n",
       " 'each_personal_essay_480.json',\n",
       " 'each_personal_essay_204.json',\n",
       " 'each_personal_essay_654.json',\n",
       " 'each_personal_essay_15.json',\n",
       " 'each_personal_essay_341.json',\n",
       " 'each_personal_essay_711.json',\n",
       " 'each_personal_essay_438.json',\n",
       " 'each_personal_essay_592.json',\n",
       " 'each_personal_essay_642.json',\n",
       " 'each_personal_essay_212.json',\n",
       " 'each_personal_essay_707.json',\n",
       " 'each_personal_essay_357.json',\n",
       " 'each_personal_essay_584.json',\n",
       " 'each_personal_essay_838.json',\n",
       " 'each_personal_essay_245.json',\n",
       " 'each_personal_essay_615.json',\n",
       " 'each_personal_essay_54.json',\n",
       " 'each_personal_essay_300.json',\n",
       " 'each_personal_essay_750.json',\n",
       " 'each_personal_essay_880.json',\n",
       " 'each_personal_essay_183.json',\n",
       " 'each_personal_essay_479.json',\n",
       " 'each_personal_essay_496.json',\n",
       " 'each_personal_essay_881.json',\n",
       " 'each_personal_essay_182.json',\n",
       " 'each_personal_essay_478.json',\n",
       " 'each_personal_essay_497.json',\n",
       " 'each_personal_essay_244.json',\n",
       " 'each_personal_essay_614.json',\n",
       " 'each_personal_essay_55.json',\n",
       " 'each_personal_essay_301.json',\n",
       " 'each_personal_essay_751.json',\n",
       " 'each_personal_essay_585.json',\n",
       " 'each_personal_essay_839.json',\n",
       " 'each_personal_essay_643.json',\n",
       " 'each_personal_essay_213.json',\n",
       " 'each_personal_essay_706.json',\n",
       " 'each_personal_essay_356.json',\n",
       " 'each_personal_essay_439.json',\n",
       " 'each_personal_essay_593.json',\n",
       " 'each_personal_essay_205.json',\n",
       " 'each_personal_essay_655.json',\n",
       " 'each_personal_essay_14.json',\n",
       " 'each_personal_essay_340.json',\n",
       " 'each_personal_essay_710.json',\n",
       " 'each_personal_essay_194.json',\n",
       " 'each_personal_essay_897.json',\n",
       " 'each_personal_essay_878.json',\n",
       " 'each_personal_essay_481.json',\n",
       " 'each_personal_essay_602.json',\n",
       " 'each_personal_essay_252.json',\n",
       " 'each_personal_essay_747.json',\n",
       " 'each_personal_essay_317.json',\n",
       " 'each_personal_essay_43.json',\n",
       " 'each_personal_essay_803.json',\n",
       " 'each_personal_essay_100.json',\n",
       " 'each_personal_essay_550.json',\n",
       " 'each_personal_essay_415.json',\n",
       " 'each_personal_essay_946.json',\n",
       " 'each_personal_essay_696.json',\n",
       " 'each_personal_essay_38.json',\n",
       " 'each_personal_essay_3.json',\n",
       " 'each_personal_essay_383.json',\n",
       " 'each_personal_essay_679.json',\n",
       " 'each_personal_essay_229.json',\n",
       " 'each_personal_essay_507.json',\n",
       " 'each_personal_essay_157.json',\n",
       " 'each_personal_essay_854.json',\n",
       " 'each_personal_essay_911.json',\n",
       " 'each_personal_essay_442.json',\n",
       " 'each_personal_essay_291.json',\n",
       " 'each_personal_essay_784.json',\n",
       " 'each_personal_essay_80.json',\n",
       " 'each_personal_essay_842.json',\n",
       " 'each_personal_essay_141.json',\n",
       " 'each_personal_essay_511.json',\n",
       " 'each_personal_essay_454.json',\n",
       " 'each_personal_essay_907.json',\n",
       " 'each_personal_essay_287.json',\n",
       " 'each_personal_essay_79.json',\n",
       " 'each_personal_essay_96.json',\n",
       " 'each_personal_essay_638.json',\n",
       " 'each_personal_essay_792.json',\n",
       " 'each_personal_essay_268.json',\n",
       " 'each_personal_essay_546.json',\n",
       " 'each_personal_essay_116.json',\n",
       " 'each_personal_essay_815.json',\n",
       " 'each_personal_essay_950.json',\n",
       " 'each_personal_essay_403.json',\n",
       " 'each_personal_essay_680.json',\n",
       " 'each_personal_essay_395.json',\n",
       " 'each_personal_essay_618.json',\n",
       " 'each_personal_essay_248.json',\n",
       " 'each_personal_essay_59.json',\n",
       " 'each_personal_essay_474.json',\n",
       " 'each_personal_essay_927.json',\n",
       " 'each_personal_essay_862.json',\n",
       " 'each_personal_essay_161.json',\n",
       " 'each_personal_essay_531.json',\n",
       " 'each_personal_essay_970.json',\n",
       " 'each_personal_essay_423.json',\n",
       " 'each_personal_essay_589.json',\n",
       " 'each_personal_essay_566.json',\n",
       " 'each_personal_essay_136.json',\n",
       " 'each_personal_essay_835.json',\n",
       " 'each_personal_essay_659.json',\n",
       " 'each_personal_essay_209.json',\n",
       " 'each_personal_essay_18.json',\n",
       " 'each_personal_essay_435.json',\n",
       " 'each_personal_essay_966.json',\n",
       " 'each_personal_essay_823.json',\n",
       " 'each_personal_essay_120.json',\n",
       " 'each_personal_essay_570.json',\n",
       " 'each_personal_essay_931.json',\n",
       " 'each_personal_essay_198.json',\n",
       " 'each_personal_essay_462.json',\n",
       " 'each_personal_essay_527.json',\n",
       " 'each_personal_essay_177.json',\n",
       " 'each_personal_essay_874.json',\n",
       " 'each_personal_essay_34.json',\n",
       " 'each_personal_essay_360.json',\n",
       " 'each_personal_essay_730.json',\n",
       " 'each_personal_essay_225.json',\n",
       " 'each_personal_essay_675.json',\n",
       " 'each_personal_essay_419.json',\n",
       " 'each_personal_essay_767.json',\n",
       " 'each_personal_essay_337.json',\n",
       " 'each_personal_essay_63.json',\n",
       " 'each_personal_essay_622.json',\n",
       " 'each_personal_essay_788.json',\n",
       " 'each_personal_essay_272.json',\n",
       " 'each_personal_essay_858.json',\n",
       " 'each_personal_essay_75.json',\n",
       " 'each_personal_essay_321.json',\n",
       " 'each_personal_essay_771.json',\n",
       " 'each_personal_essay_264.json',\n",
       " 'each_personal_essay_634.json',\n",
       " 'each_personal_essay_458.json',\n",
       " 'each_personal_essay_726.json',\n",
       " 'each_personal_essay_376.json',\n",
       " 'each_personal_essay_22.json',\n",
       " 'each_personal_essay_399.json',\n",
       " 'each_personal_essay_663.json',\n",
       " 'each_personal_essay_233.json',\n",
       " 'each_personal_essay_819.json',\n",
       " 'each_personal_essay_687.json',\n",
       " 'each_personal_essay_29.json',\n",
       " 'each_personal_essay_238.json',\n",
       " 'each_personal_essay_392.json',\n",
       " 'each_personal_essay_668.json',\n",
       " 'each_personal_essay_812.json',\n",
       " 'each_personal_essay_541.json',\n",
       " 'each_personal_essay_111.json',\n",
       " 'each_personal_essay_404.json',\n",
       " 'each_personal_essay_957.json',\n",
       " 'each_personal_essay_280.json',\n",
       " 'each_personal_essay_795.json',\n",
       " 'each_personal_essay_91.json',\n",
       " 'each_personal_essay_146.json',\n",
       " 'each_personal_essay_516.json',\n",
       " 'each_personal_essay_845.json',\n",
       " 'each_personal_essay_900.json',\n",
       " 'each_personal_essay_453.json',\n",
       " 'each_personal_essay_296.json',\n",
       " 'each_personal_essay_68.json',\n",
       " 'each_personal_essay_87.json',\n",
       " 'each_personal_essay_783.json',\n",
       " 'each_personal_essay_279.json',\n",
       " 'each_personal_essay_629.json',\n",
       " 'each_personal_essay_853.json',\n",
       " 'each_personal_essay_500.json',\n",
       " 'each_personal_essay_150.json',\n",
       " 'each_personal_essay_445.json',\n",
       " 'each_personal_essay_916.json',\n",
       " 'each_personal_essay_691.json',\n",
       " 'each_personal_essay_384.json',\n",
       " 'each_personal_essay_4.json',\n",
       " 'each_personal_essay_107.json',\n",
       " 'each_personal_essay_557.json',\n",
       " 'each_personal_essay_804.json',\n",
       " 'each_personal_essay_941.json',\n",
       " 'each_personal_essay_412.json',\n",
       " 'each_personal_essay_605.json',\n",
       " 'each_personal_essay_255.json',\n",
       " 'each_personal_essay_44.json',\n",
       " 'each_personal_essay_740.json',\n",
       " 'each_personal_essay_310.json',\n",
       " 'each_personal_essay_890.json',\n",
       " 'each_personal_essay_193.json',\n",
       " 'each_personal_essay_469.json',\n",
       " 'each_personal_essay_486.json',\n",
       " 'each_personal_essay_202.json',\n",
       " 'each_personal_essay_652.json',\n",
       " 'each_personal_essay_347.json',\n",
       " 'each_personal_essay_717.json',\n",
       " 'each_personal_essay_13.json',\n",
       " 'each_personal_essay_594.json',\n",
       " 'each_personal_essay_828.json',\n",
       " 'each_personal_essay_644.json',\n",
       " 'each_personal_essay_214.json',\n",
       " 'each_personal_essay_701.json',\n",
       " 'each_personal_essay_351.json',\n",
       " 'each_personal_essay_582.json',\n",
       " 'each_personal_essay_428.json',\n",
       " 'each_personal_essay_243.json',\n",
       " 'each_personal_essay_613.json',\n",
       " 'each_personal_essay_306.json',\n",
       " 'each_personal_essay_756.json',\n",
       " 'each_personal_essay_52.json',\n",
       " 'each_personal_essay_185.json',\n",
       " 'each_personal_essay_886.json',\n",
       " 'each_personal_essay_869.json',\n",
       " 'each_personal_essay_490.json',\n",
       " 'each_personal_essay_408.json',\n",
       " 'each_personal_essay_25.json',\n",
       " 'each_personal_essay_721.json',\n",
       " 'each_personal_essay_371.json',\n",
       " 'each_personal_essay_664.json',\n",
       " 'each_personal_essay_234.json',\n",
       " 'each_personal_essay_849.json',\n",
       " 'each_personal_essay_326.json',\n",
       " 'each_personal_essay_776.json',\n",
       " 'each_personal_essay_72.json',\n",
       " 'each_personal_essay_799.json',\n",
       " 'each_personal_essay_263.json',\n",
       " 'each_personal_essay_633.json',\n",
       " 'each_personal_essay_449.json',\n",
       " 'each_personal_essay_64.json',\n",
       " 'each_personal_essay_760.json',\n",
       " 'each_personal_essay_330.json',\n",
       " 'each_personal_essay_625.json',\n",
       " 'each_personal_essay_275.json',\n",
       " 'each_personal_essay_808.json',\n",
       " 'each_personal_essay_367.json',\n",
       " 'each_personal_essay_737.json',\n",
       " 'each_personal_essay_33.json',\n",
       " 'each_personal_essay_222.json',\n",
       " 'each_personal_essay_8.json',\n",
       " 'each_personal_essay_388.json',\n",
       " 'each_personal_essay_672.json',\n",
       " 'each_personal_essay_465.json',\n",
       " 'each_personal_essay_936.json',\n",
       " 'each_personal_essay_873.json',\n",
       " 'each_personal_essay_520.json',\n",
       " 'each_personal_essay_170.json',\n",
       " 'each_personal_essay_259.json',\n",
       " 'each_personal_essay_609.json',\n",
       " 'each_personal_essay_48.json',\n",
       " 'each_personal_essay_961.json',\n",
       " 'each_personal_essay_598.json',\n",
       " 'each_personal_essay_432.json',\n",
       " 'each_personal_essay_127.json',\n",
       " 'each_personal_essay_577.json',\n",
       " 'each_personal_essay_824.json',\n",
       " 'each_personal_essay_424.json',\n",
       " 'each_personal_essay_977.json',\n",
       " 'each_personal_essay_832.json',\n",
       " 'each_personal_essay_561.json',\n",
       " 'each_personal_essay_131.json',\n",
       " 'each_personal_essay_218.json',\n",
       " 'each_personal_essay_648.json',\n",
       " 'each_personal_essay_920.json',\n",
       " 'each_personal_essay_189.json',\n",
       " 'each_personal_essay_473.json',\n",
       " 'each_personal_essay_166.json',\n",
       " 'each_personal_essay_536.json',\n",
       " 'each_personal_essay_865.json',\n",
       " 'each_personal_essay_921.json',\n",
       " 'each_personal_essay_188.json',\n",
       " 'each_personal_essay_472.json',\n",
       " 'each_personal_essay_167.json',\n",
       " 'each_personal_essay_537.json',\n",
       " 'each_personal_essay_864.json',\n",
       " 'each_personal_essay_219.json',\n",
       " 'each_personal_essay_649.json',\n",
       " 'each_personal_essay_425.json',\n",
       " 'each_personal_essay_976.json',\n",
       " 'each_personal_essay_833.json',\n",
       " 'each_personal_essay_560.json',\n",
       " 'each_personal_essay_130.json',\n",
       " 'each_personal_essay_960.json',\n",
       " 'each_personal_essay_599.json',\n",
       " 'each_personal_essay_433.json',\n",
       " 'each_personal_essay_126.json',\n",
       " 'each_personal_essay_576.json',\n",
       " 'each_personal_essay_825.json',\n",
       " 'each_personal_essay_258.json',\n",
       " 'each_personal_essay_608.json',\n",
       " 'each_personal_essay_49.json',\n",
       " 'each_personal_essay_464.json',\n",
       " 'each_personal_essay_937.json',\n",
       " 'each_personal_essay_872.json',\n",
       " 'each_personal_essay_521.json',\n",
       " 'each_personal_essay_171.json',\n",
       " 'each_personal_essay_366.json',\n",
       " 'each_personal_essay_736.json',\n",
       " 'each_personal_essay_32.json',\n",
       " 'each_personal_essay_223.json',\n",
       " 'each_personal_essay_389.json',\n",
       " 'each_personal_essay_9.json',\n",
       " 'each_personal_essay_673.json',\n",
       " 'each_personal_essay_809.json',\n",
       " 'each_personal_essay_65.json',\n",
       " 'each_personal_essay_761.json',\n",
       " 'each_personal_essay_331.json',\n",
       " 'each_personal_essay_624.json',\n",
       " 'each_personal_essay_274.json',\n",
       " 'each_personal_essay_448.json',\n",
       " 'each_personal_essay_327.json',\n",
       " 'each_personal_essay_777.json',\n",
       " 'each_personal_essay_73.json',\n",
       " 'each_personal_essay_798.json',\n",
       " 'each_personal_essay_262.json',\n",
       " 'each_personal_essay_632.json',\n",
       " 'each_personal_essay_848.json',\n",
       " 'each_personal_essay_24.json',\n",
       " 'each_personal_essay_720.json',\n",
       " 'each_personal_essay_370.json',\n",
       " 'each_personal_essay_665.json',\n",
       " 'each_personal_essay_235.json',\n",
       " 'each_personal_essay_409.json',\n",
       " 'each_personal_essay_184.json',\n",
       " 'each_personal_essay_887.json',\n",
       " 'each_personal_essay_868.json',\n",
       " 'each_personal_essay_491.json',\n",
       " 'each_personal_essay_242.json',\n",
       " 'each_personal_essay_612.json',\n",
       " 'each_personal_essay_307.json',\n",
       " 'each_personal_essay_757.json',\n",
       " 'each_personal_essay_53.json',\n",
       " 'each_personal_essay_583.json',\n",
       " 'each_personal_essay_429.json',\n",
       " 'each_personal_essay_645.json',\n",
       " 'each_personal_essay_215.json',\n",
       " 'each_personal_essay_700.json',\n",
       " 'each_personal_essay_350.json',\n",
       " 'each_personal_essay_595.json',\n",
       " 'each_personal_essay_829.json',\n",
       " 'each_personal_essay_203.json',\n",
       " 'each_personal_essay_653.json',\n",
       " 'each_personal_essay_346.json',\n",
       " 'each_personal_essay_716.json',\n",
       " 'each_personal_essay_12.json',\n",
       " 'each_personal_essay_891.json',\n",
       " 'each_personal_essay_192.json',\n",
       " 'each_personal_essay_468.json',\n",
       " 'each_personal_essay_487.json',\n",
       " 'each_personal_essay_604.json',\n",
       " 'each_personal_essay_254.json',\n",
       " 'each_personal_essay_45.json',\n",
       " 'each_personal_essay_741.json',\n",
       " 'each_personal_essay_311.json',\n",
       " 'each_personal_essay_106.json',\n",
       " 'each_personal_essay_556.json',\n",
       " 'each_personal_essay_805.json',\n",
       " 'each_personal_essay_940.json',\n",
       " 'each_personal_essay_413.json',\n",
       " 'each_personal_essay_690.json',\n",
       " 'each_personal_essay_5.json',\n",
       " 'each_personal_essay_385.json',\n",
       " 'each_personal_essay_852.json',\n",
       " 'each_personal_essay_501.json',\n",
       " 'each_personal_essay_151.json',\n",
       " 'each_personal_essay_444.json',\n",
       " 'each_personal_essay_917.json',\n",
       " 'each_personal_essay_297.json',\n",
       " 'each_personal_essay_69.json',\n",
       " 'each_personal_essay_86.json',\n",
       " 'each_personal_essay_782.json',\n",
       " 'each_personal_essay_278.json',\n",
       " 'each_personal_essay_628.json',\n",
       " 'each_personal_essay_147.json',\n",
       " 'each_personal_essay_517.json',\n",
       " 'each_personal_essay_844.json',\n",
       " 'each_personal_essay_901.json',\n",
       " 'each_personal_essay_452.json',\n",
       " 'each_personal_essay_281.json',\n",
       " 'each_personal_essay_794.json',\n",
       " 'each_personal_essay_90.json',\n",
       " 'each_personal_essay_813.json',\n",
       " 'each_personal_essay_540.json',\n",
       " 'each_personal_essay_110.json',\n",
       " 'each_personal_essay_405.json',\n",
       " 'each_personal_essay_956.json',\n",
       " 'each_personal_essay_686.json',\n",
       " 'each_personal_essay_28.json',\n",
       " 'each_personal_essay_239.json',\n",
       " 'each_personal_essay_393.json',\n",
       " 'each_personal_essay_669.json']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Users/kimkwangil/Documents/008_AI/Story_Analysis-master 8/CHARACTER_/ps_data/\"\n",
    "file_list = os.listdir(path)\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/981 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loadling~\n",
      "pred_loader: <torch.utils.data.dataloader.DataLoader object at 0x7fae09af3490>\n",
      "check!\n",
      "text: (\"In our Singaporean version of a Toyota minivan, we passed it almost daily.As a whole, my childhood was relatively sheltered, and the biggest problems I faced were losing my goggles or failing to remember how to brake on rollerskates- but that doesn't mean that I don't care, that I don't want to learn.\",)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred : tensor([1])\n",
      "text: (\"Few questions asked, and definitely none answered, land was considered a precious resource and no amount of history or character could prevent the destruction.Prompt: Person in history you would want to meetThe grotesque nature of Chicago's massive slaughterhouse and poverty-stricken, soot-infested neighborhoods earned it the nickname of 'The Black City' in the late 1800s, but despite the striking lack of sanitation, upon reading Erik Larson's The Devil in the White City, I would love the opportunity to see the dichotomy between the city's prosperous World's Fair in 1893 and the desolate alleys that coined the phrase.\",)\n",
      "pred : tensor([1])\n",
      "text: ('I still have a list of possible ideas and introductions saved on my computer, and trust me, most would make for some fun reading material.',)\n",
      "pred : tensor([1])\n",
      "text: ('I might not have the resources or education to face these global problems right now, but as clich\\\\u00e9 as it sounds, I like to think that my future opportunities in both service and business are endless, and I believe that the University of Pennsylvania can offer me the support and diverse education I crave to accomplish these goals.',)\n",
      "pred : tensor([1])\n",
      "text: ('Only when I pushed aside prior notions could I acknowledge the sheer beauty disguised by a lifetime of materialism.',)\n",
      "pred : tensor([1])\n",
      "text: (\"Although I'm sure I will change my mind more than once after I'm exposed to the myriad of offered programs, I'm currently interested in the Consult for America organization and the Penn Environmental Group.\",)\n",
      "pred : tensor([1])\n",
      "text: ('Just like that.',)\n",
      "pred : tensor([0])\n",
      "text: ('Halloween specials on Disney and the intermittent Goosebumps books I read idealized the dead as zombie-like creatures or portrayed cemeteries as haunted graveyards crawling with ill-tempered spirits.',)\n",
      "pred : tensor([1])\n",
      "text: ('Instead they will be buried, alongside asphalt covering the trails engrained with stories of the past and grooved by the footfalls of thousands.It all started in fifth grade when my mother sat me down in our basement with a bowl of kettle corn and turned on An Inconvenient Truth.',)\n",
      "pred : tensor([1])\n",
      "text: ('In combination with my concentration on Environmental Policy and Management, I know that these programs will help unite my passion for the environment, supporting the local community, and help me receive a well-rounded business education.',)\n",
      "pred : tensor([1])\n",
      "text: (\"What matters isthat I have been given a chance- a chance to succeed, to get an education, a chanceto make a difference in someone else's life, and I need to take it.After weeks of gluing myself to the squirmy backs of snorting beasts, I realized I was in more danger of being trampled by ornery horses than mystical souls from gravesites.\",)\n",
      "pred : tensor([1])\n",
      "text: (\"Don't just list off your years of extracurriculars or accomplishments, because it is really the only chance for admissions officers to get a glimpse of your individuality.Bukit Brown is a few years from obliteration.\",)\n",
      "pred : tensor([1])\n",
      "text: ('On multiple occasions, snapping twigs or the thrilled squeals of pedestrians transformed our group of horses into a galloping, bucking, and rearing charade, and the next five minutes would be spent clutching the saddle with all of the strength my body could muster.',)\n",
      "pred : tensor([1])\n",
      "text: (\"But, over the months it took me to complete my applications, I learned quite a few things!Although the amount of writing required can be daunting, it is important to give yourself enough time to complete each application thoroughly, and remember to let your inner voice and creativity shine through in your essays.But the clatter of metal horseshoes on worn pavement wasn't the only traffic through the picturesque cemetery.\",)\n",
      "pred : tensor([1])\n",
      "text: ('I want to make a difference, no matter how small.',)\n",
      "pred : tensor([0])\n",
      "text: ('Too many people go through life ignorant of problems they can help fight, but ignorance is not bliss.',)\n",
      "pred : tensor([0])\n",
      "text: ('Between multiple personal statements, supplements, and scholarship applications, I wrote over 20 essays on a wide variety of topics.',)\n",
      "pred : tensor([1])\n",
      "text: ('As much as I hate sharing my writing, I can only proofread my own work so much, and it is invaluable to get the advice of teachers/parents/friends.',)\n",
      "pred : tensor([1])\n",
      "text: (\"I was deeply saddened when I realized the majority of the Exposition's beauty and masterpieces had been dismantled or destroyed, but I figure the next best thing would be to converse with its Head of Works and lead architect, Daniel Burnham.\",)\n",
      "pred : tensor([1])\n",
      "text: ('Searching for Sugar Man, check.I grew up fearing the occasional spider in my basement and most definitely strangers, but deeper down was a more intrinsic fear\\\\u2014 the unknown, death.',)\n",
      "pred : tensor([1])\n",
      "text: ('His perseverance throughout the ordeal is beyond incredible, but what I am dying to ask is how and where he looked for beauty in a time when the economy was in shambles and cities were overcome with the stench of industrialization and overpopulation.',)\n",
      "pred : tensor([1])\n",
      "text: (\"Although the 'explosions' were a little less thrilling than those in perhaps Pirates of the Caribbean, I was enthralled and have spent many a Friday night since with the remote in one hand and my phone in the other- ready to research any topic I found engaging.\",)\n",
      "pred : tensor([1])\n",
      "text: ('The footsteps of naturalists, artists, dog-walkers, and even the elderly caretakers whose dilapidated huts lay hidden among the undergrowth were living reminders\\\\u2014 we were the silent protestors that the futuristic skyscrapers and engineering feats a few kilometers down the road would never compensate for years of vibrant cultural customs.I also cannot stress enough the importance of feedback.',)\n",
      "pred : tensor([1])\n",
      "text: ('What matters is whether I keep that promise.',)\n",
      "pred : tensor([0])\n",
      "text: ('In a world lacking human empathy and stripped of emotion, a guise of perfection saturates the beginning pages of the novel, but as I quickly learned, the dystopian society Lowry creates represent the flaws in our own.',)\n",
      "pred : tensor([1])\n",
      "text: (\"Not only did a mud-splattered, stark white tarp by the side of a highway seem like such an insensitive way to notify families, but on a crowded island my parents often nicknamed the 'concrete jungle,' that cemetery was an oasis\\\\u2014 and the government planned to build a road precisely on top of it.\",)\n",
      "pred : tensor([1])\n",
      "text: (\"I'm especially excited to pursue the consultation initiative because I have longed for the days when teaching escapes the four concrete walls of traditional classrooms.\",)\n"
     ]
    }
   ],
   "source": [
    "essay_results = []\n",
    "\n",
    "for file_name_raw in tqdm(file_list): # 파일이름을 하나씩 불러와서\n",
    "    file_name = path  + file_name_raw #경로를 추가하고\n",
    "    with open(file_name, 'r') as json_file : #JSON파일로 파일을 열어서\n",
    "        ps_input_each_data = json.load(json_file) # JSON파일로 로드한다.\n",
    "        input_data = json.dumps(ps_input_each_data)\n",
    "        re_each_ess = character_all_section(input_data)\n",
    "        essay_results.append(re_each_ess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Character Descriptiveness :  33.0\n",
    "# ===============================================================\n",
    "# ['their', 'they', 'me', 'you', 'her', 'it', 'myself', 'i', 'my', 'them']\n",
    "# ai_character_section.py:92: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n",
    "#   ext_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "# =============================================\n",
    "# Number of Characters : 92\n",
    "# =============================================\n",
    "# ['i', 'my', 'me']\n",
    "# ai_character_section.py:338: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n",
    "#   ext_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "# =============================================\n",
    "# Emphasis on You : 60\n",
    "# =============================================\n",
    "# ['their', 'they', 'you', 'her', 'it', 'myself', 'them']\n",
    "# ai_character_section.py:409: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n",
    "#   ext_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "# =============================================\n",
    "# Emphasis on Others : 32\n",
    "# =============================================\n",
    "# (33.0, 92, 60, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r = pd.DataFrame(essay_results, columns =['char_desc', 'num_chars', 'emph_you', 'emph_others'])  #e데이터프레임에 넣기\n",
    "df_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1000명 데이터의 각 값의 평균 계산\n",
    "char_desc_mean = df_r['char_desc'].mean()\n",
    "char_desc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chars_mean = df_r['num_chars'].mean()\n",
    "num_chars_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emph_you_mean = df_r['emph_you'].mean()\n",
    "emph_you_mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emph_others_mean = df_r['emph_others'].mean()\n",
    "emph_others_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30명의 최종 평균값 계산\n",
    "char_re_30 = [char_desc_mean, num_chars_mean, emph_you_mean, emph_others_mean]\n",
    "char_re_30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1명의 입력데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Bloomington Normal is almost laughably cliché for a midwestern city. Vast swathes of corn envelop winding roads and the heady smell of BBQ smoke pervades the countryside every summer. Yet, underlying the trite norms of Normal is the prescriptive force of tradition—the expectation to fulfill my role as a female Filipino by playing Debussy in the yearly piano festival and enrolling in multivariable calculus instead of political philosophy.So when I discovered the technical demand of bebop, the triplet groove, and the intricacies of chordal harmony after ten years of grueling classical piano, I was fascinated by the music's novelty. Jazz guitar was not only evocative and creative, but also strangely liberating. I began to explore different pedagogical methods, transcribe solos from the greats, and experiment with various approaches until my own unique sound began to develop. And, although I did not know what would be the 'best' route for me to follow as a musician, the freedom to forge whatever path I felt was right seemed to be exactly what I needed; there were no expectations for me to continue in any particular way—only the way that suited my own desires.While journeying this trail, I found myself at Interlochen Arts Camp the summer before my junior year. Never before had I been immersed in an environment so conducive to musical growth: I was surrounded by people intensely passionate about pursuing all kinds of art with no regard for ideas of what art 'should' be. I knew immediately that this would be a perfect opportunity to cultivate my sound, unbounded by the limits of confining tradition. On the first day of camp, I found that my peer guitarist in big band was another Filipino girl from Illinois. Until that moment, my endeavors in jazz guitar had been a solitary effort; I had no one with whom to collaborate and no one against whom I could compare myself, much less someone from a background mirroring my own. I was eager to play with her, but while I quickly recognized a slew of differences between us—different heights, guitars, and even playing styles—others seemed to have trouble making that distinction during performances. Some even went as far as calling me 'other-Francesca.' Thus, amidst the glittering lakes and musky pine needles of Interlochen, I once again confronted Bloomington's frustrating expectations.After being mistaken for her several times, I could not help but view Francesca as a standard of what the 'female Filipino jazz guitarist' should embody. Her improvisatory language, comping style and even personal qualities loomed above me as something I had to live up to. Nevertheless, as Francesca and I continued to play together, it was not long before we connected through our creative pursuit. In time, I learned to draw inspiration from her instead of feeling pressured to follow whatever precedent I thought she set. I found that I grew because of, rather than in spite of, her presence; I could find solace in our similarities and even a sense of comfort in an unfamiliar environment without being trapped by expectation. Though the pressure to conform was still present—and will likely remain present in my life no matter what genre I'm playing or what pursuits I engage in—I learned to eschew its corrosive influence and enjoy the rewards that it brings. While my encounter with Francesca at first sparked a feeling of pressure to conform in a setting where I never thought I would feel its presence, it also carried the warmth of finding someone with whom I could connect. Like the admittedly trite conditions of my hometown, the resemblances between us provided comfort to me through their familiarity. I ultimately found that I can embrace this warmth while still rejecting the pressure to succumb to expectations, and that, in the careful balance between these elements, I can grow in a way that feels both like discove\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_char_re = character_all_section(input_text)\n",
    "ps_char_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ps_char_re )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_ps_char_desc = ps_char_re[0] #튜플에서 첫번재 인댁스 값 가져오기 : Character Descriptiveness\n",
    "\n",
    "# 결과도출 1명의 Character Descriptiveness 계산결과\n",
    "one_ps_char_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_mean = char_desc_mean #1000명 데이터의 각 값의 평균 값 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ = int(ideal_mean-ideal_mean*0.6)\n",
    "print('min_', min_)\n",
    "max_ = int(ideal_mean+ideal_mean*0.6)\n",
    "print('max_: ', max_)\n",
    "div_ = int(((ideal_mean+ideal_mean*0.6)-(ideal_mean-ideal_mean*0.6))/3)\n",
    "print('div_:', div_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_abs = abs(ideal_mean - one_ps_char_desc) # 개인 - 단체 값의 절대값계산\n",
    "\n",
    "print('cal_abs 절대값 :', cal_abs)\n",
    "compare = (one_ps_char_desc + ideal_mean)/7\n",
    "print('compare :', compare)\n",
    "\n",
    "if one_ps_char_desc > ideal_mean: # 개인점수가 평균보다 클 경우는 overboard\n",
    "    if cal_abs > compare: # 개인점수가 개인평균차의 절대값보다 클 경우, 즉 차이가 많이 날경우\n",
    "        print(\"Overboard\")\n",
    "    else: #차이가 많이 안나면\n",
    "        print(\"Ideal\")\n",
    "        \n",
    "    \n",
    "elif one_ps_char_desc < ideal_mean: # 개인점수가 평균보다 작을 경우 lacking\n",
    "    if cal_abs > compare: #차이가 많이나면 # 개인점수가  평균보다 작을 경우 Lacking이고 \n",
    "        print(\"Lacking\")\n",
    "    else: #차이가 많이 안나면\n",
    "        print (\"Ideal\")\n",
    "        \n",
    "else:\n",
    "    print(\"Ideal\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1000명 에세이 데이터 처리 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save\n",
    "with open('1000_essay_results.pickle', 'wb') as f:\n",
    "    pickle.dump(essay_results, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "with open('data_ai_theme.pickle', 'rb') as f:\n",
    "    result_most_simWords = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
