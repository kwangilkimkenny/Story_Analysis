{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kimkwangil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/kimkwangil/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#conflict\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "from mpld3 import plugins, fig_to_html, save_html, fig_to_dict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "#character, setting\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import multiprocessing\n",
    "import os\n",
    "from pathlib import Path\n",
    "import io\n",
    "from gensim.models import Phrases\n",
    "from textblob import TextBlob\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#human을 의미하는 단어가 문장 전체에 포함 비율\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class ContainsBio(Enum):\n",
    "    YES = 1\n",
    "    NO = 0\n",
    "    MAYBE = 2\n",
    "\n",
    "\n",
    "def human_contains_pronouns(bio):\n",
    "    perfect_matches = ['he/him', 'she/her', 'they/them', 'ze', 'hir']\n",
    "    probably_matches = ['they', 'he', 'her', 'him', 'her', 'pronouns']\n",
    "\n",
    "    words = re.findall(r\"[\\w'/]+\", bio.lower())\n",
    "    for match in perfect_matches:\n",
    "        if match in words:\n",
    "            return ContainsBio.YES\n",
    "\n",
    "    for match in probably_matches:\n",
    "        if match in words:\n",
    "            return ContainsBio.MAYBE\n",
    "    return ContainsBio.NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text= \"\"\"A window into the soul.For most people, this would be the eyes. The eyes cannot lie; they often tell more about a person's emotions than their words. What distinguishes a fake smile from a genuine one? The eyes. What shows sadness? The eyes. What gives away a liar? The eyes.But are the eyes the only window into the soul?Recently, I began painting with watercolors. With watercolors, there is no turning back: if one section is too dark, it is nearly impossible to lighten the area again. Every stroke must be done purposefully, every color mixed to its exact value.I laid my materials before me, preparing myself for the worst. I checked my list of supplies, making sure my setup was perfect.I wet my brush, dipped it into some yellow ochre, and dabbed off the excess paint. Too little water on my brush. I dipped my brush back into my trusty water jar; the colors swirled beautifully, forming an abstract art piece before my eyes. \\u2014It's a shame that I couldn't appreciate it.I continued mixing colors to their exact value. More alizarin crimson. More water. More yellow ochre. Less water. More phthalo blue. The cycle continued. Eventually, I was satisfied. The colors looked good, there was enough contrast between facial features, and the watercolors stayed inside the lines.Craving feedback, I posted my art to Snapchat. I got a few messages such as 'wow' and 'pretty,' but one message stood out. 'You were anxious with this one, huh? Anyways, love the hair!'I was caught off guard. Was it a lucky guess? Did they know something I didn't? I immediately responded: 'Haha, how could you tell?' No response.What I didn't know at the time was that my response would come a few months later while babysitting. Since the girl I was babysitting loved art, I took out some Crayola watercolors and some watercolor paper for her to play with. After I went to the bathroom and came back, the watercolors were doused with water. 'You were impatient with this one, huh? Anyways, love the little dog you drew!'The little girl looked up at me, confused. 'How could you tell?' 'You used a lot of water for a brighter color, but you couldn't wait for it to slowly soak in.''Oh.'Now, I would be lying if I said I realized the connection between the two events immediately.Instead, I made the connection when I decided to sit down one day and objectively critique my art. The piece that I once loved now seemed like a nervous wreck: the paper was overworked, the brushstrokes were undecided, the facial features blended together, and each drop of water was bound inside the lines as if it was a prisoner in a cage.From then on, I started noticing pieces of personality in additional creations surrounding me: website designs, solutions to math problems, code written for class, and even the preparation of a meal.When I peer around at people's projects during Code Club, I notice the clear differences between their code. Some people break it up by commenting in every possible section. Others breeze through the project, not caring to comment or organize their code. I could also see clear differences in personalities when our club members began coding the Arduino for the first time. Some followed the tutorials to the letter, while others immediately started experimenting with different colored LEDs and ways of wiring the circuit.It became clear to me that, as humans, we leave pieces of our souls in everything we do, more than we intend to. If we entertain this thought, perhaps the key to better understanding others around us is simply noticing the subtler clues under our noses?Perhaps there are endless windows to the soul, and we simply need to peer through them. I shakily rose my hand. 'We should create workshops of our own,' I suggested.I got a few strange looks. 'It's a good idea, but it's too much work.' 'We just don't have enough free time to make it work.' 'Maybe we could, but I don't know how to make workshops.' My suggestion was shot down. I shuffled in my seat. 'I could make them.' A few people stared at me in disbelief. I glanced over at the club advisor, Mr. C, nervous to hear his response.'If you're willing to take on the work, we can try it.' Mr. C replied. And so I embarked on my quest. I researched different workshops on the internet, learning the information myself at first. Then, I transitioned into creating workshops of my own, making sure that the information was easy to understand for even a beginner. I was exhausted; my first workshop took 16 cumulative hours to create.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ContainsBio.MAYBE: 2>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humnan_pronoun = human_contains_pronouns(input_text)\n",
    "humnan_pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def academic_words_ratio(text):\n",
    "\n",
    "    essay_input_corpus = str(text) #문장입력\n",
    "    essay_input_corpus = essay_input_corpus.lower()#소문자 변환\n",
    "\n",
    "    sentences  = sent_tokenize(essay_input_corpus) #문장 토큰화\n",
    "    total_sentences = len(sentences)#토큰으로 처리된 총 문장 수\n",
    "    total_words = len(word_tokenize(essay_input_corpus))# 총 단어수\n",
    "    \n",
    "    split_sentences = []\n",
    "    for sentence in sentences:\n",
    "        processed = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "        words = processed.split()\n",
    "        split_sentences.append(words)\n",
    "\n",
    "    skip_gram = 1\n",
    "    workers = multiprocessing.cpu_count()\n",
    "    bigram_transformer = Phrases(split_sentences)\n",
    "\n",
    "    model = gensim.models.word2vec.Word2Vec(bigram_transformer[split_sentences], workers=workers, sg=skip_gram, min_count=1)\n",
    "\n",
    "    model.train(split_sentences, total_examples=sum([len(sentence) for sentence in sentences]), epochs=100)\n",
    "    \n",
    "    #모델 설계 완료\n",
    "\n",
    "    #OVERLAP 단어들을 리스트에 넣어서 필터로 만들고\n",
    "    character_list = ['Arts','abandon','abstract','academy','access',  'accommodate','accompany','accumulate',\n",
    "                      'accurate','achieve',  'acknowledge','acquire','adapt','adequate','adjacent','adjust',\n",
    "                      'administration',  'adult','advocate','affect','aggregate','aid','albeit','allocate','alter',\n",
    "                      'alternative','ambiguous','amend','analogy','analyse','annual','anticipate','apparent','append',\n",
    "                      'appreciate',  'approach','appropriate','approximate','arbitrary','area','aspect','assemble',\n",
    "                      'assess','assign','assist','assume','assure','attach','attain','attitude',  'attribute',\n",
    "                      'author','authority','automate','available','aware','behalf','benefit','bias','bond','brief',\n",
    "                      'bulk','capable','capacity','category','cease','challenge','channel','chapter',  'chart',\n",
    "                      'chemical','circumstance','cite','civil','clarify','classic','clause','code','coherent',\n",
    "                      'coincide','collapse','colleague','commence','comment','commission','commit','commodity',\n",
    "                      'communicate','community','compatible','compensate','compile','complement',  'complex',\n",
    "                      'component','compound','comprehensive','comprise','compute','conceive',  'concentrate',\n",
    "                      'concept','conclude','concurrent','conduct','confer','confine',  'confirm',  'conflict',\n",
    "                      'conform','consent','consequent','considerable','consist','constant','constitute','constrain',\n",
    "                      'construct','consult','consume','contact','contemporary','context','contract','contradict',\n",
    "                      'contrary',  'contrast',  'contribute',  'controversy',  'convene',  'converse',  'convert',  \n",
    "                      'convince',  'cooperate',  'coordinate',  'core',  'corporate',  'correspond',  'couple',  \n",
    "                      'create',  'credit',  'criteria',  'crucial',  'culture',  'currency',  'cycle',  'data',  \n",
    "                      'debate',  'decade',  'decline',  'deduce',  'define',  'definite',  'demonstrate',  'denote', \n",
    "                      'deny',  'depress',  'derive',  'design',  'despite',  'detect',  'deviate',  'device',  \n",
    "                      'devote',  'differentiate',  'dimension',  'diminish',  'discrete',  'discriminate','displace',\n",
    "                      'display',  'dispose',  'distinct',  'distort',  'distribute',  'diverse',  'document',  \n",
    "                      'domain',  'domestic',  'dominate',  'draft',  'drama',  'duration',  'dynamic',  'economy',\n",
    "                      'edit',  'element',  'eliminate',  'emerge',  'emphasis',  'empirical',  'enable',  'encounter',\n",
    "                      'energy',  'enforce',  'enhance',  'enormous',  'ensure',  'entity',  'environment',  'equate',\n",
    "                      'equip',  'equivalent',  'erode',  'error',  'establish',  'estate',  'estimate',  'ethic',\n",
    "                      'ethnic',  'evaluate',  'eventual',  'evident',  'evolve',  'exceed',  'exclude',  'exhibit',  \n",
    "                      'expand',  'expert',  'explicit',  'exploit',  'export',  'expose',  'external',  'extract',  \n",
    "                      'facilitate',  'factor',  'feature',  'federal',  'fee',  'file',  'final',  'finance',  \n",
    "                      'finite',  'flexible',  'fluctuate',  'focus',  'format',  'formula',  'forthcoming',  \n",
    "                      'foundation',  'found',  'framework',  'function',  'fund',  'fundamental',  'furthermore',  \n",
    "                      'gender',  'generate',  'generation',  'globe',  'goal',  'grade',  'grant',  'guarantee',  \n",
    "                      'guideline',  'hence',  'hierarchy',  'highlight',  'hypothesis',  'identical',  'identify',  \n",
    "                      'ideology',  'ignorance',  'illustrate',  'image',  'immigrate',  'impact',  'implement',  \n",
    "                      'implicate',  'implicit',  'imply',  'impose',  'incentive',  'incidence',  'incline',  \n",
    "                      'income',  'incorporate',  'index',  'indicate',  'individual',  'induce',  'inevitable', \n",
    "                      'infer',  'infrastructure',  'inherent',  'inhibit',  'initial',  'initiate',  'injure',  \n",
    "                      'innovate',  'input',  'insert',  'insight',  'inspect',  'instance',  'institute',  'instruct',\n",
    "                      'integral',  'integrate',  'integrity',  'intelligence',  'intense',  'interact',  'intermediate',\n",
    "                      'internal',  'interpret',  'interval',  'intervene',  'intrinsic',  'invest',  'investigate',\n",
    "                      'invoke',  'involve',  'isolate',  'issue',  'item',  'job',  'journal',  'justify',  'label',\n",
    "                      'labour',  'layer',  'lecture',  'legal',  'legislate',  'levy',  'liberal',  'licence',\n",
    "                      'likewise',  'link',  'locate',  'logic',  'maintain',  'major',  'manipulate',  'manual',\n",
    "                      'margin',  'mature',  'maximise',  'mechanism',  'media',  'mediate',  'medical',  'medium',\n",
    "                      'mental',  'method',  'migrate',  'military',  'minimal',  'minimise',  'minimum',  'ministry',\n",
    "                      'minor',  'mode',  'modify',  'monitor',  'motive',  'mutual',  'negate',  'network',  'neutral',\n",
    "                      'nevertheless',  'nonetheless',  'norm',  'normal',  'notion',  'notwithstanding',  'nuclear',\n",
    "                      'objective',  'obtain',  'obvious',  'occupy',  'occur',  'odd',  'offset',  'ongoing',  'option',\n",
    "                      'orient',  'outcome',  'output',  'overall',  'overlap',  'overseas',  'panel',  'paradigm',\n",
    "                      'paragraph',  'parallel',  'parameter',  'participate',  'partner',  'passive',  'perceive',\n",
    "                      'percent',  'period',  'persist',  'perspective',  'phase',  'phenomenon',  'philosophy',  'physical',\n",
    "                      'plus',  'policy',  'portion',  'pose',  'positive',  'potential',  'practitioner',  'precede',\n",
    "                      'precise',  'predict',  'predominant',  'preliminary',  'presume',  'previous',  'primary',\n",
    "                      'prime',  'principal',  'principle',  'prior',  'priority',  'proceed',  'process',  'professional',\n",
    "                      'prohibit',  'project',  'promote',  'proportion',  'prospect',  'protocol',  'psychology',\n",
    "                      'publication',  'publish',  'purchase',  'pursue',  'qualitative',  'quote',  'radical',  'random',\n",
    "                      'range',  'ratio',  'rational',  'react',  'recover',  'refine',  'regime',  'region',  'register',\n",
    "                      'regulate',  'reinforce',  'reject',  'relax',  'release',  'relevant',  'reluctance',  'rely',\n",
    "                      'remove',  'require',  'research',  'reside',  'resolve',  'resource',  'respond',  'restore',\n",
    "                      'restrain',  'restrict',  'retain',  'reveal',  'revenue',  'reverse',  'revise',  'revolution',\n",
    "                      'rigid',  'role',  'route',  'scenario',  'schedule',  'scheme',  'scope',  'section',  'sector',\n",
    "                      'secure',  'seek',  'select',  'sequence',  'series',  'sex',  'shift',  'significant',  'similar',\n",
    "                      'simulate',  'site',  'so-called',  'sole',  'somewhat',  'source',  'specific',  'specify',  'sphere',\n",
    "                      'stable',  'statistic',  'status',  'straightforward',  'strategy',  'stress',  'structure',  'style',\n",
    "                      'submit',  'subordinate',  'subsequent',  'subsidy',  'substitute',  'successor',  'sufficient',  'sum',\n",
    "                      'summary',  'supplement',  'survey',  'survive',  'suspend',  'sustain',  'symbol',  'tape',  'target',\n",
    "                      'task',  'team',  'technical',  'technique',  'technology',  'temporary',  'tense',  'terminate',  'text',\n",
    "                      'theme',  'theory',  'thereby',  'thesis',  'topic',  'trace',  'tradition',  'transfer',  'transform',\n",
    "                      'transit',  'transmit',  'transport',  'trend',  'trigger',  'ultimate',  'undergo',  'underlie',  'undertake',\n",
    "                      'uniform',  'unify',  'unique',  'utilise',  'valid',  'vary',  'vehicle',  'version',  'via',  'violate',\n",
    "                      'virtual',  'visible',  'vision',  'visual',  'volume',  'voluntary',  'welfare',  'whereas', 'Education',\n",
    "                      'History', 'Linguistics', 'Philosophy', 'Politics', 'Psychology', 'Sociology', 'Commerce', 'Accounting',\n",
    "                      'Economics', 'Finance', 'Industrial Relations', 'Management', 'Marketing', 'Public Policy', 'Law',\n",
    "                      'Constitutional Law', 'Criminal Law', 'Family Law and Medico-Legal', 'International Law', 'Pure Commercial Law',\n",
    "                      'Quasi-Commercial Law', 'Rights and Remedies', 'Biology', 'Chemistry', 'Computer Science', 'Geography',\n",
    "                      'Geology', 'Mathematics', 'Physics'\n",
    "                     ]\n",
    "    \n",
    "    ####문장에 char_list의 단어들이 있는지 확인하고, 있다면 유사단어를 추출한다.\n",
    "    #우선 토큰화한다.\n",
    "    retokenize = RegexpTokenizer(\"[\\w]+\") #줄바꿈 제거하여 한줄로 만들고\n",
    "    token_input_text = retokenize.tokenize(essay_input_corpus)\n",
    "    #print (token_input_text) #토큰화 처리 확인.. 토큰들이 리스트에 담김\n",
    "    #리트스로 정리된 개별 토큰을 char_list와 비교해서 존재하는 것만 추출한다.\n",
    "    filtered_chr_text = []\n",
    "    for k in token_input_text:\n",
    "        for j in character_list:\n",
    "            if k == j:\n",
    "                filtered_chr_text.append(j)\n",
    "    \n",
    "    #print (filtered_chr_text) # 유사단어 비교 추출 완료, 겹치는 단어는 제거하자.\n",
    "    \n",
    "    filtered_chr_text_ = set(filtered_chr_text) #중복제거\n",
    "    filtered_chr_text__ = list(filtered_chr_text_) #다시 리스트로 변환\n",
    "    #print (filtered_chr_text__) # 중복값 제거 확인\n",
    "    \n",
    "    for i in filtered_chr_text__:\n",
    "        ext_sim_words_key = model.most_similar_cosmul(i) #모델적용\n",
    "    \n",
    "    char_total_count = len(filtered_chr_text) # 중복이 제거되지 않은 에세이 총 문장에 사용된 캐릭터 표현 수\n",
    "    char_count_ = len(filtered_chr_text__) #중복제거된 캐릭터 표현 총 수\n",
    "        \n",
    "    result_char_ratio = round(char_total_count/total_words * 100, 2)\n",
    "\n",
    "    #return result_char_ratio, total_sentences, total_words, char_total_count, char_count_, ext_sim_words_key\n",
    "    return result_char_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "academic words ratio: 1.61 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:124: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n"
     ]
    }
   ],
   "source": [
    "aca_ratio = academic_words_ratio(input_text)\n",
    "print (\"academic words ratio:\", aca_ratio, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 문자 전처리 코드...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"abandon 8 abstract 6 academy 5 access 4 accommodate 9 accompany 8 accumulate 8 accurate 6 achieve 2 acknowledge 6 acquire 2 adapt 7 adequate 4 adjacent 10 adjust 5 administration 2 adult 7 advocate 7 affect 2 aggregate 6 aid 7 albeit 10 allocate 6 alter 5 alternative 3 ambiguous 8 amend 5 analogy 9 analyse 1 annual 4 anticipate 9 apparent 4 append 8 appreciate 8 approach 1 appropriate 2 approximate 4 arbitrary 8 area 1 aspect 2 assemble 10 assess 1 assign 6 assist 2 assume 1 assure 9 attach 6 attain 9 attitude 4 attribute 4 author 6 authority 1 automate 8 available 1 aware 5 behalf 9 benefit 1 bias 8 bond 6 brief 6 bulk 9 capable 6 capacity 5 category 2 cease 9 challenge 5 channel 7 chapter 2 chart 8 chemical 7 circumstance 3 cite 6 civil 4 clarify 8 classic 7 clause 5 code 4 coherent 9 coincide 9 collapse 10 colleague 10 commence 9 comment 3 commission 2 commit 4 commodity 8 communicate 4 community 2 compatible 9 compensate 3 compile 10 complement 8 complex 2 component 3 compound 5 comprehensive 7 comprise 7 compute 2 conceive 10 concentrate 4 concept 1 conclude 2 concurrent 9 conduct 2 confer 4 confine 9 confirm 7 conflict 5 conform 8 consent 3 consequent 2 considerable 3 consist 1 constant 3 constitute 1 constrain 3 construct 2 consult 5 consume 2 contact 5 contemporary 8 context 1 contract 1 contradict 8 contrary 7 contrast 4 contribute 3 controversy 9 convene 3 converse 9 convert 7 convince 10 cooperate 6 coordinate 3 core 3 corporate 3 correspond3 couple 7 create 1 credit 2 criteria 3 crucial 8 culture 2 currency 8 cycle 4 data 1 debate 4 decade 7 decline 5 deduce 3 define 1 definite 7 demonstrate 3 denote 8 deny 7 depress 10 derive 1 design 2 despite 4 detect 8 deviate 8 device 9 devote 9 differentiate 7 dimension 4 diminish 9 discrete 5 discriminate 6 displace 8 display 6 dispose 7 distinct 2 distort 9 distribute 1 diverse 6 document 3 domain 6 domestic 4 dominate 3 draft 5 drama 8 duration 9 dynamic 7 economy 1 edit 6 element 2 eliminate 7 emerge 4 emphasis 3 empirical 7 enable 5 encounter 10 energy 5 enforce 5 enhance 6 enormous 10 ensure 3 entity 5 environment 1 equate 2 equip 7 equivalent 5 erode 9 error 4 establish 1 estate 6 estimate 1 ethic 9 ethnic 4 evaluate 2 eventual 8 evident 1 evolve 5 exceed 6 exclude 3 exhibit 8 expand 5 expert 6 explicit 6 exploit 8 export 1 expose 5 external 5 extract 7 facilitate 5 factor 1 feature 2 federal 6 fee 6 file 7 final 2 finance 1 finite 7 flexible 6 fluctuate 8 focus 2 format 9 formula 1 forthcoming 10 foundation 7 found 9 framework 3 function 1 fund 3 fundamental 5 furthermore 6 gender 6 generate 5 generation 5 globe 7 goal 4 grade 7 grant 4 guarantee 7 guideline 8 hence 4 hierarchy 7 highlight 8 hypothesis 4 identical 7 identify 1 ideology 7 ignorance 6 illustrate 3 image 5 immigrate 3 impact 2 implement 4 implicate 4 implicit 8 imply 3 impose 4 incentive 6 incidence 6 incline 10 income 1 incorporate 6 index 6 indicate 1 individual 1 induce 8 inevitable 8 infer 7 infrastructure 8 inherent 9 inhibit 6 initial 3 initiate 6 injure 2 innovate 7 input 6 insert 7 insight 9 inspect 8 instance 3 institute 2 instruct 6 integral 9 integrate 4 integrity 10 intelligence 6 intense 8 interact 3 intermediate 9 internal 4 interpret 1 interval 6 intervene 7 intrinsic 10 invest 2 investigate 4 invoke 10 involve 1 isolate 7 issue 1 item 2 job 4 journal 2 justify 3 label 4 labour 1 layer 3 lecture 6 legal 1 legislate 1 levy 10 liberal 5 licence 5 likewise 10 link 3 locate 3 logic 5 maintain 2 major 1 manipulate 8 manual 9 margin 5 mature 9 maximise 3 mechanism 4 media 7 mediate 9 medical 5 medium 9 mental 5 method 1 migrate 6 military 9 minimal 9 minimise 8 minimum 6 ministry 6 minor 3 mode 7 modify 5 monitor 5 motive 6 mutual 9 negate 3 network 5 neutral 6 nevertheless 6 nonetheless 10 norm 9 normal 2 notion 5 notwithstanding 10 nuclear 8 objective 5 obtain 2 obvious 4 occupy 4 occur 1 odd 10 offset 8 ongoing 10 option 4 orient 5 outcome 3 output 4 overall 4 overlap 9 overseas 6 panel 10 paradigm 7 paragraph 8 parallel 4 parameter 4 participate 2 partner 3 passive 9 perceive 2 percent 1 period 1 persist 10 perspective 5 phase 4 phenomenon 7 philosophy 3 physical 3 plus 8 policy 1 portion 9 pose 10 positive 2 potential 2 practitioner 8 precede 6 precise 5 predict 4 predominant 8 preliminary 9 presume 6 previous 2 primary 2 prime 5 principal 4 principle 1 prior 4 priority 7 proceed 1 process 1 professional 4 prohibit 7 project 4 promote 4 proportion 3 prospect 8 protocol 9 psychology 5 publication 7 publish 3 purchase 2 pursue 5 qualitative 9 quote 7 radical 8 random 8 range 2 ratio 5 rational 6 react 3 recover 6 refine 9 regime 4 region 2 register 3 regulate 2 reinforce 8 reject 5 relax 9 release 7 relevant 2 reluctance 10 rely 3 remove 3 require 1 research 1 reside 2 resolve 4 resource 2 respond 1 restore 8 restrain 9 restrict 2 retain 4 reveal 6 revenue 5 reverse 7 revise 8 revolution 9 rigid 9 role 1 route 9 scenario 9 schedule 8 scheme 3 scope 6 section 1 sector 1 secure 2 seek 2 select 2 sequence 3 series 4 sex 3 shift 3 significant 1 similar 1 simulate 7 site 2 so-called 10 sole 7 somewhat 7 source 1 specific 1 specify 3 sphere 9 stable 5 statistic 4 status 4 straightforward 10 strategy 2 stress 4 structure 1 style 5 submit 7 subordinate 9 subsequent 4 subsidy 6 substitute 5 successor 7 sufficient 3 sum 4 summary 4 supplement 9 survey 2 survive 7 suspend 9 sustain 5 symbol 5 tape 6 target 5 task 3 team 9 technical 3 technique 3 technology 3 temporary 9 tense 8 terminate 8 text 2 theme 8 theory 1 thereby 8 thesis 7 topic 7 trace 6 tradition 2 transfer 2 transform 6 transit 5 transmit 7 transport 6 trend 5 trigger 9 ultimate 7 undergo 10 underlie 6 undertake 4 uniform 8 unify 9 unique 7 utilise 6 valid 3 vary 1 vehicle 8 version 5 via 8 violate 9 virtual 8 visible 7 vision 9 visual 8 volume 3 voluntary 7 welfare 5 whereas 5 \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abandon',\n",
       " 'abstract',\n",
       " 'academy',\n",
       " 'access',\n",
       " 'accommodate',\n",
       " 'accompany',\n",
       " 'accumulate',\n",
       " 'accurate',\n",
       " 'achieve',\n",
       " 'acknowledge',\n",
       " 'acquire',\n",
       " 'adapt',\n",
       " 'adequate',\n",
       " 'adjacent',\n",
       " 'adjust',\n",
       " 'administration',\n",
       " 'adult',\n",
       " 'advocate',\n",
       " 'affect',\n",
       " 'aggregate',\n",
       " 'aid',\n",
       " 'albeit',\n",
       " 'allocate',\n",
       " 'alter',\n",
       " 'alternative',\n",
       " 'ambiguous',\n",
       " 'amend',\n",
       " 'analogy',\n",
       " 'analyse',\n",
       " 'annual',\n",
       " 'anticipate',\n",
       " 'apparent',\n",
       " 'append',\n",
       " 'appreciate',\n",
       " 'approach',\n",
       " 'appropriate',\n",
       " 'approximate',\n",
       " 'arbitrary',\n",
       " 'area',\n",
       " 'aspect',\n",
       " 'assemble',\n",
       " 'assess',\n",
       " 'assign',\n",
       " 'assist',\n",
       " 'assume',\n",
       " 'assure',\n",
       " 'attach',\n",
       " 'attain',\n",
       " 'attitude',\n",
       " 'attribute',\n",
       " 'author',\n",
       " 'authority',\n",
       " 'automate',\n",
       " 'available',\n",
       " 'aware',\n",
       " 'behalf',\n",
       " 'benefit',\n",
       " 'bias',\n",
       " 'bond',\n",
       " 'brief',\n",
       " 'bulk',\n",
       " 'capable',\n",
       " 'capacity',\n",
       " 'category',\n",
       " 'cease',\n",
       " 'challenge',\n",
       " 'channel',\n",
       " 'chapter',\n",
       " 'chart',\n",
       " 'chemical',\n",
       " 'circumstance',\n",
       " 'cite',\n",
       " 'civil',\n",
       " 'clarify',\n",
       " 'classic',\n",
       " 'clause',\n",
       " 'code',\n",
       " 'coherent',\n",
       " 'coincide',\n",
       " 'collapse',\n",
       " 'colleague',\n",
       " 'commence',\n",
       " 'comment',\n",
       " 'commission',\n",
       " 'commit',\n",
       " 'commodity',\n",
       " 'communicate',\n",
       " 'community',\n",
       " 'compatible',\n",
       " 'compensate',\n",
       " 'compile',\n",
       " 'complement',\n",
       " 'complex',\n",
       " 'component',\n",
       " 'compound',\n",
       " 'comprehensive',\n",
       " 'comprise',\n",
       " 'compute',\n",
       " 'conceive',\n",
       " 'concentrate',\n",
       " 'concept',\n",
       " 'conclude',\n",
       " 'concurrent',\n",
       " 'conduct',\n",
       " 'confer',\n",
       " 'confine',\n",
       " 'confirm',\n",
       " 'conflict',\n",
       " 'conform',\n",
       " 'consent',\n",
       " 'consequent',\n",
       " 'considerable',\n",
       " 'consist',\n",
       " 'constant',\n",
       " 'constitute',\n",
       " 'constrain',\n",
       " 'construct',\n",
       " 'consult',\n",
       " 'consume',\n",
       " 'contact',\n",
       " 'contemporary',\n",
       " 'context',\n",
       " 'contract',\n",
       " 'contradict',\n",
       " 'contrary',\n",
       " 'contrast',\n",
       " 'contribute',\n",
       " 'controversy',\n",
       " 'convene',\n",
       " 'converse',\n",
       " 'convert',\n",
       " 'convince',\n",
       " 'cooperate',\n",
       " 'coordinate',\n",
       " 'core',\n",
       " 'corporate',\n",
       " 'correspond',\n",
       " 'couple',\n",
       " 'create',\n",
       " 'credit',\n",
       " 'criteria',\n",
       " 'crucial',\n",
       " 'culture',\n",
       " 'currency',\n",
       " 'cycle',\n",
       " 'data',\n",
       " 'debate',\n",
       " 'decade',\n",
       " 'decline',\n",
       " 'deduce',\n",
       " 'define',\n",
       " 'definite',\n",
       " 'demonstrate',\n",
       " 'denote',\n",
       " 'deny',\n",
       " 'depress',\n",
       " 'derive',\n",
       " 'design',\n",
       " 'despite',\n",
       " 'detect',\n",
       " 'deviate',\n",
       " 'device',\n",
       " 'devote',\n",
       " 'differentiate',\n",
       " 'dimension',\n",
       " 'diminish',\n",
       " 'discrete',\n",
       " 'discriminate',\n",
       " 'displace',\n",
       " 'display',\n",
       " 'dispose',\n",
       " 'distinct',\n",
       " 'distort',\n",
       " 'distribute',\n",
       " 'diverse',\n",
       " 'document',\n",
       " 'domain',\n",
       " 'domestic',\n",
       " 'dominate',\n",
       " 'draft',\n",
       " 'drama',\n",
       " 'duration',\n",
       " 'dynamic',\n",
       " 'economy',\n",
       " 'edit',\n",
       " 'element',\n",
       " 'eliminate',\n",
       " 'emerge',\n",
       " 'emphasis',\n",
       " 'empirical',\n",
       " 'enable',\n",
       " 'encounter',\n",
       " 'energy',\n",
       " 'enforce',\n",
       " 'enhance',\n",
       " 'enormous',\n",
       " 'ensure',\n",
       " 'entity',\n",
       " 'environment',\n",
       " 'equate',\n",
       " 'equip',\n",
       " 'equivalent',\n",
       " 'erode',\n",
       " 'error',\n",
       " 'establish',\n",
       " 'estate',\n",
       " 'estimate',\n",
       " 'ethic',\n",
       " 'ethnic',\n",
       " 'evaluate',\n",
       " 'eventual',\n",
       " 'evident',\n",
       " 'evolve',\n",
       " 'exceed',\n",
       " 'exclude',\n",
       " 'exhibit',\n",
       " 'expand',\n",
       " 'expert',\n",
       " 'explicit',\n",
       " 'exploit',\n",
       " 'export',\n",
       " 'expose',\n",
       " 'external',\n",
       " 'extract',\n",
       " 'facilitate',\n",
       " 'factor',\n",
       " 'feature',\n",
       " 'federal',\n",
       " 'fee',\n",
       " 'file',\n",
       " 'final',\n",
       " 'finance',\n",
       " 'finite',\n",
       " 'flexible',\n",
       " 'fluctuate',\n",
       " 'focus',\n",
       " 'format',\n",
       " 'formula',\n",
       " 'forthcoming',\n",
       " 'foundation',\n",
       " 'found',\n",
       " 'framework',\n",
       " 'function',\n",
       " 'fund',\n",
       " 'fundamental',\n",
       " 'furthermore',\n",
       " 'gender',\n",
       " 'generate',\n",
       " 'generation',\n",
       " 'globe',\n",
       " 'goal',\n",
       " 'grade',\n",
       " 'grant',\n",
       " 'guarantee',\n",
       " 'guideline',\n",
       " 'hence',\n",
       " 'hierarchy',\n",
       " 'highlight',\n",
       " 'hypothesis',\n",
       " 'identical',\n",
       " 'identify',\n",
       " 'ideology',\n",
       " 'ignorance',\n",
       " 'illustrate',\n",
       " 'image',\n",
       " 'immigrate',\n",
       " 'impact',\n",
       " 'implement',\n",
       " 'implicate',\n",
       " 'implicit',\n",
       " 'imply',\n",
       " 'impose',\n",
       " 'incentive',\n",
       " 'incidence',\n",
       " 'incline',\n",
       " 'income',\n",
       " 'incorporate',\n",
       " 'index',\n",
       " 'indicate',\n",
       " 'individual',\n",
       " 'induce',\n",
       " 'inevitable',\n",
       " 'infer',\n",
       " 'infrastructure',\n",
       " 'inherent',\n",
       " 'inhibit',\n",
       " 'initial',\n",
       " 'initiate',\n",
       " 'injure',\n",
       " 'innovate',\n",
       " 'input',\n",
       " 'insert',\n",
       " 'insight',\n",
       " 'inspect',\n",
       " 'instance',\n",
       " 'institute',\n",
       " 'instruct',\n",
       " 'integral',\n",
       " 'integrate',\n",
       " 'integrity',\n",
       " 'intelligence',\n",
       " 'intense',\n",
       " 'interact',\n",
       " 'intermediate',\n",
       " 'internal',\n",
       " 'interpret',\n",
       " 'interval',\n",
       " 'intervene',\n",
       " 'intrinsic',\n",
       " 'invest',\n",
       " 'investigate',\n",
       " 'invoke',\n",
       " 'involve',\n",
       " 'isolate',\n",
       " 'issue',\n",
       " 'item',\n",
       " 'job',\n",
       " 'journal',\n",
       " 'justify',\n",
       " 'label',\n",
       " 'labour',\n",
       " 'layer',\n",
       " 'lecture',\n",
       " 'legal',\n",
       " 'legislate',\n",
       " 'levy',\n",
       " 'liberal',\n",
       " 'licence',\n",
       " 'likewise',\n",
       " 'link',\n",
       " 'locate',\n",
       " 'logic',\n",
       " 'maintain',\n",
       " 'major',\n",
       " 'manipulate',\n",
       " 'manual',\n",
       " 'margin',\n",
       " 'mature',\n",
       " 'maximise',\n",
       " 'mechanism',\n",
       " 'media',\n",
       " 'mediate',\n",
       " 'medical',\n",
       " 'medium',\n",
       " 'mental',\n",
       " 'method',\n",
       " 'migrate',\n",
       " 'military',\n",
       " 'minimal',\n",
       " 'minimise',\n",
       " 'minimum',\n",
       " 'ministry',\n",
       " 'minor',\n",
       " 'mode',\n",
       " 'modify',\n",
       " 'monitor',\n",
       " 'motive',\n",
       " 'mutual',\n",
       " 'negate',\n",
       " 'network',\n",
       " 'neutral',\n",
       " 'nevertheless',\n",
       " 'nonetheless',\n",
       " 'norm',\n",
       " 'normal',\n",
       " 'notion',\n",
       " 'notwithstanding',\n",
       " 'nuclear',\n",
       " 'objective',\n",
       " 'obtain',\n",
       " 'obvious',\n",
       " 'occupy',\n",
       " 'occur',\n",
       " 'odd',\n",
       " 'offset',\n",
       " 'ongoing',\n",
       " 'option',\n",
       " 'orient',\n",
       " 'outcome',\n",
       " 'output',\n",
       " 'overall',\n",
       " 'overlap',\n",
       " 'overseas',\n",
       " 'panel',\n",
       " 'paradigm',\n",
       " 'paragraph',\n",
       " 'parallel',\n",
       " 'parameter',\n",
       " 'participate',\n",
       " 'partner',\n",
       " 'passive',\n",
       " 'perceive',\n",
       " 'percent',\n",
       " 'period',\n",
       " 'persist',\n",
       " 'perspective',\n",
       " 'phase',\n",
       " 'phenomenon',\n",
       " 'philosophy',\n",
       " 'physical',\n",
       " 'plus',\n",
       " 'policy',\n",
       " 'portion',\n",
       " 'pose',\n",
       " 'positive',\n",
       " 'potential',\n",
       " 'practitioner',\n",
       " 'precede',\n",
       " 'precise',\n",
       " 'predict',\n",
       " 'predominant',\n",
       " 'preliminary',\n",
       " 'presume',\n",
       " 'previous',\n",
       " 'primary',\n",
       " 'prime',\n",
       " 'principal',\n",
       " 'principle',\n",
       " 'prior',\n",
       " 'priority',\n",
       " 'proceed',\n",
       " 'process',\n",
       " 'professional',\n",
       " 'prohibit',\n",
       " 'project',\n",
       " 'promote',\n",
       " 'proportion',\n",
       " 'prospect',\n",
       " 'protocol',\n",
       " 'psychology',\n",
       " 'publication',\n",
       " 'publish',\n",
       " 'purchase',\n",
       " 'pursue',\n",
       " 'qualitative',\n",
       " 'quote',\n",
       " 'radical',\n",
       " 'random',\n",
       " 'range',\n",
       " 'ratio',\n",
       " 'rational',\n",
       " 'react',\n",
       " 'recover',\n",
       " 'refine',\n",
       " 'regime',\n",
       " 'region',\n",
       " 'register',\n",
       " 'regulate',\n",
       " 'reinforce',\n",
       " 'reject',\n",
       " 'relax',\n",
       " 'release',\n",
       " 'relevant',\n",
       " 'reluctance',\n",
       " 'rely',\n",
       " 'remove',\n",
       " 'require',\n",
       " 'research',\n",
       " 'reside',\n",
       " 'resolve',\n",
       " 'resource',\n",
       " 'respond',\n",
       " 'restore',\n",
       " 'restrain',\n",
       " 'restrict',\n",
       " 'retain',\n",
       " 'reveal',\n",
       " 'revenue',\n",
       " 'reverse',\n",
       " 'revise',\n",
       " 'revolution',\n",
       " 'rigid',\n",
       " 'role',\n",
       " 'route',\n",
       " 'scenario',\n",
       " 'schedule',\n",
       " 'scheme',\n",
       " 'scope',\n",
       " 'section',\n",
       " 'sector',\n",
       " 'secure',\n",
       " 'seek',\n",
       " 'select',\n",
       " 'sequence',\n",
       " 'series',\n",
       " 'sex',\n",
       " 'shift',\n",
       " 'significant',\n",
       " 'similar',\n",
       " 'simulate',\n",
       " 'site',\n",
       " 'so-called',\n",
       " 'sole',\n",
       " 'somewhat',\n",
       " 'source',\n",
       " 'specific',\n",
       " 'specify',\n",
       " 'sphere',\n",
       " 'stable',\n",
       " 'statistic',\n",
       " 'status',\n",
       " 'straightforward',\n",
       " 'strategy',\n",
       " 'stress',\n",
       " 'structure',\n",
       " 'style',\n",
       " 'submit',\n",
       " 'subordinate',\n",
       " 'subsequent',\n",
       " 'subsidy',\n",
       " 'substitute',\n",
       " 'successor',\n",
       " 'sufficient',\n",
       " 'sum',\n",
       " 'summary',\n",
       " 'supplement',\n",
       " 'survey',\n",
       " 'survive',\n",
       " 'suspend',\n",
       " 'sustain',\n",
       " 'symbol',\n",
       " 'tape',\n",
       " 'target',\n",
       " 'task',\n",
       " 'team',\n",
       " 'technical',\n",
       " 'technique',\n",
       " 'technology',\n",
       " 'temporary',\n",
       " 'tense',\n",
       " 'terminate',\n",
       " 'text',\n",
       " 'theme',\n",
       " 'theory',\n",
       " 'thereby',\n",
       " 'thesis',\n",
       " 'topic',\n",
       " 'trace',\n",
       " 'tradition',\n",
       " 'transfer',\n",
       " 'transform',\n",
       " 'transit',\n",
       " 'transmit',\n",
       " 'transport',\n",
       " 'trend',\n",
       " 'trigger',\n",
       " 'ultimate',\n",
       " 'undergo',\n",
       " 'underlie',\n",
       " 'undertake',\n",
       " 'uniform',\n",
       " 'unify',\n",
       " 'unique',\n",
       " 'utilise',\n",
       " 'valid',\n",
       " 'vary',\n",
       " 'vehicle',\n",
       " 'version',\n",
       " 'via',\n",
       " 'violate',\n",
       " 'virtual',\n",
       " 'visible',\n",
       " 'vision',\n",
       " 'visual',\n",
       " 'volume',\n",
       " 'voluntary',\n",
       " 'welfare',\n",
       " 'whereas']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(\"[^0-9]\") #숫자제거\n",
    "q = \"\".join(p.findall(text)) #문자만 다시 추출\n",
    "r = q.split() #공백으로 구분해서 리스트로 만듬\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## contextual semantic search  ---- topic\n",
    "\n",
    "https://github.com/ddangelov/Top2Vec/blob/master/notebooks/CORD-19_top2vec.ipynb\n",
    "\n",
    "Top2Vec is an algorithm for topic modelling and semantic search. It automatically detects topics present in text and generates jointly embedded topic, document and word vectors. Once you train the Top2Vec model you can:\n",
    "\n",
    "Get number of detected topics.\n",
    "Get topics.\n",
    "Search topics by keywords.\n",
    "Search documents by topic.\n",
    "Find similar words.\n",
    "Find similar documents.\n",
    "\n",
    "\n",
    "Once the model is trained you can do semantic search for documents by topic, searching for documents with keywords, searching for topics with keywords, and for finding similar words. These methods all leverage the joint topic, document, word embeddings distances, which represent semantic similarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting top2vec\n",
      "  Downloading top2vec-1.0.16-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: pandas in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from top2vec) (1.1.2)\n",
      "Requirement already satisfied: gensim in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from top2vec) (3.8.3)\n",
      "Requirement already satisfied: wordcloud in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from top2vec) (1.8.0)\n",
      "Collecting hdbscan\n",
      "  Downloading hdbscan-0.8.26.tar.gz (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 508 kB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pynndescent>=0.4\n",
      "  Downloading pynndescent-0.5.0.tar.gz (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from top2vec) (1.19.1)\n",
      "Collecting umap-learn\n",
      "  Downloading umap-learn-0.4.6.tar.gz (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 4.8 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from pandas->top2vec) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from pandas->top2vec) (2.8.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from gensim->top2vec) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from gensim->top2vec) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from gensim->top2vec) (1.4.1)\n",
      "Requirement already satisfied: pillow in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from wordcloud->top2vec) (7.2.0)\n",
      "Requirement already satisfied: matplotlib in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from wordcloud->top2vec) (3.3.2)\n",
      "Requirement already satisfied: joblib in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from hdbscan->top2vec) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn>=0.17 in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from hdbscan->top2vec) (0.23.2)\n",
      "Collecting cython>=0.27\n",
      "  Using cached Cython-0.29.21-cp37-cp37m-macosx_10_9_x86_64.whl (1.9 MB)\n",
      "Collecting numba>=0.51.2\n",
      "  Downloading numba-0.51.2-cp37-cp37m-macosx_10_14_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 7.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting llvmlite>=0.30\n",
      "  Downloading llvmlite-0.34.0-cp37-cp37m-macosx_10_9_x86_64.whl (18.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 18.4 MB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim->top2vec) (2.24.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from matplotlib->wordcloud->top2vec) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from matplotlib->wordcloud->top2vec) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from matplotlib->wordcloud->top2vec) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from matplotlib->wordcloud->top2vec) (2020.6.20)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from scikit-learn>=0.17->hdbscan->top2vec) (2.1.0)\n",
      "Requirement already satisfied: setuptools in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from numba>=0.51.2->pynndescent>=0.4->top2vec) (49.6.0.post20200814)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim->top2vec) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim->top2vec) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim->top2vec) (2.10)\n",
      "Building wheels for collected packages: hdbscan, pynndescent, umap-learn\n",
      "  Building wheel for hdbscan (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdbscan: filename=hdbscan-0.8.26-cp37-cp37m-macosx_10_9_x86_64.whl size=676420 sha256=25c0b189658560361bf7ca45e68499cd2551c726d2d1eba4c3663a9df2459325\n",
      "  Stored in directory: /Users/kimkwangil/Library/Caches/pip/wheels/12/bb/54/7401fdc7883b5975dba5514a185d75d7a0986472df4957637b\n",
      "  Building wheel for pynndescent (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pynndescent: filename=pynndescent-0.5.0-py3-none-any.whl size=48987 sha256=490bbf9379ebcef5db9a958bc77ef259975eedcda5c7e071dbe68a66506cc6c4\n",
      "  Stored in directory: /Users/kimkwangil/Library/Caches/pip/wheels/a5/69/1b/4f452904877ffec179c32e84122e700e14d4d06f1316e55b41\n",
      "  Building wheel for umap-learn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for umap-learn: filename=umap_learn-0.4.6-py3-none-any.whl size=67950 sha256=bb9f1974c122acbd176f777b5a8f7cab20d0e852a726876e3550864a4e319095\n",
      "  Stored in directory: /Users/kimkwangil/Library/Caches/pip/wheels/14/9a/ed/66159a5e13d3b6341b6542cf4f1faf478834753bed5ecaef8d\n",
      "Successfully built hdbscan pynndescent umap-learn\n",
      "Installing collected packages: cython, hdbscan, llvmlite, numba, pynndescent, umap-learn, top2vec\n",
      "Successfully installed cython-0.29.21 hdbscan-0.8.26 llvmlite-0.34.0 numba-0.51.2 pynndescent-0.5.0 top2vec-1.0.16 umap-learn-0.4.6\n"
     ]
    }
   ],
   "source": [
    "!pip install top2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import json\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display\n",
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (1,4,5,6,13,14,15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>mag_id</th>\n",
       "      <th>who_covidence_id</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>pdf_json_files</th>\n",
       "      <th>pmc_json_files</th>\n",
       "      <th>url</th>\n",
       "      <th>s2_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ug7v899j</td>\n",
       "      <td>d1aafb70c066a2068b02786f8929fd9c900897fb</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Clinical features of culture-proven Mycoplasma...</td>\n",
       "      <td>10.1186/1471-2334-1-6</td>\n",
       "      <td>PMC35282</td>\n",
       "      <td>11472636</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>OBJECTIVE: This retrospective chart review des...</td>\n",
       "      <td>2001-07-04</td>\n",
       "      <td>Madani, Tariq A; Al-Ghamdi, Aisha A</td>\n",
       "      <td>BMC Infect Dis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/d1aafb70c066a2068b027...</td>\n",
       "      <td>document_parses/pmc_json/PMC35282.xml.json</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02tnwd4m</td>\n",
       "      <td>6b0567729c2143a66d737eb0a2f63f2dce2e5a7d</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Nitric oxide: a pro-inflammatory mediator in l...</td>\n",
       "      <td>10.1186/rr14</td>\n",
       "      <td>PMC59543</td>\n",
       "      <td>11667967</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Inflammatory diseases of the respiratory tract...</td>\n",
       "      <td>2000-08-15</td>\n",
       "      <td>Vliet, Albert van der; Eiserich, Jason P; Cros...</td>\n",
       "      <td>Respir Res</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/6b0567729c2143a66d737...</td>\n",
       "      <td>document_parses/pmc_json/PMC59543.xml.json</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ejv2xln0</td>\n",
       "      <td>06ced00a5fc04215949aa72528f2eeaae1d58927</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Surfactant protein-D and pulmonary host defense</td>\n",
       "      <td>10.1186/rr19</td>\n",
       "      <td>PMC59549</td>\n",
       "      <td>11667972</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Surfactant protein-D (SP-D) participates in th...</td>\n",
       "      <td>2000-08-25</td>\n",
       "      <td>Crouch, Erika C</td>\n",
       "      <td>Respir Res</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/06ced00a5fc04215949aa...</td>\n",
       "      <td>document_parses/pmc_json/PMC59549.xml.json</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2b73a28n</td>\n",
       "      <td>348055649b6b8cf2b9a376498df9bf41f7123605</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Role of endothelin-1 in lung disease</td>\n",
       "      <td>10.1186/rr44</td>\n",
       "      <td>PMC59574</td>\n",
       "      <td>11686871</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Endothelin-1 (ET-1) is a 21 amino acid peptide...</td>\n",
       "      <td>2001-02-22</td>\n",
       "      <td>Fagan, Karen A; McMurtry, Ivan F; Rodman, David M</td>\n",
       "      <td>Respir Res</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/348055649b6b8cf2b9a37...</td>\n",
       "      <td>document_parses/pmc_json/PMC59574.xml.json</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9785vg6d</td>\n",
       "      <td>5f48792a5fa08bed9f56016f4981ae2ca6031b32</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Gene expression in epithelial cells in respons...</td>\n",
       "      <td>10.1186/rr61</td>\n",
       "      <td>PMC59580</td>\n",
       "      <td>11686888</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Respiratory syncytial virus (RSV) and pneumoni...</td>\n",
       "      <td>2001-05-11</td>\n",
       "      <td>Domachowske, Joseph B; Bonville, Cynthia A; Ro...</td>\n",
       "      <td>Respir Res</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/5f48792a5fa08bed9f560...</td>\n",
       "      <td>document_parses/pmc_json/PMC59580.xml.json</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                       sha source_x  \\\n",
       "0  ug7v899j  d1aafb70c066a2068b02786f8929fd9c900897fb      PMC   \n",
       "1  02tnwd4m  6b0567729c2143a66d737eb0a2f63f2dce2e5a7d      PMC   \n",
       "2  ejv2xln0  06ced00a5fc04215949aa72528f2eeaae1d58927      PMC   \n",
       "3  2b73a28n  348055649b6b8cf2b9a376498df9bf41f7123605      PMC   \n",
       "4  9785vg6d  5f48792a5fa08bed9f56016f4981ae2ca6031b32      PMC   \n",
       "\n",
       "                                               title                    doi  \\\n",
       "0  Clinical features of culture-proven Mycoplasma...  10.1186/1471-2334-1-6   \n",
       "1  Nitric oxide: a pro-inflammatory mediator in l...           10.1186/rr14   \n",
       "2    Surfactant protein-D and pulmonary host defense           10.1186/rr19   \n",
       "3               Role of endothelin-1 in lung disease           10.1186/rr44   \n",
       "4  Gene expression in epithelial cells in respons...           10.1186/rr61   \n",
       "\n",
       "      pmcid pubmed_id license  \\\n",
       "0  PMC35282  11472636   no-cc   \n",
       "1  PMC59543  11667967   no-cc   \n",
       "2  PMC59549  11667972   no-cc   \n",
       "3  PMC59574  11686871   no-cc   \n",
       "4  PMC59580  11686888   no-cc   \n",
       "\n",
       "                                            abstract publish_time  \\\n",
       "0  OBJECTIVE: This retrospective chart review des...   2001-07-04   \n",
       "1  Inflammatory diseases of the respiratory tract...   2000-08-15   \n",
       "2  Surfactant protein-D (SP-D) participates in th...   2000-08-25   \n",
       "3  Endothelin-1 (ET-1) is a 21 amino acid peptide...   2001-02-22   \n",
       "4  Respiratory syncytial virus (RSV) and pneumoni...   2001-05-11   \n",
       "\n",
       "                                             authors         journal  mag_id  \\\n",
       "0                Madani, Tariq A; Al-Ghamdi, Aisha A  BMC Infect Dis     NaN   \n",
       "1  Vliet, Albert van der; Eiserich, Jason P; Cros...      Respir Res     NaN   \n",
       "2                                    Crouch, Erika C      Respir Res     NaN   \n",
       "3  Fagan, Karen A; McMurtry, Ivan F; Rodman, David M      Respir Res     NaN   \n",
       "4  Domachowske, Joseph B; Bonville, Cynthia A; Ro...      Respir Res     NaN   \n",
       "\n",
       "  who_covidence_id arxiv_id  \\\n",
       "0              NaN      NaN   \n",
       "1              NaN      NaN   \n",
       "2              NaN      NaN   \n",
       "3              NaN      NaN   \n",
       "4              NaN      NaN   \n",
       "\n",
       "                                      pdf_json_files  \\\n",
       "0  document_parses/pdf_json/d1aafb70c066a2068b027...   \n",
       "1  document_parses/pdf_json/6b0567729c2143a66d737...   \n",
       "2  document_parses/pdf_json/06ced00a5fc04215949aa...   \n",
       "3  document_parses/pdf_json/348055649b6b8cf2b9a37...   \n",
       "4  document_parses/pdf_json/5f48792a5fa08bed9f560...   \n",
       "\n",
       "                               pmc_json_files  \\\n",
       "0  document_parses/pmc_json/PMC35282.xml.json   \n",
       "1  document_parses/pmc_json/PMC59543.xml.json   \n",
       "2  document_parses/pmc_json/PMC59549.xml.json   \n",
       "3  document_parses/pmc_json/PMC59574.xml.json   \n",
       "4  document_parses/pmc_json/PMC59580.xml.json   \n",
       "\n",
       "                                                 url  s2_id  \n",
       "0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...    NaN  \n",
       "1  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
       "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
       "3  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n",
       "4  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df = pd.read_csv(\"metadata.csv\")\n",
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Papers\n",
    "A document will be created for each section of every paper. This document will contain the id, title, abstract, and setion of the paper. It will also contain the text of that section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproccess_papers():\n",
    "\n",
    "    dataset_dir = \"../input/CORD-19-research-challenge/\"\n",
    "    comm_dir = dataset_dir+\"comm_use_subset/comm_use_subset/\"\n",
    "    noncomm_dir = dataset_dir+\"noncomm_use_subset/noncomm_use_subset/\"\n",
    "    custom_dir = dataset_dir+\"custom_license/custom_license/\"\n",
    "    biorxiv_dir = dataset_dir+\"biorxiv_medrxiv/biorxiv_medrxiv/\"\n",
    "    directories_to_process = [comm_dir,noncomm_dir, custom_dir, biorxiv_dir]\n",
    "\n",
    "    papers_with_text = list(metadata_df[metadata_df.has_full_text==True].sha)\n",
    "\n",
    "    paper_ids = []\n",
    "    titles = []\n",
    "    abstracts = []\n",
    "    sections = []\n",
    "    body_texts = []\n",
    "\n",
    "    for directory in directories_to_process:\n",
    "\n",
    "        filenames = os.listdir(directory)\n",
    "\n",
    "        for filename in filenames:\n",
    "\n",
    "          file = json.load(open(directory+filename, 'rb'))\n",
    "\n",
    "          #check if file contains text\n",
    "          if file[\"paper_id\"] in papers_with_text:\n",
    "\n",
    "            section = []\n",
    "            text = []\n",
    "\n",
    "            for bod in file[\"body_text\"]:\n",
    "              section.append(bod[\"section\"])\n",
    "              text.append(bod[\"text\"])\n",
    "\n",
    "            res_df = pd.DataFrame({\"section\":section, \"text\":text}).groupby(\"section\")[\"text\"].apply(' '.join).reset_index()\n",
    "\n",
    "            for index, row in res_df.iterrows():\n",
    "\n",
    "              # metadata\n",
    "              paper_ids.append(file[\"paper_id\"])\n",
    "\n",
    "              if(len(file[\"abstract\"])):\n",
    "                abstracts.append(file[\"abstract\"][0][\"text\"])\n",
    "              else:\n",
    "                abstracts.append(\"\")\n",
    "\n",
    "              titles.append(file[\"metadata\"][\"title\"])\n",
    "\n",
    "              # add section and text\n",
    "              sections.append(row.section)\n",
    "              body_texts.append(row.text)\n",
    "\n",
    "    return pd.DataFrame({\"id\":paper_ids, \"title\": titles, \"abstract\": abstracts, \"section\": sections, \"text\": body_texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'has_full_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-4ff27f8d230d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpapers_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreproccess_papers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpapers_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-d836e8419c2e>\u001b[0m in \u001b[0;36mpreproccess_papers\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdirectories_to_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcomm_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoncomm_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiorxiv_dir\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpapers_with_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetadata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_full_text\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpaper_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'has_full_text'"
     ]
    }
   ],
   "source": [
    "# papers_df = preproccess_papers()\n",
    "# papers_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Short Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short(papers_df):\n",
    "    papers_df[\"token_counts\"] = papers_df[\"text\"].str.split().map(len)\n",
    "    papers_df = papers_df[papers_df.token_counts>200].reset_index(drop=True)\n",
    "    papers_df.drop('token_counts', axis=1, inplace=True)\n",
    "    \n",
    "    return papers_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train Top2Vec Model\n",
    "Parameters:\n",
    "\n",
    "documents: Input corpus, should be a list of strings.\n",
    "\n",
    "speed: This parameter will determine how fast the model takes to train. The 'fast-learn' option is the fastest and will generate the lowest quality vectors. The 'learn' option will learn better quality vectors but take a longer time to train. The 'deep-learn' option will learn the best quality vectors but will take significant time to train.\n",
    "\n",
    "workers: The amount of worker threads to be used in training the model. Larger amount will lead to faster training.\n",
    "\n",
    "See Documentation.>>>> https://top2vec.readthedocs.io/en/latest/README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'papers_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-d16af5876566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtop2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTop2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpapers_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"learn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'papers_df' is not defined"
     ]
    }
   ],
   "source": [
    "top2vec = Top2Vec(documents=papers_df.text, speed=\"learn\", workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##(Recommended) Load Pre-trained Model and Pre-processed Data :)\n",
    "The Top2Vec model was trained with the 'deep-learn' speed parameter and took very long to train. It will give much better results than training with 'fast-learn' or 'learn'.\n",
    "\n",
    "Data is available on my kaggle. >>>>> https://www.kaggle.com/dangelov/covid19top2vec\n",
    "\n",
    "1. Load pre-trained Top2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2vec = Top2Vec.load(\"covid19_deep_learn_top2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df = pd.read_feather(\"covid19_papers_processed.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Top2Vec for Semantic Search\n",
    "1. Search Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab784890f93482bb6b0b355619989c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Enter keywords seperated by space: ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe63a677bae44ae8d6e3b73f83c0101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='covid')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8559f3ef2049d88a93389eb79a6045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Enter negative keywords seperated by space: ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bc8f52f7a34cc3b8d17c45ac7b1034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='victim')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a02b32695641d2956f2ac5e85c9bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Choose number of topics: ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f116e3c55b4ffda51bb4e86339e95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='5')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914dfaa0519e43b2b37ac91608ad73d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='show topics', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Top2Vec' object has no attribute 'embedding_model'\n"
     ]
    }
   ],
   "source": [
    "keywords_select_st = widgets.Label('Enter keywords seperated by space: ')\n",
    "display(keywords_select_st)\n",
    "\n",
    "keywords_input_st = widgets.Text()\n",
    "display(keywords_input_st)\n",
    "\n",
    "keywords_neg_select_st = widgets.Label('Enter negative keywords seperated by space: ')\n",
    "display(keywords_neg_select_st)\n",
    "\n",
    "keywords_neg_input_st = widgets.Text()\n",
    "display(keywords_neg_input_st)\n",
    "\n",
    "doc_num_select_st = widgets.Label('Choose number of topics: ')\n",
    "display(doc_num_select_st)\n",
    "\n",
    "doc_num_input_st = widgets.Text(value='5')\n",
    "display(doc_num_input_st)\n",
    "\n",
    "def display_similar_topics(*args):\n",
    "    \n",
    "    clear_output()\n",
    "    display(keywords_select_st)\n",
    "    display(keywords_input_st)\n",
    "    display(keywords_neg_select_st)\n",
    "    display(keywords_neg_input_st)\n",
    "    display(doc_num_select_st)\n",
    "    display(doc_num_input_st)\n",
    "    display(keyword_btn_st)\n",
    "    \n",
    "    try:\n",
    "        topic_words, word_scores, topic_scores, topic_nums = top2vec.search_topics(keywords=keywords_input_st.value.split(),num_topics=int(doc_num_input_st.value), keywords_neg=keywords_neg_input_st.value.split())\n",
    "        for topic in topic_nums:\n",
    "            top2vec.generate_topic_wordcloud(topic, background_color=\"black\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "keyword_btn_st = widgets.Button(description=\"show topics\")\n",
    "display(keyword_btn_st)\n",
    "keyword_btn_st.on_click(display_similar_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Papers by Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1182eb407fcc491583d16ec3e0c35a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Select topic number: ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d5a8f047e1423ebb149dde9d351ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='20')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1783963eab4c7d830a43ac30288d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Choose number of documents: ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0547eae44d0342c795e16db04243e1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='10')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146553ee8deb4c9a85ab2c310500888c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='show documents', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Top2Vec' object has no attribute 'topic_sizes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-a67d00a3069e>\u001b[0m in \u001b[0;36mdisplay_topics\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_btn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_nums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_documents_by_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_num_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mresult_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpapers_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocument_nums\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/top2vec/Top2Vec.py\u001b[0m in \u001b[0;36msearch_documents_by_topic\u001b[0;34m(self, topic_num, num_docs, return_documents, reduced)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_topic_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_topic_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0mtopic_document_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_top\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtopic_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/top2vec/Top2Vec.py\u001b[0m in \u001b[0;36m_validate_topic_search\u001b[0;34m(self, topic_num, num_docs, reduced)\u001b[0m\n\u001b[1;32m    731\u001b[0m                                  f\" only has {self.topic_sizes_reduced[topic_num]} documents.\")\n\u001b[1;32m    732\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnum_docs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m                 raise ValueError(f\"Invalid number of documents: original topic {topic_num}\"\n\u001b[1;32m    735\u001b[0m                                  f\" only has {self.topic_sizes[topic_num]} documents.\")\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Top2Vec' object has no attribute 'topic_sizes'"
     ]
    }
   ],
   "source": [
    "topic_num_select = widgets.Label('Select topic number: ')\n",
    "display(topic_num_select)\n",
    "\n",
    "topic_input = widgets.Text()\n",
    "display(topic_input)\n",
    "\n",
    "doc_num_select = widgets.Label('Choose number of documents: ')\n",
    "display(doc_num_select)\n",
    "\n",
    "doc_num_input = widgets.Text(value='10')\n",
    "display(doc_num_input)\n",
    "\n",
    "def display_topics(*args):\n",
    "    \n",
    "    clear_output()\n",
    "    display(topic_num_select)\n",
    "    display(topic_input)\n",
    "    display(doc_num_select)\n",
    "    display(doc_num_input)\n",
    "    display(topic_btn)\n",
    "\n",
    "    documents, document_scores, document_nums = top2vec.search_documents_by_topic(topic_num=int(topic_input.value), num_docs=int(doc_num_input.value))\n",
    "    \n",
    "    result_df = papers_df.loc[document_nums]\n",
    "    result_df[\"document_scores\"] = document_scores\n",
    "    \n",
    "    for index,row in result_df.iterrows():\n",
    "        print(f\"Document: {index}, Score: {row.document_scores}\")\n",
    "        print(f\"Section: {row.section}\")\n",
    "        print(f\"Title: {row.title}\")\n",
    "        print(\"-----------\")\n",
    "        print(row.text)\n",
    "        print(\"-----------\")\n",
    "        print()\n",
    "\n",
    "topic_btn = widgets.Button(description=\"show documents\")\n",
    "display(topic_btn)\n",
    "topic_btn.on_click(display_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Search Papers by Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a5faeb59144c82a1bb7094c827d506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Enter keywords seperated by space: ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55b3acf0bb04436ab6e51698d9b7f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74c89c196174104a6d9ba554e38f38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Enter negative keywords seperated by space: ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5d3d6d7f034e8785d65600e866688f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22654eed98745efaa420aac1f7c7fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Choose number of documents: ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac18e00ef934ef79a7f6cff84495060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='10')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b29c5d8d91e47bdb380066d70844884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='show documents', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keywords_select_kw = widgets.Label('Enter keywords seperated by space: ')\n",
    "display(keywords_select_kw)\n",
    "\n",
    "keywords_input_kw = widgets.Text()\n",
    "display(keywords_input_kw)\n",
    "\n",
    "keywords_neg_select_kw = widgets.Label('Enter negative keywords seperated by space: ')\n",
    "display(keywords_neg_select_kw)\n",
    "\n",
    "keywords_neg_input_kw = widgets.Text()\n",
    "display(keywords_neg_input_kw)\n",
    "\n",
    "doc_num_select_kw = widgets.Label('Choose number of documents: ')\n",
    "display(doc_num_select_kw)\n",
    "\n",
    "doc_num_input_kw = widgets.Text(value='10')\n",
    "display(doc_num_input_kw)\n",
    "\n",
    "def display_keywords(*args):\n",
    "    \n",
    "    clear_output()\n",
    "    display(keywords_select_kw)\n",
    "    display(keywords_input_kw)\n",
    "    display(keywords_neg_select_kw)\n",
    "    display(keywords_neg_input_kw)\n",
    "    display(doc_num_select_kw)\n",
    "    display(doc_num_input_kw)\n",
    "    display(keyword_btn_kw)\n",
    "    \n",
    "    try:\n",
    "        documents, document_scores, document_nums = top2vec.search_documents_by_keyword(keywords=keywords_input_kw.value.split(), num_docs=int(doc_num_input_kw.value), keywords_neg=keywords_neg_input_kw.value.split())\n",
    "        result_df = papers_df.loc[document_nums]\n",
    "        result_df[\"document_scores\"] = document_scores\n",
    "\n",
    "        for index,row in result_df.iterrows():\n",
    "            print(f\"Document: {index}, Score: {row.document_scores}\")\n",
    "            print(f\"Section: {row.section}\")\n",
    "            print(f\"Title: {row.title}\")\n",
    "            print(\"-----------\")\n",
    "            print(row.text)\n",
    "            print(\"-----------\")\n",
    "            print()\n",
    "           \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "\n",
    "keyword_btn_kw = widgets.Button(description=\"show documents\")\n",
    "display(keyword_btn_kw)\n",
    "keyword_btn_kw.on_click(display_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Find Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7437a7897544a7b1ea8ddc5a23ad78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Enter keywords seperated by space: ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4791a373e374753a6ed77428de78b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c363394ecb455abfd658455919372d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Enter negative keywords seperated by space: ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526b4f2fc8da4f2b88d318e94b59b326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c8104375a34a199551705eb256c793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Choose number of words: ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a1c4d2c0b34c94b7af7ad52296179f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='20')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c395cc0564e4a94920e658330d68cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='show similar words', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keywords_select_sw = widgets.Label('Enter keywords seperated by space: ')\n",
    "display(keywords_select_sw)\n",
    "\n",
    "keywords_input_sw = widgets.Text()\n",
    "display(keywords_input_sw)\n",
    "\n",
    "keywords_neg_select_sw = widgets.Label('Enter negative keywords seperated by space: ')\n",
    "display(keywords_neg_select_sw)\n",
    "\n",
    "keywords_neg_input_sw = widgets.Text()\n",
    "display(keywords_neg_input_sw)\n",
    "\n",
    "\n",
    "doc_num_select_sw = widgets.Label('Choose number of words: ')\n",
    "display(doc_num_select_sw)\n",
    "\n",
    "doc_num_input_sw = widgets.Text(value='20')\n",
    "display(doc_num_input_sw)\n",
    "\n",
    "def display_similar_words(*args):\n",
    "    \n",
    "    clear_output()\n",
    "    display(keywords_select_sw)\n",
    "    display(keywords_input_sw)\n",
    "    display(keywords_neg_select_sw)\n",
    "    display(keywords_neg_input_sw)\n",
    "    display(doc_num_select_sw)\n",
    "    display(doc_num_input_sw)\n",
    "    display(sim_word_btn_sw)\n",
    "    \n",
    "    try:            \n",
    "        words, word_scores = top2vec.similar_words(keywords=keywords_input_sw.value.split(), keywords_neg=keywords_neg_input_sw.value.split(), num_words=int(doc_num_input_sw.value))\n",
    "        for word, score in zip(words, word_scores):\n",
    "            print(f\"{word} {score}\")\n",
    "   \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "sim_word_btn_sw = widgets.Button(description=\"show similar words\")\n",
    "display(sim_word_btn_sw)\n",
    "sim_word_btn_sw.on_click(display_similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
