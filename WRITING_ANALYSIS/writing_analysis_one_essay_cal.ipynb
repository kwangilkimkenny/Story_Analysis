{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/kimkwangil/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kimkwangil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Converted to Method...\n",
    "\n",
    "import collections as coll\n",
    "import math\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import style\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('cmudict')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "style.use(\"ggplot\")\n",
    "cmuDictionary = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 텍스트 한 단락을 취해서 그것을 지정된 수의 문장으로 나눈다.\n",
    "def slidingWindow(sequence, winSize, step=1):\n",
    "    try:\n",
    "        it = iter(sequence)\n",
    "        print ('slideingWinows it :', it)\n",
    "        #slideingWinows it : <str_iterator object at 0x7fe51178b910>  #작동할 때 결과\n",
    "    except TypeError:\n",
    "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
    "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
    "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
    "    if step > winSize:\n",
    "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
    "    if winSize > len(sequence):\n",
    "        raise Exception(\"**ERROR** winSize must not be larger than sequence length.\")\n",
    "\n",
    "    sequence = sent_tokenize(sequence)\n",
    "\n",
    "    # 생략할 사전 계산 청크 수\n",
    "    numOfChunks = int(((len(sequence) - winSize) / step) + 1)\n",
    "\n",
    "    l = []\n",
    "    for i in range(0, numOfChunks * step, step):\n",
    "        l.append(\" \".join(sequence[i:i + winSize]))\n",
    "\n",
    "    return l\n",
    "\n",
    "\n",
    "\n",
    "#모음수 세기\n",
    "def syllable_count_Manual(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "            if word.endswith(\"e\"):\n",
    "                count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    print(\"vowels numbers:\", count)\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "# COUNTS NUMBER OF 음절을 소문자로 바꾸고 문자열만 가져와서 에세이 내에서 음절이 몇개인지 카운트\n",
    "def syllable_count(word):\n",
    "    global cmuDictionary #사전을 사용하여 syl 수를 세어서 가저옴\n",
    "    d = cmuDictionary\n",
    "    try:\n",
    "        syl = [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    except:\n",
    "        syl = syllable_count_Manual(word)\n",
    "    # print(\"syllable numbers:\", syl)\n",
    "    return syl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# removing stop words, 구두점 등의 문장기호 제거하고 단어 평균 길이 계사\n",
    "def Avg_wordLength(str):\n",
    "    str.translate(string.punctuation)\n",
    "    tokens = word_tokenize(str, language='english')\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n', 'u00e9', '[', ']', 'n','n201', '\\u201d' ]\n",
    "    stop = stopwords.words('english') + st\n",
    "    words = [word for word in tokens if word not in stop]\n",
    "    return np.average([len(word) for word in words])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 문장수 계산\n",
    "def Avg_SentLenghtByCh(text):\n",
    "    tokens = sent_tokenize(text)\n",
    "    return np.average([len(token) for token in tokens])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 문장의 평균 단어 수 반환\n",
    "def Avg_SentLenghtByWord(text):\n",
    "    tokens = sent_tokenize(text)\n",
    "    return np.average([len(token.split()) for token in tokens])\n",
    "\n",
    "\n",
    "\n",
    "# 불용어 제거하고 단어당 음절 수를 파악하여 가독성 계산에 반영\n",
    "def Avg_Syllable_per_Word(text):\n",
    "    tokens = word_tokenize(text, language='english')\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    stop = stopwords.words('english') + st\n",
    "    words = [word for word in tokens if word not in stop]\n",
    "    syllabls = [syllable_count(word) for word in words]\n",
    "    p = (\" \".join(words))\n",
    "    return sum(syllabls) / max(1, len(words))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 청크 길이에서 정규화된 특수 문자 수\n",
    "def CountSpecialCharacter(text):\n",
    "    st = [\"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \"/\", \"<\", \"=\", '>',\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    count = 0\n",
    "    for i in text:\n",
    "        if (i in st):\n",
    "            count = count + 1\n",
    "    return count / len(text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def CountPuncuation(text):\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \";\", \"?\", \":\", \";\"]\n",
    "    count = 0\n",
    "    for i in text:\n",
    "        if (i in st):\n",
    "            count = count + 1\n",
    "    return float(count) / float(len(text))\n",
    "\n",
    "\n",
    "\n",
    "# 기능어 수를 파악, 물론 특수문자 제거\n",
    "# 온라인 메시지 작성자 식별: 쓰기 유형 특성 및 분류 기법을 용\n",
    "\n",
    "def CountFunctionalWords(text):\n",
    "    functional_words = \"\"\"a between in nor some upon\n",
    "    about both including nothing somebody us\n",
    "    above but inside of someone used\n",
    "    after by into off something via\n",
    "    all can is on such we\n",
    "    although cos it once than what\n",
    "    am do its one that whatever\n",
    "    among down latter onto the when\n",
    "    an each less opposite their where\n",
    "    and either like or them whether\n",
    "    another enough little our these which\n",
    "    any every lots outside they while\n",
    "    anybody everybody many over this who\n",
    "    anyone everyone me own those whoever\n",
    "    anything everything more past though whom\n",
    "    are few most per through whose\n",
    "    around following much plenty till will\n",
    "    as for must plus to with\n",
    "    at from my regarding toward within\n",
    "    be have near same towards without\n",
    "    because he need several under worth\n",
    "    before her neither she unless would\n",
    "    behind him no should unlike yes\n",
    "    below i nobody since until you\n",
    "    beside if none so up your\n",
    "    \"\"\"\n",
    "\n",
    "    functional_words = functional_words.split()\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    count = 0\n",
    "\n",
    "    for i in text:\n",
    "        if i in functional_words:\n",
    "            count += 1\n",
    "\n",
    "    return count / len(words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 표현다양성 파악하기 위해 추출해야 값\n",
    "def hapaxLegemena(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    V1 = 0\n",
    "    # dictionary comprehension . har word kay against value 0 kardi\n",
    "    freqs = {key: 0 for key in words}\n",
    "    for word in words: # 단어중 유일한 단어들이 몇개인가 세어서 \n",
    "        freqs[word] += 1\n",
    "    for word in freqs:\n",
    "        if freqs[word] == 1:\n",
    "            V1 += 1\n",
    "    N = len(words)\n",
    "    V = float(len(set(words)))\n",
    "    R = 100 * math.log(N) / max(1, (1 - (V1 / V))) # 독특한 단어의 비율을 확률로 계산\n",
    "    h = V1 / N\n",
    "    return R, h\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#동일한 단어가 얼마나 사용되는지 파악\n",
    "def hapaxDisLegemena(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    count = 0\n",
    "\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    for word in freqs:\n",
    "        if freqs[word] == 2:\n",
    "            count += 1\n",
    "\n",
    "    h = count / float(len(words))\n",
    "    S = count / float(len(set(words)))\n",
    "    return S, h\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#어휘 풍부성 파악\n",
    "def AvgWordFrequencyClass(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    # dictionary comprehension . har word kay against value 0 kardi\n",
    "    freqs = {key: 0 for key in words}\n",
    "    for word in words:\n",
    "        freqs[word] += 1\n",
    "    maximum = float(max(list(freqs.values())))\n",
    "    return np.average([math.floor(math.log((maximum + 1) / (freqs[word]) + 1, 2)) for word in words])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 총 단어를 토큰화하여 중복되지 않는 토큰의 비율 계산\n",
    "def typeTokenRatio(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# logW = V-a/log(N)\n",
    "# N = total words , V = vocabulary richness (unique words) ,  a=0.17\n",
    "# 문장내 다른 단어들을 파악하기 위해서 설정\n",
    "def BrunetsMeasureW(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    a = 0.17\n",
    "    V = float(len(set(words)))\n",
    "    N = len(words)\n",
    "    B = (V - a) / (math.log(N))\n",
    "    return B\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def RemoveSpecialCHs(text):\n",
    "    text = word_tokenize(text)\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "\n",
    "    words = [word for word in text if word not in st]\n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# K  10,000 * (M - N) / N**2\n",
    "# , where M  Sigma i**2 * Vi.\n",
    "def YulesCharacteristicK(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    N = len(words)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    vi = coll.Counter()\n",
    "    vi.update(freqs.values())\n",
    "    M = sum([(value * value) * vi[value] for key, value in freqs.items()])\n",
    "    K = 10000 * (M - N) / math.pow(N, 2)\n",
    "    return K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -1*sigma(pi*lnpi)\n",
    "# Shannon과 Simpsons 다양성 지수 활용\n",
    "def ShannonEntropy(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    lenght = len(words)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    arr = np.array(list(freqs.values()))\n",
    "    distribution = 1. * arr\n",
    "    distribution /= max(1, lenght)\n",
    "    import scipy as sc\n",
    "    H = sc.stats.entropy(distribution, base=2)\n",
    "    # H = sum([(i/lenght)*math.log(i/lenght,math.e) for i in freqs.values()])\n",
    "    return H\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1 - (sigma(n(n - 1))/N(N-1)\n",
    "# N is total number of words\n",
    "# n is the number of each type of word\n",
    "def SimpsonsIndex(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    N = len(words)\n",
    "    n = sum([1.0 * i * (i - 1) for i in freqs.values()])\n",
    "    D = 1 - (n / (N * (N - 1)))\n",
    "    return D\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def FleschReadingEase(text, NoOfsentences):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    l = float(len(words))\n",
    "    scount = 0\n",
    "    for word in words:\n",
    "        scount += syllable_count(word)\n",
    "\n",
    "    I = 206.835 - 1.015 * (l / float(NoOfsentences)) - 84.6 * (scount / float(l))\n",
    "    return I\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def FleschCincadeGradeLevel(text, NoOfSentences):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    scount = 0\n",
    "    for word in words:\n",
    "        scount += syllable_count(word)\n",
    "\n",
    "    l = len(words)\n",
    "    F = 0.39 * (l / NoOfSentences) + 11.8 * (scount / float(l)) - 15.59\n",
    "    return F\n",
    "\n",
    "\n",
    "\n",
    "def dale_chall_readability_formula(text, NoOfSectences):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    difficult = 0\n",
    "    adjusted = 0\n",
    "    NoOfWords = len(words)\n",
    "    #with open('J:\\Django\\EssayFit_Django\\essayfitaiproject\\essayai\\data\\dale-chall.pkl', 'rb') as f:\n",
    "    #with open('/Users/jongphilkim/Desktop/Django_WEB/01_ESSAYFITAI_11-02/essayfitaiproject/essayai/data/dale-chall.pkl', 'rb') as f:\n",
    "    with open('dale-chall.pkl', 'rb') as f:\n",
    "        fimiliarWords = pickle.load(f)\n",
    "    for word in words:\n",
    "        if word not in fimiliarWords:\n",
    "            difficult += 1\n",
    "    percent = (difficult / NoOfWords) * 100\n",
    "    if (percent > 5):\n",
    "        adjusted = 3.6365\n",
    "    D = 0.1579 * (percent) + 0.0496 * (NoOfWords / NoOfSectences) + adjusted\n",
    "    return D\n",
    "\n",
    "\n",
    "\n",
    "def GunningFoxIndex(text, NoOfSentences):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    NoOFWords = float(len(words))\n",
    "    complexWords = 0\n",
    "    for word in words:\n",
    "        if (syllable_count(word) > 2):\n",
    "            complexWords += 1\n",
    "\n",
    "    G = 0.4 * ((NoOFWords / NoOfSentences) + 100 * (complexWords / NoOFWords))\n",
    "    return G\n",
    "\n",
    "\n",
    "def PrepareData(text1, text2, Winsize):\n",
    "    chunks1 = slidingWindow(text1, Winsize, Winsize)\n",
    "    chunks2 = slidingWindow(text2, Winsize, Winsize)\n",
    "    return \" \".join(str(chunk1) + str(chunk2) for chunk1, chunk2 in zip(chunks1, chunks2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 글자의 특징들을 모두 추출하여 백터값을 반환\n",
    "def FeatureExtration(text, winSize, step):\n",
    "    # cmu dictionary for syllables\n",
    "    global cmuDictionary\n",
    "    cmuDictionary = cmudict.dict()\n",
    "\n",
    "    chunks = slidingWindow(text, winSize, step)\n",
    "\n",
    "    # #문장분석 특징 출력\n",
    "    # #평균 단어 길이 등등...\n",
    "    # avgWordLength = []\n",
    "\n",
    "    # #문장수 계산\n",
    "    # avgSentLength = []\n",
    "\n",
    "    # #문장에 단어가 평균적으로 몇개가 있는지 계산\n",
    "    # avgSentLengthByWord = []\n",
    "\n",
    "    # # 불용어 제거하고 단어당 음절 수를 파악하여 가독성 계산에 반영  >> 가독성판단\n",
    "    # avgSyllablePerWord = []\n",
    "\n",
    "    # # 문장에 특수문자가 얼마나 포함되었는지 계산\n",
    "    # count_spcial_chr_nums = []\n",
    "\n",
    "    # # 구두점 수 계산\n",
    "    # cont_punc_numbers = []\n",
    "\n",
    "    # # 문법적 구조를 표현하는데 사용하는 function word의 사용 수\n",
    "    # cont_func_word = []\n",
    "\n",
    "    # # 총 단어를 토큰화하여 중복되지 않는 토큰의 비율 계산\n",
    "    # token_ratio = []\n",
    "\n",
    "    # #표현다양성 파악(전체 단어에서 유일한 단어 사용비율)\n",
    "    # HonoreMeasureR__ = []\n",
    "    # hapax__ = []\n",
    "\n",
    "    # #동일한 단어가 얼마나 사용되는지 파악\n",
    "    # SichelesMeasureS__ = []\n",
    "\n",
    "    # dihapax__ = []\n",
    "\n",
    "    # #Yule's index\n",
    "    # YuleK__ = []\n",
    "\n",
    "    # #Shannon과 Simpsons 다양성 지수 활용\n",
    "    # Shannon__ = []\n",
    "\n",
    "    # # 문장 표현의 다양성 지수로 0으로 가까우면 다양한 표현이 되고, 1에 가까우면 다양하지 않은, 즉 독창적인 단어들로 구성되 있다는 의미\n",
    "    # S__ = []\n",
    "\n",
    "    # # 문장내 다른 단어(unique 단어)들을 파악하기 위해서 설정\n",
    "    # B__ = []\n",
    "\n",
    "    # # 가독성 파악 지수\n",
    "    # FR__ = []\n",
    "\n",
    "    # #The Flesch Reading Ease formula will output a number from 0 to 100 - a higher score indicates easier reading. \n",
    "    # #An average document has a Flesch Reading Ease score between 6 - 70. As a rule of thumb, scores of 90-100 can be understood by an average 5th grader. \n",
    "    # #8th and 9th grade students can understand documents with a score of 60-70; and college graduates can understand documents with a score of 0-30.\n",
    "    # FC__ = []\n",
    "\n",
    "    # # 표현이 다른 특징들 추출하여 수집\n",
    "    # # 점수\t                    설명\n",
    "    # # 4.9 이하\t평균 4 학년 이하 학생이 쉽게 이해할 수 있음\n",
    "    # # 5.0 ~ 5.9\t평균 5 학년 또는 6 학년 학생이 쉽게 이해할 수 있음\n",
    "    # # 6.0 ~ 6.9\t평균 7 학년 또는 8 학년 학생이 쉽게 이해할 수 있음\n",
    "    # # 7.0 ~ 7.9\t평균 9 학년 또는 10 학년 학생이 쉽게 이해할 수 있음\n",
    "    # # 8.0 ~ 8.9\t평균 11 학년 또는 12 학년 학생이 쉽게 이해할 수 있음\n",
    "    # # 9.0 ~ 9.9\t평균 13 ~ 15 학년 (대학) 학생이 쉽게 이해할 수 있음\n",
    "    # D__ = []\n",
    "\n",
    "    # #가독성 분석\n",
    "    # # Fog Index\tReading level by grade\n",
    "    # # 17\t    College graduate\n",
    "    # # 16\t    College senior\n",
    "    # # 15\t    College junior\n",
    "    # # 14\t    College sophomore\n",
    "    # # 13\t    College freshman\n",
    "    # # 12\t    High school senior\n",
    "    # # 11\t    High school junior\n",
    "    # # 10\t    High school sophomore\n",
    "    # # 9\t    High school freshman\n",
    "    # # 8\t    Eighth grade\n",
    "    # # 7\t    Seventh grade\n",
    "    # # 6\t    Sixth grade\n",
    "    # G__ = []\n",
    "\n",
    "    vector = []\n",
    "    # extracted_features =[]\n",
    "    for chunk in chunks:\n",
    "        ###########특징들 별도 저장 후 출력\n",
    "        # avgWordLength = []\n",
    "        # avgSentLength = []\n",
    "        # avgSentLengthByWord = []\n",
    "        # avgSyllablePerWord = []\n",
    "        # count_spcial_chr_nums = []\n",
    "        # cont_punc_numbers = []\n",
    "        # cont_func_word = []\n",
    "        # token_ratio = []\n",
    "        # HonoreMeasureR__ = []\n",
    "        # hapax__ = []\n",
    "        # SichelesMeasureS__ = []\n",
    "        # dihapax__ = []\n",
    "        # YuleK__ = []\n",
    "        # Shannon__ = []\n",
    "        # S__ = []\n",
    "        # B__ = []\n",
    "        # FR__ = []\n",
    "        # FC__ = []\n",
    "        # D__ = []\n",
    "        # G__ = []\n",
    "        ########################\n",
    "        feature = []\n",
    "\n",
    "        # LEXICAL 특징들\n",
    "\n",
    "        # meanwl = (Avg_wordLength(chunk))\n",
    "        # feature.append(meanwl)\n",
    "        # # avgWordLength.append(meanwl)\n",
    "\n",
    "        # meansl = (Avg_SentLenghtByCh(chunk))\n",
    "        # feature.append(meansl)\n",
    "        # # avgSentLength.append(meansl)\n",
    "\n",
    "        # 문장에 단어가 평균적으로 몇개가 있는지 계산\n",
    "        # mean = (Avg_SentLenghtByWord(chunk))\n",
    "        # feature.append(mean)\n",
    "        # # avgSentLengthByWord.append(mean)\n",
    "\n",
    "        # #불용어 제거하고 단어당 음절 수를 파악하여 가독성 계산에 반영\n",
    "        # meanSyllable = Avg_Syllable_per_Word(chunk)\n",
    "        # feature.append(meanSyllable)\n",
    "        # avgSyllablePerWord.append(meanSyllable)\n",
    "\n",
    "        # means = CountSpecialCharacter(chunk)\n",
    "        # feature.append(means)\n",
    "        # count_spcial_chr_nums.append(means)\n",
    "\n",
    "        # p = CountPuncuation(chunk)\n",
    "        # feature.append(p)\n",
    "        # cont_punc_numbers.append(p)\n",
    "\n",
    "        # f = CountFunctionalWords(text)\n",
    "        # feature.append(f)\n",
    "        # # cont_func_word.append(f)\n",
    "\n",
    "        # VOCABULARY 풍부성 특징들 파악\n",
    "        # 총 단어를 토큰화하여 중복되지 않는 토큰의 비율 계산\n",
    "        TTratio = round(typeTokenRatio(chunk), 2)\n",
    "        feature.append(TTratio)\n",
    "        print(\"TTratio 표현다양성: \", TTratio)\n",
    "        # token_ratio.append(TTratio)\n",
    "\n",
    "        # 표현다양성 파악(전체 단어에서 유일한 단어 사용비율)\n",
    "        HonoreMeasureR, hapax = hapaxLegemena(chunk)\n",
    "        feature.append(HonoreMeasureR)\n",
    "        print('HonoreMeasureR 표현다양성 :', HonoreMeasureR)\n",
    "        feature.append(hapax)\n",
    "        print('hapax 표현다양성 :', hapax )\n",
    "        # hapax__.append(hapax)\n",
    "        # HonoreMeasureR__.append(HonoreMeasureR)\n",
    "\n",
    "        # SichelesMeasureS, dihapax = hapaxDisLegemena(chunk)\n",
    "        # feature.append(dihapax)\n",
    "        # feature.append(SichelesMeasureS)\n",
    "        # # SichelesMeasureS__.append(SichelesMeasureS)\n",
    "        # dihapax__.append(dihapax)\n",
    "\n",
    "        #다양성 지수 분석\n",
    "        YuleK = round(YulesCharacteristicK(chunk),2)\n",
    "        feature.append(YuleK)\n",
    "        print('YuleK 다양성지수:',YuleK )\n",
    "        # YuleK__.append(YuleK)\n",
    "\n",
    "        # Shannon과 Simpsons 다양성 지수 활용\n",
    "        S = round(SimpsonsIndex(chunk), 2)\n",
    "        feature.append(S)\n",
    "        print('SimpsonIndex 다양성지수', S)\n",
    "        # S__.append(S)\n",
    "\n",
    "        # B = BrunetsMeasureW(chunk)\n",
    "        # feature.append(B)\n",
    "        # # B__.append(B)\n",
    "\n",
    "        Shannon = round(ShannonEntropy(text),2)\n",
    "        feature.append(Shannon)\n",
    "        # Shannon__.append(Shannon)\n",
    "        print('Shannon 다양성지수:', Shannon)\n",
    "\n",
    "        # 가독성\n",
    "        FR = round(FleschReadingEase(chunk, winSize),2)\n",
    "        feature.append(FR)\n",
    "        print(\"FR_readerablity :\", FR)\n",
    "        # FR__.append(FR)\n",
    "\n",
    "        # FC = round(FleschCincadeGradeLevel(chunk, winSize),2)\n",
    "        # feature.append(FC)\n",
    "        # # FC__.append(FC)\n",
    "\n",
    "        # # 표현이 다른 특징들 추출하여 수집\n",
    "        # D = dale_chall_readability_formula(chunk, winSize)\n",
    "        # feature.append(D)\n",
    "        # # D__.append(D)\n",
    "\n",
    "        # # 다른 특징들 추출하여 수집\n",
    "        # G = GunningFoxIndex(chunk, winSize)\n",
    "        # feature.append(G)\n",
    "        # # G__.append(G)\n",
    "\n",
    "        vector.append(feature)\n",
    "\n",
    "    return vector\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ELBOW METHOD - 분류값을 계산하기 위해서 적용, 현재는 분류하지 않음. 1개의 에세이만 분석할 것\n",
    "# def ElbowMethod(data):\n",
    "#     X = data  # <your_data>\n",
    "#     distorsions = []\n",
    "#     for k in range(1, 10): #최대 10개의 군집까지 분류할 수 있도록\n",
    "#         kmeans = KMeans(n_clusters=k)\n",
    "#         kmeans.fit(X)\n",
    "#         distorsions.append(kmeans.inertia_)\n",
    "\n",
    "#     fig = plt.figure(figsize=(15, 5))\n",
    "#     plt.plot(range(1, 10), distorsions, 'bo-')\n",
    "#     plt.grid(True)\n",
    "#     plt.ylabel(\"Square Root Error\")\n",
    "#     plt.xlabel(\"Number of Clusters\")\n",
    "#     plt.title('Elbow curve')\n",
    "#     plt.savefig(\"ElbowCurve.png\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# ANALYSIS PART\n",
    "\n",
    "# 엘보우 그래프를 이용해서 몇개의 k 값을 넣을 것이지 파악하여 계산에 적용, 현재는 1개의 에세이 스타일만 파악할 것임\n",
    "# # 1000의 에세이를 파악하여 공통점을 찾을 계획임\n",
    "# def Analysis(vector, K=1):\n",
    "#     arr = (np.array(vector))\n",
    "#     # mean normalization of the data . converting into normal distribution having mean=0 , -0.1<x<0.1\n",
    "#     sc = StandardScaler()\n",
    "#     x = sc.fit_transform(arr)\n",
    "\n",
    "#     # Breaking into principle components\n",
    "#     pca = PCA(n_components=2)\n",
    "#     components = (pca.fit_transform(x))\n",
    "#     # Applying kmeans algorithm for finding centroids\n",
    "\n",
    "#     kmeans = KMeans(n_clusters=K, n_jobs=-1)\n",
    "#     kmeans.fit_transform(components)\n",
    "#     #print(\"labels: \", kmeans.labels_)\n",
    "#     centers = kmeans.cluster_centers_\n",
    "\n",
    "#     # lables are assigned by the algorithm if 2 clusters then lables would be 0 or 1\n",
    "#     lables = kmeans.labels_\n",
    "#     colors = [\"r.\", \"g.\", \"b.\", \"y.\", \"c.\"]\n",
    "#     colors = colors[:K + 1]\n",
    "\n",
    "#     return components\n",
    "\n",
    "#     # for i in range(len(components)):\n",
    "#     #     plt.plot(components[i][0], components[i][1], colors[lables[i]], markersize=10)\n",
    "\n",
    "#     # plt.scatter(centers[:, 0], centers[:, 1], marker=\"x\", s=150, linewidths=10, zorder=15)\n",
    "#     # plt.xlabel(\"1st Principle Component\")\n",
    "#     # plt.ylabel(\"2nd Principle Component\")\n",
    "#     # title = \"Styles Clusters\"\n",
    "#     # plt.title(title)\n",
    "#     # plt.savefig(\"Results\" + \".png\")\n",
    "#     # plt.show()\n",
    "\n",
    "\n",
    "##################################\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     text = open(\"/Users/kimkwangil/Documents/001_ESSAYFITAI/01_WEB_2020-10-17/essayfitaiproject/essayai/data/essay_sample.txt\").read()\n",
    "\n",
    "#     vector = FeatureExtration(text, winSize=10, step=10)\n",
    "#     #ElbowMethod(np.array(vector)) 사용하지 않을 것임(이미 적용함 K=1)\n",
    "#     Analysis(vector)\n",
    "\n",
    "##################################\n",
    "# 모든 분석 항목을 개별 결과로 추출하여 리스트에 담고 이것을 테이블로 result_all.html 에 구현할것\n",
    "\n",
    "### [\n",
    "#     [5.417910447761194, 62.0, 10.9, 1.7761194029850746, 0.0, 0.03656597774244833, 0.5493133583021224, 0.6946564885496184, 0.6607142857142857, 471.8498871295094, 0.05357142857142857, 0.06976744186046512, 4529.655612244898, 0.9911518661518661, 18.190107138129314, 7.9451896491259655, 66.30092857142861, 6.794071428571428, 9.549341428571429, 9.48], [5.21875, 49.2, 8.6, 1.625, 0.0, 0.03792415169660679, 0.5493133583021224, 0.6601941747572816, 0.6022727272727273, 447.7336814478207, 0.07954545454545454, 0.1076923076923077, 3959.194214876033, 0.9879832810867294, 14.479589695008315, 7.9451896491259655, 71.96436363636367, 5.40790909090909, 10.711957272727274, 6.247272727272727], \n",
    "#     [5.390625, 57.0, 9.9, 1.703125, 0.0, 0.0535405872193437, 0.5493133583021224, 0.6935483870967742, 0.6831683168316832, 461.512051684126, 0.06930693069306931, 0.0875, 4879.913733947652, 0.9914851485148515, 17.297489785735493, 7.9451896491259655, 73.4528069306931, 5.523257425742575, 9.452905544554456, 8.000396039603961], \n",
    "#     [5.579710144927536, 125.5, 22.0, 1.7826086956521738, 0.0, 0.03322784810126582, 0.5493133583021224, 0.6126482213438735, 0.5223214285714286, 541.164605185504, 0.08035714285714286, 0.12080536912751678, 3124.6014030612246, 0.9906710442024343, 27.501798634628567, 7.9451896491259655, 57.5766785714286, 10.793321428571428, 9.75240607142857, 13.96], \n",
    "#     [5.623762376237623, 97.0, 17.0, 1.7722772277227723, 0.0, 0.03575076608784474, 0.5493133583021224, 0.655, 0.5885714285714285, 516.4785973923515, 0.09714285714285714, 0.13385826771653545, 3934.6938775510203, 0.9919868637110016, 24.556680691194547, 7.9451896491259655, 64.34792857142861, 8.631571428571426, 9.46707142857143, 11.114285714285714], [5.426229508196721, 54.5, 10.0, 1.7704918032786885, 0.0, 0.04332129963898917, 0.5493133583021224, 0.6446280991735537, 0.6039603960396039, 461.512051684126, 0.06930693069306931, 0.0945945945945946, 3911.3812371336144, 0.9889108910891089, 15.997415393722303, 7.9451896491259655, 70.10231188118814, 5.990584158415842, 9.29656891089109, 8.792475247524752]\n",
    "###  ]\n",
    "\n",
    "# def ext_each_features(text):\n",
    "#     my_posts = str(text)\n",
    "#     ext_features_re = FeatureExtration(text, winSize=10, step=10)\n",
    "    \n",
    "#     return ext_each_features\n",
    "\n",
    "\n",
    "# def writingstyle(text):\n",
    "#     my_posts = str(text)\n",
    "#     vector = FeatureExtration(text, winSize=10, step=10)\n",
    "#     #ElbowMethod(np.array(vector)) 사용하지 않을 것임(이미 적용함 K=1)\n",
    "#     result = Analysis(vector)\n",
    "#     return result\n",
    "\n",
    "\n",
    "# def writestyleResult(input_text__) :\n",
    "\n",
    "#     #input_text__ = \"\"\"A window into the soul.For most people, this would be the eyes. The eyes cannot lie; they often tell more about a person's emotions than their words. What distinguishes a fake smile from a genuine one? The eyes. What shows sadness? The eyes. What gives away a liar? The eyes.But are the eyes the only window into the soul?Recently, I began painting with watercolors. With watercolors, there is no turning back: if one section is too dark, it is nearly impossible to lighten the area again. Every stroke must be done purposefully, every color mixed to its exact value.I laid my materials before me, preparing myself for the worst. I checked my list of supplies, making sure my setup was perfect.I wet my brush, dipped it into some yellow ochre, and dabbed off the excess paint. Too little water on my brush. I dipped my brush back into my trusty water jar; the colors swirled beautifully, forming an abstract art piece before my eyes. \\u2014It's a shame that I couldn't appreciate it.I continued mixing colors to their exact value. More alizarin crimson. More water. More yellow ochre. Less water. More phthalo blue. The cycle continued. Eventually, I was satisfied. The colors looked good, there was enough contrast between facial features, and the watercolors stayed inside the lines.Craving feedback, I posted my art to Snapchat. I got a few messages such as 'wow' and 'pretty,' but one message stood out. 'You were anxious with this one, huh? Anyways, love the hair!'I was caught off guard. Was it a lucky guess? Did they know something I didn't? I immediately responded: 'Haha, how could you tell?' No response.What I didn't know at the time was that my response would come a few months later while babysitting. Since the girl I was babysitting loved art, I took out some Crayola watercolors and some watercolor paper for her to play with. After I went to the bathroom and came back, the watercolors were doused with water. 'You were impatient with this one, huh? Anyways, love the little dog you drew!'The little girl looked up at me, confused. 'How could you tell?' 'You used a lot of water for a brighter color, but you couldn't wait for it to slowly soak in.''Oh.'Now, I would be lying if I said I realized the connection between the two events immediately.Instead, I made the connection when I decided to sit down one day and objectively critique my art. The piece that I once loved now seemed like a nervous wreck: the paper was overworked, the brushstrokes were undecided, the facial features blended together, and each drop of water was bound inside the lines as if it was a prisoner in a cage.From then on, I started noticing pieces of personality in additional creations surrounding me: website designs, solutions to math problems, code written for class, and even the preparation of a meal.When I peer around at people's projects during Code Club, I notice the clear differences between their code. Some people break it up by commenting in every possible section. Others breeze through the project, not caring to comment or organize their code. I could also see clear differences in personalities when our club members began coding the Arduino for the first time. Some followed the tutorials to the letter, while others immediately started experimenting with different colored LEDs and ways of wiring the circuit.It became clear to me that, as humans, we leave pieces of our souls in everything we do, more than we intend to. If we entertain this thought, perhaps the key to better understanding others around us is simply noticing the subtler clues under our noses?Perhaps there are endless windows to the soul, and we simply need to peer through them. I shakily rose my hand. 'We should create workshops of our own,' I suggested.I got a few strange looks. 'It's a good idea, but it's too much work.' 'We just don't have enough free time to make it work.' 'Maybe we could, but I don't know how to make workshops.' My suggestion was shot down. I shuffled in my seat. 'I could make them.' A few people stared at me in disbelief. I glanced over at the club advisor, Mr. C, nervous to hear his response.'If you're willing to take on the work, we can try it.' Mr. C replied. And so I embarked on my quest. I researched different workshops on the internet, learning the information myself at first. Then, I transitioned into creating workshops of my own, making sure that the information was easy to understand for even a beginner. I was exhausted; my first workshop took 16 cumulative hours to create.\"\"\"\n",
    "#     #input_text__ = open(\"/Users/kimkwangil/Documents/001_ESSAYFITAI/01_WEB_2020-10-17/essayfitaiproject/essayai/data/essay_sample.txt\").read()\n",
    "#     print(\"input_text__:\", input_text__)\n",
    "#     result__ = writingstyle(input_text__)\n",
    "#     print (result__)\n",
    "#     print (type(result__)) #numpy array\n",
    "\n",
    "#     return result__\n",
    "\n",
    "# result__  이것으로 그래프를 그리게 되는 거임\n",
    "\n",
    "# [[-1.53070217 -1.96372503]\n",
    "#  [-2.97194764  3.56488797]\n",
    "#  [-3.16646339 -1.8099431 ]\n",
    "#  [ 5.75572062  0.63072175]\n",
    "#  [ 3.17393929 -0.5196705 ]\n",
    "#  [-1.2605467   0.0977289 ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_input = \"\"\"A window into the soul.For most people, this would be the eyes. The eyes cannot lie; they often tell more about a person's emotions than their words. What distinguishes a fake smile from a genuine one? The eyes. What shows sadness? The eyes. What gives away a liar? The eyes.But are the eyes the only window into the soul?Recently, I began painting with watercolors. With watercolors, there is no turning back: if one section is too dark, it is nearly impossible to lighten the area again. Every stroke must be done purposefully, every color mixed to its exact value.I laid my materials before me, preparing myself for the worst. I checked my list of supplies, making sure my setup was perfect.I wet my brush, dipped it into some yellow ochre, and dabbed off the excess paint. Too little water on my brush. I dipped my brush back into my trusty water jar; the colors swirled beautifully, forming an abstract art piece before my eyes. \\u2014It's a shame that I couldn't appreciate it.I continued mixing colors to their exact value. More alizarin crimson. More water. More yellow ochre. Less water. More phthalo blue. The cycle continued. Eventually, I was satisfied. The colors looked good, there was enough contrast between facial features, and the watercolors stayed inside the lines.Craving feedback, I posted my art to Snapchat. I got a few messages such as 'wow' and 'pretty,' but one message stood out. 'You were anxious with this one, huh? Anyways, love the hair!'I was caught off guard. Was it a lucky guess? Did they know something I didn't? I immediately responded: 'Haha, how could you tell?' No response.What I didn't know at the time was that my response would come a few months later while babysitting. Since the girl I was babysitting loved art, I took out some Crayola watercolors and some watercolor paper for her to play with. After I went to the bathroom and came back, the watercolors were doused with water. 'You were impatient with this one, huh? Anyways, love the little dog you drew!'The little girl looked up at me, confused. 'How could you tell?' 'You used a lot of water for a brighter color, but you couldn't wait for it to slowly soak in.''Oh.'Now, I would be lying if I said I realized the connection between the two events immediately.Instead, I made the connection when I decided to sit down one day and objectively critique my art. The piece that I once loved now seemed like a nervous wreck: the paper was overworked, the brushstrokes were undecided, the facial features blended together, and each drop of water was bound inside the lines as if it was a prisoner in a cage.From then on, I started noticing pieces of personality in additional creations surrounding me: website designs, solutions to math problems, code written for class, and even the preparation of a meal.When I peer around at people's projects during Code Club, I notice the clear differences between their code. Some people break it up by commenting in every possible section. Others breeze through the project, not caring to comment or organize their code. I could also see clear differences in personalities when our club members began coding the Arduino for the first time. Some followed the tutorials to the letter, while others immediately started experimenting with different colored LEDs and ways of wiring the circuit.It became clear to me that, as humans, we leave pieces of our souls in everything we do, more than we intend to. If we entertain this thought, perhaps the key to better understanding others around us is simply noticing the subtler clues under our noses?Perhaps there are endless windows to the soul, and we simply need to peer through them. I shakily rose my hand. 'We should create workshops of our own,' I suggested.I got a few strange looks. 'It's a good idea, but it's too much work.' 'We just don't have enough free time to make it work.' 'Maybe we could, but I don't know how to make workshops.' My suggestion was shot down. I shuffled in my seat. 'I could make them.' A few people stared at me in disbelief. I glanced over at the club advisor, Mr. C, nervous to hear his response.'If you're willing to take on the work, we can try it.' Mr. C replied. And so I embarked on my quest. I researched different workshops on the internet, learning the information myself at first. Then, I transitioned into creating workshops of my own, making sure that the information was easy to understand for even a beginner. I was exhausted; my first workshop took 16 cumulative hours to create.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slideingWinows it : <str_iterator object at 0x7fdc9976dfd0>\n",
      "TTratio 표현다양성:  0.69\n",
      "HonoreMeasureR 표현다양성 : 471.8498871295094\n",
      "hapax 표현다양성 : 0.6607142857142857\n",
      "YuleK 다양성지수: 4529.66\n",
      "SimpsonIndex 다양성지수 0.99\n",
      "Shannon 다양성지수: 7.95\n",
      "vowels numbers: 2\n",
      "vowels numbers: 2\n",
      "vowels numbers: 3\n",
      "FR_readerablity : 66.3\n",
      "TTratio 표현다양성:  0.66\n",
      "HonoreMeasureR 표현다양성 : 447.7336814478207\n",
      "hapax 표현다양성 : 0.6022727272727273\n",
      "YuleK 다양성지수: 3959.19\n",
      "SimpsonIndex 다양성지수 0.99\n",
      "Shannon 다양성지수: 7.95\n",
      "vowels numbers: 3\n",
      "vowels numbers: 2\n",
      "vowels numbers: 1\n",
      "vowels numbers: 1\n",
      "vowels numbers: 2\n",
      "vowels numbers: 4\n",
      "vowels numbers: 2\n",
      "FR_readerablity : 71.96\n",
      "TTratio 표현다양성:  0.69\n",
      "HonoreMeasureR 표현다양성 : 461.512051684126\n",
      "hapax 표현다양성 : 0.6831683168316832\n",
      "YuleK 다양성지수: 4879.91\n",
      "SimpsonIndex 다양성지수 0.99\n",
      "Shannon 다양성지수: 7.95\n",
      "vowels numbers: 4\n",
      "vowels numbers: 2\n",
      "vowels numbers: 1\n",
      "vowels numbers: 2\n",
      "vowels numbers: 1\n",
      "vowels numbers: 1\n",
      "vowels numbers: 2\n",
      "vowels numbers: 4\n",
      "vowels numbers: 1\n",
      "FR_readerablity : 73.45\n",
      "TTratio 표현다양성:  0.61\n",
      "HonoreMeasureR 표현다양성 : 541.164605185504\n",
      "hapax 표현다양성 : 0.5223214285714286\n",
      "YuleK 다양성지수: 3124.6\n",
      "SimpsonIndex 다양성지수 0.99\n",
      "Shannon 다양성지수: 7.95\n",
      "vowels numbers: 2\n",
      "vowels numbers: 1\n",
      "vowels numbers: 1\n",
      "vowels numbers: 1\n",
      "vowels numbers: 1\n",
      "vowels numbers: 1\n",
      "vowels numbers: 1\n",
      "vowels numbers: 1\n",
      "vowels numbers: 7\n",
      "vowels numbers: 3\n",
      "vowels numbers: 3\n",
      "vowels numbers: 2\n",
      "FR_readerablity : 57.58\n",
      "TTratio 표현다양성:  0.66\n",
      "HonoreMeasureR 표현다양성 : 516.4785973923515\n",
      "hapax 표현다양성 : 0.5885714285714285\n",
      "YuleK 다양성지수: 3934.69\n",
      "SimpsonIndex 다양성지수 0.99\n",
      "Shannon 다양성지수: 7.95\n",
      "vowels numbers: 3\n",
      "vowels numbers: 1\n",
      "vowels numbers: 3\n",
      "vowels numbers: 3\n",
      "vowels numbers: 1\n",
      "vowels numbers: 4\n",
      "vowels numbers: 1\n",
      "vowels numbers: 1\n",
      "vowels numbers: 1\n",
      "vowels numbers: 1\n",
      "vowels numbers: 1\n",
      "FR_readerablity : 64.35\n",
      "TTratio 표현다양성:  0.64\n",
      "HonoreMeasureR 표현다양성 : 461.512051684126\n",
      "hapax 표현다양성 : 0.6039603960396039\n",
      "YuleK 다양성지수: 3911.38\n",
      "SimpsonIndex 다양성지수 0.99\n",
      "Shannon 다양성지수: 7.95\n",
      "vowels numbers: 1\n",
      "vowels numbers: 1\n",
      "vowels numbers: 4\n",
      "vowels numbers: 1\n",
      "FR_readerablity : 70.1\n",
      "type:>>>>> <class 'list'>\n",
      "extracted_features_result [[0.69, 471.8498871295094, 0.6607142857142857, 4529.66, 0.99, 7.95, 66.3], [0.66, 447.7336814478207, 0.6022727272727273, 3959.19, 0.99, 7.95, 71.96], [0.69, 461.512051684126, 0.6831683168316832, 4879.91, 0.99, 7.95, 73.45], [0.61, 541.164605185504, 0.5223214285714286, 3124.6, 0.99, 7.95, 57.58], [0.66, 516.4785973923515, 0.5885714285714285, 3934.69, 0.99, 7.95, 64.35], [0.64, 461.512051684126, 0.6039603960396039, 3911.38, 0.99, 7.95, 70.1]]\n"
     ]
    }
   ],
   "source": [
    "extracted_features_result = FeatureExtration(text_input, winSize=10, step=10) #특징들을 모두 추출하여 출력할 거임\n",
    "print(\"type:>>>>>\",type(extracted_features_result))\n",
    "print ('extracted_features_result', extracted_features_result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TTratio_WD</th>\n",
       "      <th>HonoreMeasureR_WD</th>\n",
       "      <th>hapax_WD</th>\n",
       "      <th>YuleK_WD</th>\n",
       "      <th>SimpsonIndex_WD</th>\n",
       "      <th>Shannon_WD</th>\n",
       "      <th>FR_readerablity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.69</td>\n",
       "      <td>471.849887</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>4529.66</td>\n",
       "      <td>0.99</td>\n",
       "      <td>7.95</td>\n",
       "      <td>66.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.66</td>\n",
       "      <td>447.733681</td>\n",
       "      <td>0.602273</td>\n",
       "      <td>3959.19</td>\n",
       "      <td>0.99</td>\n",
       "      <td>7.95</td>\n",
       "      <td>71.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.69</td>\n",
       "      <td>461.512052</td>\n",
       "      <td>0.683168</td>\n",
       "      <td>4879.91</td>\n",
       "      <td>0.99</td>\n",
       "      <td>7.95</td>\n",
       "      <td>73.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.61</td>\n",
       "      <td>541.164605</td>\n",
       "      <td>0.522321</td>\n",
       "      <td>3124.60</td>\n",
       "      <td>0.99</td>\n",
       "      <td>7.95</td>\n",
       "      <td>57.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.66</td>\n",
       "      <td>516.478597</td>\n",
       "      <td>0.588571</td>\n",
       "      <td>3934.69</td>\n",
       "      <td>0.99</td>\n",
       "      <td>7.95</td>\n",
       "      <td>64.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.64</td>\n",
       "      <td>461.512052</td>\n",
       "      <td>0.603960</td>\n",
       "      <td>3911.38</td>\n",
       "      <td>0.99</td>\n",
       "      <td>7.95</td>\n",
       "      <td>70.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TTratio_WD  HonoreMeasureR_WD  hapax_WD  YuleK_WD  SimpsonIndex_WD  \\\n",
       "0        0.69         471.849887  0.660714   4529.66             0.99   \n",
       "1        0.66         447.733681  0.602273   3959.19             0.99   \n",
       "2        0.69         461.512052  0.683168   4879.91             0.99   \n",
       "3        0.61         541.164605  0.522321   3124.60             0.99   \n",
       "4        0.66         516.478597  0.588571   3934.69             0.99   \n",
       "5        0.64         461.512052  0.603960   3911.38             0.99   \n",
       "\n",
       "   Shannon_WD  FR_readerablity  \n",
       "0        7.95            66.30  \n",
       "1        7.95            71.96  \n",
       "2        7.95            73.45  \n",
       "3        7.95            57.58  \n",
       "4        7.95            64.35  \n",
       "5        7.95            70.10  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(extracted_features_result, columns = ['TTratio_WD', 'HonoreMeasureR_WD', 'hapax_WD', 'YuleK_WD', 'SimpsonIndex_WD', 'Shannon_WD', 'FR_readerablity'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.83333333333333"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#컬럼 합산 평균값 계산하기\n",
    "TTratio_WD_mean = df['TTratio_WD'].mean() * 100\n",
    "TTratio_WD_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.337514575390635"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HonoreMeasureR_WD_mean = df['HonoreMeasureR_WD'].mean() * 0.1\n",
    "HonoreMeasureR_WD_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.01680971668596"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hapax_WD_mean = df['hapax_WD'].mean() * 100\n",
    "hapax_WD_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.56571666666667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YuleK_WD_mean = df['YuleK_WD'].mean() * 0.01\n",
    "HonoreMeasureR_WD_mean\n",
    "YuleK_WD_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.337514575390635"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HonoreMeasureR_WD_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.00000000000001"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SimpsonIndex_WD_mean = df['SimpsonIndex_WD'].mean() * 100\n",
    "SimpsonIndex_WD_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Shannon_WD_mean = df['Shannon_WD'].mean() * 10\n",
    "Shannon_WD_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.29"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가독성\n",
    "FR_readerablity_mean = df['FR_readerablity'].mean()\n",
    "FR_readerablity_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.22726983820961"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Writing Diversity ratio\n",
    "wri_div_mean = (TTratio_WD_mean + HonoreMeasureR_WD_mean + \n",
    "                hapax_WD_mean + YuleK_WD_mean + HonoreMeasureR_WD_mean + \n",
    "                SimpsonIndex_WD_mean + Shannon_WD_mean)/7 \n",
    "\n",
    "wri_div_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_ 20\n",
      "max_:  80\n",
      "div_: 20\n"
     ]
    }
   ],
   "source": [
    "one_ps_char_desc = wri_div_mean  # 이하 -13 ~ +17 이상 범위로 Lacking , Ideal, Overboard \n",
    "ideal_mean = 50 # 1000명의 평균값 (writing diversity)\n",
    "\n",
    "\n",
    "min_ = int(ideal_mean-ideal_mean*0.6)\n",
    "print('min_', min_)\n",
    "max_ = int(ideal_mean+ideal_mean*0.6)\n",
    "print('max_: ', max_)\n",
    "div_ = int(((ideal_mean+ideal_mean*0.6)-(ideal_mean-ideal_mean*0.6))/3)\n",
    "print('div_:', div_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cal_abs 절대값 : 13.227269838209608\n",
      "compare : 16.17532426260137\n",
      "Ideal\n"
     ]
    }
   ],
   "source": [
    "cal_abs = abs(ideal_mean - one_ps_char_desc) # 개인 - 단체 값의 절대값계산\n",
    "\n",
    "print('cal_abs 절대값 :', cal_abs)\n",
    "compare = (one_ps_char_desc + ideal_mean)/7\n",
    "print('compare :', compare)\n",
    "\n",
    "if one_ps_char_desc > ideal_mean: # 개인점수가 평균보다 클 경우는 overboard\n",
    "    if cal_abs > compare: # 개인점수가 개인평균차의 절대값보다 클 경우, 즉 차이가 많이 날경우\n",
    "        print(\"Overboard\")\n",
    "    else: #차이가 많이 안나면\n",
    "        print(\"Ideal\")\n",
    "        \n",
    "    \n",
    "elif one_ps_char_desc < ideal_mean: # 개인점수가 평균보다 작을 경우 lacking\n",
    "    if cal_abs > compare: #차이가 많이나면 # 개인점수가  평균보다 작을 경우 Lacking이고 \n",
    "        print(\"Lacking\")\n",
    "    else: #차이가 많이 안나면\n",
    "        print (\"Ideal\")\n",
    "        \n",
    "else:\n",
    "    print(\"Ideal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
