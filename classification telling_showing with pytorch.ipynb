{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification showing and telling with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The house was creepy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I heard footsteps creeping behind me and it ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>She was my best friend. I could tell her almos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>She hated it there because it smelled bad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>When they embraced she could tell he had been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>2</td>\n",
       "      <td>Her hand reached for the massive, iron door ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>2</td>\n",
       "      <td>The way the door decisively slammed behind her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>2</td>\n",
       "      <td>Dust coated every last surface. He ran his fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2</td>\n",
       "      <td>The lime green patio umbrella flapped happily ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>2</td>\n",
       "      <td>\"And after all the weather was ideal. They cou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>258 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                               text\n",
       "index                                                         \n",
       "0         1                              The house was creepy.\n",
       "1         1  I heard footsteps creeping behind me and it ma...\n",
       "2         1  She was my best friend. I could tell her almos...\n",
       "3         1         She hated it there because it smelled bad.\n",
       "4         1  When they embraced she could tell he had been ...\n",
       "...     ...                                                ...\n",
       "253       2  Her hand reached for the massive, iron door ha...\n",
       "254       2  The way the door decisively slammed behind her...\n",
       "255       2  Dust coated every last surface. He ran his fin...\n",
       "256       2  The lime green patio umbrella flapped happily ...\n",
       "257       2  \"And after all the weather was ideal. They cou...\n",
       "\n",
       "[258 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('data/showingTelling_csv.csv', delimiter = ',')\n",
    "\n",
    "data.index.name = \"index\"\n",
    "data.columns = [\"type\", \"text\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "def shuffle(df, n=1, axis=0): #데이터가 1111 0000이라 잘 섞어주자. \n",
    "    df = data.copy()\n",
    "    for _ in range(n):\n",
    "        df.apply(np.random.shuffle, axis=axis)\n",
    "    return df\n",
    "\n",
    "shuffle(data)\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>The weather was bad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2</td>\n",
       "      <td>The lime green patio umbrella flapped happily ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2</td>\n",
       "      <td>Archie scrabbling up the stairs, as usual curs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>Jim was so angry that Blair was afraid.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2</td>\n",
       "      <td>Whenever I am scheduled to give a speech, I su...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                               text\n",
       "index                                                         \n",
       "66        1                               The weather was bad.\n",
       "256       2  The lime green patio umbrella flapped happily ...\n",
       "167       2  Archie scrabbling up the stairs, as usual curs...\n",
       "65        1            Jim was so angry that Blair was afraid.\n",
       "147       2  Whenever I am scheduled to give a speech, I su..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head() # 잘 섞였군만! 하지만 데이터 전처리가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2</td>\n",
       "      <td>Old Mr Chan used a tissue to wipe the sweat fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2</td>\n",
       "      <td>championships, sings lead vocals in a rock ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>The father went to the hoghouse to kill the sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>I saw clouds and lightning from the sea. I was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2</td>\n",
       "      <td>The flowers in the front garden were long dead...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                               text\n",
       "index                                                         \n",
       "129       2  Old Mr Chan used a tissue to wipe the sweat fr...\n",
       "125       2  championships, sings lead vocals in a rock ban...\n",
       "41        1  The father went to the hoghouse to kill the sm...\n",
       "98        1  I saw clouds and lightning from the sea. I was...\n",
       "227       2  The flowers in the front garden were long dead..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas\n",
    "import numpy\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(review, remove_stopwords=False):\n",
    "        \n",
    "    # 영어가 아닌 특수문자들을 공백(\" \")으로 바꾸기\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review)\n",
    "\n",
    "    # 대문자들을 소문자로 바꾸고 공백단위로 텍스트들 나눠서 리스트로 만든다.\n",
    "    words = review_text.lower().split()\n",
    "\n",
    "    if remove_stopwords: \n",
    "        # 불용어들을 제거\n",
    "    \n",
    "        #영어에 관련된 불용어 불러오기\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        # 불용어가 아닌 단어들로 이루어진 새로운 리스트 생성\n",
    "        words = [w for w in words if not w in stops]\n",
    "        # 단어 리스트를 공백을 넣어서 하나의 글로 합친다.\n",
    "        clean_review = ' '.join(words)\n",
    "\n",
    "    else: # 불용어 제거하지 않을 때\n",
    "        clean_review = ' '.join(words)\n",
    "\n",
    "    return clean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather bad'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_ = []\n",
    "for review in train['text']:\n",
    "    clean_train_.append(preprocessing(review, remove_stopwords=True))\n",
    "\n",
    "# 전처리된 데이터 확인. 잘됨 !! ㅎ\n",
    "clean_train_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weather bad',\n",
       " 'lime green patio umbrella flapped happily breeze covered strawberry slushies watermelon pies bright cheerful stacks donuts emily stepped outside feature crystal pitcher pink lemonade spray warm cookies center table favorite summer dress knew soon stepped patio gate life would never',\n",
       " 'archie scrabbling stairs usual cursing blinding wilting weight boxes clara could carry two three time without effort clara taking break squinting warm may sunshine trying get bearings peeled little purple vest leaned front gate kind place thing see ft sure',\n",
       " 'jim angry blair afraid',\n",
       " 'whenever scheduled give speech suffer wet clammy hands']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'old mr chan used tissue wipe sweat face got ready'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_ = []\n",
    "for review in test['text']:\n",
    "    clean_test_.append(preprocessing(review, remove_stopwords=True))\n",
    "    \n",
    "# 전처리된 데이터 확인. 잘됨 !! ㅎ\n",
    "clean_test_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['old mr chan used tissue wipe sweat face got ready',\n",
       " 'championships sings lead vocals rock band speaks five languages',\n",
       " 'father went hoghouse kill smallest pig born last night fern ft want father kill',\n",
       " 'saw clouds lightning sea nervous upcoming storm avoided rain canopy ft see anyone streets filled bad smell sewers darkness',\n",
       " 'flowers front garden long dead grass knee high paint flaking window frames pushed open front door rotten smell hit patches damp mold crept walls took one step forward stepping onto uneven creaky floorboard']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\n",
      "66     1\n",
      "256    2\n",
      "167    2\n",
      "65     1\n",
      "147    2\n",
      "      ..\n",
      "113    2\n",
      "101    1\n",
      "5      1\n",
      "186    2\n",
      "78     1\n",
      "Name: type, Length: 206, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train[\"type\"]) #showing telling 컬럽값을 확인해보고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>The weather was bad.</td>\n",
       "      <td>weather bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2</td>\n",
       "      <td>The lime green patio umbrella flapped happily ...</td>\n",
       "      <td>lime green patio umbrella flapped happily bree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2</td>\n",
       "      <td>Archie scrabbling up the stairs, as usual curs...</td>\n",
       "      <td>archie scrabbling stairs usual cursing blindin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>Jim was so angry that Blair was afraid.</td>\n",
       "      <td>jim angry blair afraid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2</td>\n",
       "      <td>Whenever I am scheduled to give a speech, I su...</td>\n",
       "      <td>whenever scheduled give speech suffer wet clam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                               text  \\\n",
       "index                                                            \n",
       "66        1                               The weather was bad.   \n",
       "256       2  The lime green patio umbrella flapped happily ...   \n",
       "167       2  Archie scrabbling up the stairs, as usual curs...   \n",
       "65        1            Jim was so angry that Blair was afraid.   \n",
       "147       2  Whenever I am scheduled to give a speech, I su...   \n",
       "\n",
       "                                            cleaned_text  \n",
       "index                                                     \n",
       "66                                           weather bad  \n",
       "256    lime green patio umbrella flapped happily bree...  \n",
       "167    archie scrabbling stairs usual cursing blindin...  \n",
       "65                                jim angry blair afraid  \n",
       "147    whenever scheduled give speech suffer wet clam...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['cleaned_text'] = clean_train_ # 이제 전처리된 내용을 한눈에 비교해 볼 수 있다.\n",
    "train[:5] #데이터 앞부분 5개반 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2</td>\n",
       "      <td>Old Mr Chan used a tissue to wipe the sweat fr...</td>\n",
       "      <td>old mr chan used tissue wipe sweat face got ready</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2</td>\n",
       "      <td>championships, sings lead vocals in a rock ban...</td>\n",
       "      <td>championships sings lead vocals rock band spea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>The father went to the hoghouse to kill the sm...</td>\n",
       "      <td>father went hoghouse kill smallest pig born la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>I saw clouds and lightning from the sea. I was...</td>\n",
       "      <td>saw clouds lightning sea nervous upcoming stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2</td>\n",
       "      <td>The flowers in the front garden were long dead...</td>\n",
       "      <td>flowers front garden long dead grass knee high...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                               text  \\\n",
       "index                                                            \n",
       "129       2  Old Mr Chan used a tissue to wipe the sweat fr...   \n",
       "125       2  championships, sings lead vocals in a rock ban...   \n",
       "41        1  The father went to the hoghouse to kill the sm...   \n",
       "98        1  I saw clouds and lightning from the sea. I was...   \n",
       "227       2  The flowers in the front garden were long dead...   \n",
       "\n",
       "                                            cleaned_text  \n",
       "index                                                     \n",
       "129    old mr chan used tissue wipe sweat face got ready  \n",
       "125    championships sings lead vocals rock band spea...  \n",
       "41     father went hoghouse kill smallest pig born la...  \n",
       "98     saw clouds lightning sea nervous upcoming stor...  \n",
       "227    flowers front garden long dead grass knee high...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['cleaned_text'] = clean_test_\n",
    "test[:5] #test 데이터셋도 전처리된 결과를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>weather bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2</td>\n",
       "      <td>lime green patio umbrella flapped happily bree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2</td>\n",
       "      <td>archie scrabbling stairs usual cursing blindin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>jim angry blair afraid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2</td>\n",
       "      <td>whenever scheduled give speech suffer wet clam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                       cleaned_text\n",
       "index                                                         \n",
       "66        1                                        weather bad\n",
       "256       2  lime green patio umbrella flapped happily bree...\n",
       "167       2  archie scrabbling stairs usual cursing blindin...\n",
       "65        1                             jim angry blair afraid\n",
       "147       2  whenever scheduled give speech suffer wet clam..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train[['type', 'cleaned_text']] #전처리 1차 끝!\n",
    "train_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2</td>\n",
       "      <td>old mr chan used tissue wipe sweat face got ready</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2</td>\n",
       "      <td>championships sings lead vocals rock band spea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>father went hoghouse kill smallest pig born la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>saw clouds lightning sea nervous upcoming stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2</td>\n",
       "      <td>flowers front garden long dead grass knee high...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                       cleaned_text\n",
       "index                                                         \n",
       "129       2  old mr chan used tissue wipe sweat face got ready\n",
       "125       2  championships sings lead vocals rock band spea...\n",
       "41        1  father went hoghouse kill smallest pig born la...\n",
       "98        1  saw clouds lightning sea nervous upcoming stor...\n",
       "227       2  flowers front garden long dead grass knee high..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = test[['type', 'cleaned_text']]  #전처리 1차 끝!\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type            object\n",
       "cleaned_text    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = test_dataset.astype(str)\n",
    "df_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type            object\n",
       "cleaned_text    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = train_dataset.astype(str)\n",
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.to_csv('datasets/train_datasets.csv', index=False, header=False, sep=',')\n",
    "test_dataset.to_csv('datasets/test_datasets.csv', index=False, header=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 개수 : 205\n",
      "테스트 샘플의 개수 : 51\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data # torchtext.data 임포트\n",
    "\n",
    "# 필드 정의\n",
    "\n",
    "LABEL = data.Field(sequential=False,\n",
    "                   use_vocab=False,\n",
    "                   batch_first=False,\n",
    "                   is_target=True)\n",
    "\n",
    "TEXT = data.Field(sequential=True,\n",
    "                  use_vocab=True,\n",
    "                  tokenize=str.split,\n",
    "                  lower=True,\n",
    "                  batch_first=True,\n",
    "                  fix_length=20)\n",
    "\n",
    "\n",
    "from torchtext.data import TabularDataset\n",
    "\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "        path='datasets/', train='train_datasets.csv', test='test_datasets.csv', format='csv',\n",
    "        fields=[('label', LABEL), ('text', TEXT)], skip_header=True)\n",
    "\n",
    "\n",
    "print('훈련 샘플의 개수 : {}'.format(len(train_data)))\n",
    "print('테스트 샘플의 개수 : {}'.format(len(test_data)))\n",
    "\n",
    "#ref: https://wikidocs.net/60314"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '1', 'text': ['jim', 'angry', 'blair', 'afraid']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data[2])) # text, label이 구분됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 536\n",
      "Token for \"<unk>\": 0\n",
      "Token for \"<pad>\": 1\n"
     ]
    }
   ],
   "source": [
    " #단어장 생성\n",
    "TEXT.build_vocab(train_data)\n",
    "#TEXT.build_vocab(test_data)\n",
    "\n",
    "#단어장 생성 확인\n",
    "print('Total vocabulary: {}'.format(len(TEXT.vocab)))\n",
    "print('Token for \"<unk>\": {}'.format(TEXT.vocab.stoi['<unk>']))\n",
    "print('Token for \"<pad>\": {}'.format(TEXT.vocab.stoi['<pad>']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('label', <torchtext.data.field.Field object at 0x10ab523d0>), ('text', <torchtext.data.field.Field object at 0x10ab52410>)])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.fields.items()) # tex, label 로 구분되어 있는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('label', <torchtext.data.field.Field object at 0x10ab523d0>), ('text', <torchtext.data.field.Field object at 0x10ab52410>)])\n"
     ]
    }
   ],
   "source": [
    "print(test_data.fields.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, min_freq=1, max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 1590\n"
     ]
    }
   ],
   "source": [
    "print('단어 집합의 크기 : {}'.format(len(TEXT.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x1a4e950d10>>, {'<unk>': 0, '<pad>': 1, 'ft': 2, 'h': 3, 'fs': 4, 'could': 5, 'like': 6, 'one': 7, 'behind': 8, 'house': 9, 'cold': 10, 'time': 11, 'door': 12, 'felt': 13, 'first': 14, 'long': 15, 'sun': 16, 'back': 17, 'black': 18, 'c': 19, 'came': 20, 'face': 21, 'garden': 22, 'hands': 23, 'little': 24, 'looked': 25, 'never': 26, 'night': 27, 'said': 28, 'walked': 29, 'window': 30, 'away': 31, 'blood': 32, 'every': 33, 'glass': 34, 'hand': 35, 'james': 36, 'knew': 37, 'made': 38, 'old': 39, 'red': 40, 'room': 41, 'saw': 42, 'see': 43, 'stairs': 44, 'another': 45, 'boat': 46, 'bright': 47, 'castle': 48, 'dark': 49, 'day': 50, 'feet': 51, 'hair': 52, 'home': 53, 'light': 54, 'man': 55, 'two': 56, 'village': 57, 'angry': 58, 'arms': 59, 'beautiful': 60, 'books': 61, 'breath': 62, 'darkness': 63, 'dress': 64, 'eyes': 65, 'find': 66, 'gi': 67, 'go': 68, 'got': 69, 'hot': 70, 'last': 71, 'late': 72, 'left': 73, 'lost': 74, 'mother': 75, 'nervous': 76, 'people': 77, 'place': 78, 'rain': 79, 'reached': 80, 'seemed': 81, 'someone': 82, 'still': 83, 'stood': 84, 'us': 85, 'went': 86, 'white': 87, 'wind': 88, 'yellow': 89, 'afraid': 90, 'already': 91, 'around': 92, 'asked': 93, 'bottom': 94, 'brother': 95, 'cheek': 96, 'city': 97, 'clouds': 98, 'crunched': 99, 'cut': 100, 'dry': 101, 'even': 102, 'f': 103, 'fast': 104, 'father': 105, 'floor': 106, 'front': 107, 'green': 108, 'heard': 109, 'held': 110, 'hill': 111, 'joey': 112, 'know': 113, 'let': 114, 'life': 115, 'lights': 116, 'lonely': 117, 'looking': 118, 'looks': 119, 'loved': 120, 'making': 121, 'malcolm': 122, 'newspaper': 123, 'open': 124, 'reading': 125, 'really': 126, 'running': 127, 'school': 128, 'second': 129, 'seen': 130, 'sky': 131, 'slowly': 132, 'small': 133, 'smile': 134, 'stepped': 135, 'steps': 136, 'storm': 137, 'tell': 138, 'thought': 139, 'thoughts': 140, 'three': 141, 'threw': 142, 'tired': 143, 'today': 144, 'took': 145, 'top': 146, 'toward': 147, 'town': 148, 'wall': 149, 'walls': 150, 'war': 151, 'water': 152, 'way': 153, 'well': 154, 'would': 155, 'across': 156, 'air': 157, 'alone': 158, 'alvin': 159, 'always': 160, 'anything': 161, 'bar': 162, 'bit': 163, 'bite': 164, 'blue': 165, 'box': 166, 'broken': 167, 'carl': 168, 'cheerful': 169, 'cheese': 170, 'close': 171, 'closer': 172, 'dirty': 173, 'drew': 174, 'engine': 175, 'enough': 176, 'fallen': 177, 'far': 178, 'fell': 179, 'found': 180, 'get': 181, 'girl': 182, 'going': 183, 'good': 184, 'ground': 185, 'head': 186, 'hear': 187, 'heart': 188, 'horse': 189, 'jay': 190, 'jim': 191, 'katie': 192, 'keep': 193, 'kept': 194, 'leaves': 195, 'look': 196, 'luke': 197, 'meet': 198, 'mike': 199, 'mind': 200, 'outside': 201, 'page': 202, 'pale': 203, 'party': 204, 'pepperoni': 205, 'pictures': 206, 'pulled': 207, 'ran': 208, 'right': 209, 'run': 210, 'sally': 211, 'says': 212, 'shadow': 213, 'shaking': 214, 'show': 215, 'side': 216, 'slammed': 217, 'smell': 218, 'smoking': 219, 'sometimes': 220, 'spilled': 221, 'spray': 222, 'spread': 223, 'standing': 224, 'stomach': 225, 'street': 226, 'streets': 227, 'summer': 228, 'sweat': 229, 'sword': 230, 'temperature': 231, 'thick': 232, 'things': 233, 'though': 234, 'times': 235, 'turned': 236, 'veins': 237, 'want': 238, 'warm': 239, 'watched': 240, 'weather': 241, 'whole': 242, 'wood': 243, 'wooden': 244, 'year': 245, 'aaron': 246, 'abandoned': 247, 'aimed': 248, 'among': 249, 'amy': 250, 'anna': 251, 'anyone': 252, 'arrive': 253, 'author': 254, 'bag': 255, 'barcel': 256, 'barely': 257, 'bathroom': 258, 'beach': 259, 'became': 260, 'belt': 261, 'beneath': 262, 'bill': 263, 'bowen': 264, 'branch': 265, 'breakfast': 266, 'bubbled': 267, 'bus': 268, 'busy': 269, 'calle': 270, 'canadian': 271, 'candles': 272, 'canuda': 273, 'carefully': 274, 'carried': 275, 'carrying': 276, 'cars': 277, 'cat': 278, 'center': 279, 'chapter': 280, 'chilled': 281, 'church': 282, 'circus': 283, 'clara': 284, 'class': 285, 'clear': 286, 'climbed': 287, 'clock': 288, 'clutched': 289, 'coffee': 290, 'college': 291, 'commander': 292, 'complete': 293, 'creditors': 294, 'crow': 295, 'crowd': 296, 'customers': 297, 'cynthia': 298, 'damp': 299, 'decorated': 300, 'deep': 301, 'delicious': 302, 'described': 303, 'diagilo': 304, 'doctor': 305, 'dog': 306, 'dominique': 307, 'downstairs': 308, 'drain': 309, 'dresser': 310, 'dust': 311, 'ears': 312, 'earth': 313, 'elected': 314, 'elroy': 315, 'empty': 316, 'end': 317, 'eyed': 318, 'fabric': 319, 'factories': 320, 'fall': 321, 'family': 322, 'fan': 323, 'favorite': 324, 'fear': 325, 'fence': 326, 'fern': 327, 'filled': 328, 'fingers': 329, 'fire': 330, 'five': 331, 'florentine': 332, 'footsteps': 333, 'forced': 334, 'foreign': 335, 'forest': 336, 'frightened': 337, 'gate': 338, 'georgie': 339, 'ggood': 340, 'give': 341, 'god': 342, 'gold': 343, 'grass': 344, 'great': 345, 'gripped': 346, 'guards': 347, 'gyou': 348, 'handle': 349, 'heaps': 350, 'heave': 351, 'heavy': 352, 'help': 353, 'hem': 354, 'high': 355, 'hit': 356, 'hockey': 357, 'hoofbeats': 358, 'hours': 359, 'hug': 360, 'hung': 361, 'ice': 362, 'inside': 363, 'insurance': 364, 'iona': 365, 'ireland': 366, 'iron': 367, 'isolated': 368, 'jack': 369, 'jacket': 370, 'jews': 371, 'john': 372, 'kicked': 373, 'knowing': 374, 'leather': 375, 'letting': 376, 'library': 377, 'liked': 378, 'lit': 379, 'london': 380, 'love': 381, 'lucy': 382, 'madam': 383, 'many': 384, 'mary': 385, 'massive': 386, 'mayor': 387, 'mean': 388, 'met': 389, 'metal': 390, 'metres': 391, 'might': 392, 'milk': 393, 'minutes': 394, 'miss': 395, 'molly': 396, 'morning': 397, 'mountains': 398, 'mouth': 399, 'moved': 400, 'mozzarella': 401, 'mr': 402, 'mushrooms': 403, 'music': 404, 'must': 405, 'name': 406, 'needs': 407, 'new': 408, 'next': 409, 'nurse': 410, 'ocean': 411, 'october': 412, 'opened': 413, 'ophelia': 414, 'orchestra': 415, 'orias': 416, 'others': 417, 'palms': 418, 'pancake': 419, 'paper': 420, 'passed': 421, 'past': 422, 'patio': 423, 'peace': 424, 'peaceful': 425, 'perfect': 426, 'person': 427, 'pigs': 428, 'pine': 429, 'pizza': 430, 'playing': 431, 'pomfrey': 432, 'pool': 433, 'popped': 434, 'public': 435, 'pulling': 436, 'puppy': 437, 'quickly': 438, 'reach': 439, 'readers': 440, 'reflected': 441, 'release': 442, 'remember': 443, 'rifles': 444, 'river': 445, 'roar': 446, 'rock': 447, 'rolling': 448, 'rooms': 449, 'sauce': 450, 'sausage': 451, 'seat': 452, 'sent': 453, 'served': 454, 'set': 455, 'several': 456, 'sharp': 457, 'sheet': 458, 'shiny': 459, 'shrieked': 460, 'sighet': 461, 'signals': 462, 'since': 463, 'single': 464, 'sister': 465, 'skin': 466, 'sleepy': 467, 'smelled': 468, 'smoke': 469, 'soldiers': 470, 'somehow': 471, 'something': 472, 'sound': 473, 'spreading': 474, 'spring': 475, 'staircase': 476, 'stale': 477, 'stone': 478, 'straightened': 479, 'stroke': 480, 'strokes': 481, 'strong': 482, 'stumbled': 483, 'sure': 484, 'sweet': 485, 'table': 486, 'take': 487, 'taking': 488, 'taste': 489, 'teeth': 490, 'tim': 491, 'tiny': 492, 'tom': 493, 'tomato': 494, 'trade': 495, 'train': 496, 'tree': 497, 'trembling': 498, 'trying': 499, 'twenty': 500, 'twisted': 501, 'usual': 502, 'usurers': 503, 'voices': 504, 'waitress': 505, 'warmed': 506, 'watching': 507, 'waters': 508, 'waves': 509, 'west': 510, 'wide': 511, 'wife': 512, 'wild': 513, 'without': 514, 'woman': 515, 'worry': 516, 'wrapped': 517, 'yards': 518, 'young': 519, 'accelerating': 520, 'ached': 521, 'active': 522, 'address': 523, 'admired': 524, 'adze': 525, 'afternoon': 526, 'afterward': 527, 'ago': 528, 'ahead': 529, 'aim': 530, 'almost': 531, 'also': 532, 'amid': 533, 'amount': 534, 'animal': 535, 'animals': 536, 'anxiously': 537, 'anymore': 538, 'anytime': 539, 'appear': 540, 'appeared': 541, 'apple': 542, 'applying': 543, 'appointment': 544, 'arable': 545, 'archie': 546, 'arm': 547, 'arrived': 548, 'atop': 549, 'attention': 550, 'attractive': 551, 'autumn': 552, 'aware': 553, 'ax': 554, 'bachelor': 555, 'backpack': 556, 'backward': 557, 'baking': 558, 'bang': 559, 'banging': 560, 'bark': 561, 'beadle': 562, 'beads': 563, 'bearings': 564, 'beatty': 565, 'becomes': 566, 'bedecked': 567, 'beg': 568, 'began': 569, 'bend': 570, 'bending': 571, 'beside': 572, 'best': 573, 'better': 574, 'bitter': 575, 'blackened': 576, 'blackout': 577, 'blades': 578, 'blair': 579, 'blank': 580, 'bleached': 581, 'blessing': 582, 'blight': 583, 'blind': 584, 'blinding': 585, 'blinked': 586, 'bloated': 587, 'blond': 588, 'bloodied': 589, 'blotting': 590, 'blushing': 591, 'board': 592, 'bobber': 593, 'bodies': 594, 'body': 595, 'boiled': 596, 'bolts': 597, 'bone': 598, 'book': 599, 'boots': 600, 'boring': 601, 'born': 602, 'bouncing': 603, 'bowed': 604, 'boxes': 605, 'brand': 606, 'break': 607, 'breathing': 608, 'breeze': 609, 'brenda': 610, 'brick': 611, 'bricks': 612, 'brighter': 613, 'brightly': 614, 'broadsword': 615, 'broke': 616, 'brought': 617, 'brown': 618, 'brush': 619, 'brushed': 620, 'buck': 621, 'buckets': 622, 'buds': 623, 'bulbs': 624, 'bulging': 625, 'bullied': 626, 'burned': 627, 'burning': 628, 'busied': 629, 'butterflies': 630, 'butterfly': 631, 'cab': 632, 'calcium': 633, 'calm': 634, 'candle': 635, 'candy': 636, 'canopy': 637, 'cappuccino': 638, 'caps': 639, 'captain': 640, 'car': 641, 'carpenter': 642, 'carpet': 643, 'carriage': 644, 'carry': 645, 'carved': 646, 'cascading': 647, 'casserole': 648, 'cast': 649, 'catch': 650, 'cattle': 651, 'caught': 652, 'cauldron': 653, 'celebrated': 654, 'chain': 655, 'chair': 656, 'champagne': 657, 'chan': 658, 'changed': 659, 'changing': 660, 'chapel': 661, 'chased': 662, 'chasing': 663, 'cheeks': 664, 'cheerfully': 665, 'chest': 666, 'child': 667, 'children': 668, 'chill': 669, 'chocolate': 670, 'choked': 671, 'choking': 672, 'christmas': 673, 'cigarette': 674, 'circle': 675, 'cjoey': 676, 'clammy': 677, 'clap': 678, 'clatter': 679, 'clattering': 680, 'clean': 681, 'click': 682, 'cliff': 683, 'climb': 684, 'climbing': 685, 'clipped': 686, 'cloth': 687, 'cloud': 688, 'cneither': 689, 'coat': 690, 'coated': 691, 'cobblestone': 692, 'cobblestones': 693, 'cocktail': 694, 'cocktails': 695, 'colds': 696, 'collar': 697, 'collect': 698, 'collection': 699, 'colorful': 700, 'colour': 701, 'come': 702, 'comes': 703, 'competes': 704, 'confused': 705, 'congealed': 706, 'connect': 707, 'considering': 708, 'consumed': 709, 'containers': 710, 'content': 711, 'continued': 712, 'conversation': 713, 'conveyance': 714, 'cookies': 715, 'cooking': 716, 'cores': 717, 'cottages': 718, 'county': 719, 'courtyard': 720, 'coveralls': 721, 'covered': 722, 'covering': 723, 'covers': 724, 'crack': 725, 'crammed': 726, 'crashed': 727, 'crawling': 728, 'crazy': 729, 'creaking': 730, 'creamy': 731, 'creature': 732, 'creek': 733, 'creeping': 734, 'creepy': 735, 'crepe': 736, 'cried': 737, 'crisscrossed': 738, 'crop': 739, 'crossed': 740, 'crumpling': 741, 'crunching': 742, 'crying': 743, 'crystal': 744, 'cubes': 745, 'culinary': 746, 'cupboard': 747, 'cupboards': 748, 'curbing': 749, 'curly': 750, 'curses': 751, 'cursing': 752, 'curtain': 753, 'dairy': 754, 'daisy': 755, 'dangled': 756, 'dashed': 757, 'dave': 758, 'dawn': 759, 'dazzling': 760, 'dearly': 761, 'deathly': 762, 'debated': 763, 'decaying': 764, 'decent': 765, 'decide': 766, 'decided': 767, 'decisively': 768, 'deepening': 769, 'delicately': 770, 'demanding': 771, 'descended': 772, 'desperately': 773, 'destroyed': 774, 'diligent': 775, 'dim': 776, 'dimly': 777, 'ding': 778, 'dinner': 779, 'dirtier': 780, 'disappeared': 781, 'discerned': 782, 'discover': 783, 'distance': 784, 'distorted': 785, 'dodging': 786, 'donuts': 787, 'doors': 788, 'drank': 789, 'drawer': 790, 'dressed': 791, 'dressing': 792, 'drink': 793, 'drinker': 794, 'drive': 795, 'dropped': 796, 'droppings': 797, 'drove': 798, 'drowning': 799, 'due': 800, 'dug': 801, 'dull': 802, 'dumb': 803, 'dusted': 804, 'ear': 805, 'early': 806, 'easily': 807, 'eerily': 808, 'effort': 809, 'eh': 810, 'ei': 811, 'eight': 812, 'eligible': 813, 'embarrassed': 814, 'embrace': 815, 'embraced': 816, 'emily': 817, 'emitted': 818, 'encouragement': 819, 'energy': 820, 'eno': 821, 'entered': 822, 'entirely': 823, 'enveloped': 824, 'especially': 825, 'evening': 826, 'everyone': 827, 'everything': 828, 'evinrude': 829, 'ewhat': 830, 'exhausted': 831, 'expect': 832, 'expelled': 833, 'expensive': 834, 'explosion': 835, 'faces': 836, 'faded': 837, 'faint': 838, 'faintly': 839, 'faithlessness': 840, 'false': 841, 'fam': 842, 'familiar': 843, 'fancy': 844, 'farther': 845, 'faster': 846, 'fd': 847, 'feature': 848, 'feel': 849, 'feeling': 850, 'fetid': 851, 'fifteen': 852, 'fifty': 853, 'fight': 854, 'figuring': 855, 'filmed': 856, 'filthy': 857, 'finger': 858, 'fingernail': 859, 'fiona': 860, 'fireplace': 861, 'fish': 862, 'fishing': 863, 'fishtail': 864, 'flame': 865, 'flapped': 866, 'flares': 867, 'flat': 868, 'flattened': 869, 'flesh': 870, 'flew': 871, 'flickered': 872, 'fll': 873, 'floating': 874, 'flooding': 875, 'flower': 876, 'flowers': 877, 'flowing': 878, 'flying': 879, 'fm': 880, 'fog': 881, 'fogged': 882, 'foggy': 883, 'folded': 884, 'food': 885, 'football': 886, 'foothold': 887, 'forehead': 888, 'foreigner': 889, 'form': 890, 'formed': 891, 'forward': 892, 'four': 893, 'frames': 894, 'friend': 895, 'frigid': 896, 'frost': 897, 'frothy': 898, 'frozen': 899, 'fueled': 900, 'full': 901, 'fullness': 902, 'fur': 903, 'fury': 904, 'fve': 905, 'ga': 906, 'gabilan': 907, 'game': 908, 'gardener': 909, 'gardens': 910, 'garlands': 911, 'gave': 912, 'gdarlene': 913, 'gdo': 914, 'general': 915, 'gentle': 916, 'gents': 917, 'getting': 918, 'gget': 919, 'gholy': 920, 'ghostly': 921, 'gibson': 922, 'ginny': 923, 'girlfriend': 924, 'girls': 925, 'given': 926, 'gives': 927, 'giving': 928, 'glances': 929, 'glet': 930, 'glided': 931, 'glistened': 932, 'glistening': 933, 'glowing': 934, 'glumly': 935, 'glutinous': 936, 'gnice': 937, 'goh': 938, 'gpiccadilly': 939, 'gpoint': 940, 'grabbing': 941, 'graceful': 942, 'graciously': 943, 'grand': 944, 'grandfather': 945, 'grasping': 946, 'gravity': 947, 'grazing': 948, 'grew': 949, 'grimace': 950, 'gritty': 951, 'grizzly': 952, 'groaned': 953, 'grounds': 954, 'grow': 955, 'gsit': 956, 'gslice': 957, 'gstop': 958, 'guessing': 959, 'gun': 960, 'gushed': 961, 'gushing': 962, 'gutter': 963, 'gutters': 964, 'hail': 965, 'half': 966, 'halfway': 967, 'halls': 968, 'hallway': 969, 'halted': 970, 'hammer': 971, 'hanging': 972, 'hansom': 973, 'happily': 974, 'hard': 975, 'harsh': 976, 'hated': 977, 'haunted': 978, 'haze': 979, 'healthy': 980, 'hearing': 981, 'heaven': 982, 'helmets': 983, 'hidden': 984, 'higher': 985, 'hills': 986, 'hoghouse': 987, 'holt': 988, 'homework': 989, 'honest': 990, 'hood': 991, 'hooked': 992, 'hopped': 993, 'horizon': 994, 'horsepower': 995, 'horses': 996, 'houses': 997, 'huddled': 998, 'hugged': 999, 'human': 1000, 'humid': 1001, 'humming': 1002, 'hungarian': 1003, 'hunk': 1004, 'hurled': 1005, 'hurry': 1006, 'husband': 1007, 'iced': 1008, 'ideal': 1009, 'ignited': 1010, 'impenetrable': 1011, 'impossible': 1012, 'impregnated': 1013, 'impression': 1014, 'ing': 1015, 'ingredient': 1016, 'inhale': 1017, 'ink': 1018, 'instantly': 1019, 'instead': 1020, 'intensity': 1021, 'invisible': 1022, 'itched': 1023, 'jerked': 1024, 'job': 1025, 'joy': 1026, 'julain': 1027, 'juli': 1028, 'jump': 1029, 'jumped': 1030, 'junkyard': 1031, 'key': 1032, 'kill': 1033, 'kind': 1034, 'kindergarten': 1035, 'kinds': 1036, 'kiosk': 1037, 'kiss': 1038, 'kitchen': 1039, 'kitchenware': 1040, 'knight': 1041, 'knuckles': 1042, 'lady': 1043, 'languages': 1044, 'lash': 1045, 'latin': 1046, 'lawns': 1047, 'layered': 1048, 'layers': 1049, 'leaden': 1050, 'leader': 1051, 'leaned': 1052, 'leave': 1053, 'leaving': 1054, 'led': 1055, 'leg': 1056, 'lemonade': 1057, 'lifetime': 1058, 'lifting': 1059, 'lifts': 1060, 'lighting': 1061, 'lightly': 1062, 'lightning': 1063, 'lime': 1064, 'lips': 1065, 'lives': 1066, 'living': 1067, 'locking': 1068, 'lookin': 1069, 'lots': 1070, 'loud': 1071, 'louder': 1072, 'low': 1073, 'lower': 1074, 'lurches': 1075, 'luscious': 1076, 'lying': 1077, 'machines': 1078, 'mactalde': 1079, 'mad': 1080, 'make': 1081, 'managed': 1082, 'manner': 1083, 'maw': 1084, 'may': 1085, 'meal': 1086, 'medicine': 1087, 'medieval': 1088, 'melted': 1089, 'melting': 1090, 'men': 1091, 'mesmerized': 1092, 'mess': 1093, 'metallic': 1094, 'michael': 1095, 'middle': 1096, 'minute': 1097, 'missed': 1098, 'missing': 1099, 'mist': 1100, 'moishe': 1101, 'mom': 1102, 'moment': 1103, 'montag': 1104, 'moon': 1105, 'moonlit': 1106, 'mornings': 1107, 'moths': 1108, 'motor': 1109, 'mounted': 1110, 'movement': 1111, 'movements': 1112, 'moving': 1113, 'mowing': 1114, 'mrs': 1115, 'much': 1116, 'muddy': 1117, 'muscles': 1118, 'n': 1119, 'nale': 1120, 'named': 1121, 'narrow': 1122, 'narrowing': 1123, 'navigated': 1124, 'nearer': 1125, 'needed': 1126, 'neighborhood': 1127, 'network': 1128, 'nice': 1129, 'nineties': 1130, 'ninja': 1131, 'noise': 1132, 'noises': 1133, 'nose': 1134, 'nostrils': 1135, 'note': 1136, 'nothing': 1137, 'notion': 1138, 'notions': 1139, 'occured': 1140, 'odours': 1141, 'offer': 1142, 'older': 1143, 'oliver': 1144, 'ones': 1145, 'onto': 1146, 'oooh': 1147, 'opaque': 1148, 'opera': 1149, 'opponent': 1150, 'opulent': 1151, 'orange': 1152, 'ordered': 1153, 'ordinary': 1154, 'outboard': 1155, 'outfits': 1156, 'outline': 1157, 'outward': 1158, 'overnight': 1159, 'overran': 1160, 'owl': 1161, 'oyster': 1162, 'packed': 1163, 'pain': 1164, 'painful': 1165, 'palatial': 1166, 'palm': 1167, 'pancakes': 1168, 'pans': 1169, 'pardon': 1170, 'parked': 1171, 'part': 1172, 'parties': 1173, 'pass': 1174, 'passageways': 1175, 'passenger': 1176, 'passionate': 1177, 'path': 1178, 'pavements': 1179, 'pay': 1180, 'pedestrian': 1181, 'pedestrians': 1182, 'peeled': 1183, 'pepperup': 1184, 'perched': 1185, 'percy': 1186, 'permeate': 1187, 'perspire': 1188, 'petals': 1189, 'philosophies': 1190, 'piccadilly': 1191, 'picking': 1192, 'picks': 1193, 'piece': 1194, 'pies': 1195, 'piled': 1196, 'pillow': 1197, 'pink': 1198, 'piss': 1199, 'pitcher': 1200, 'pitches': 1201, 'places': 1202, 'plane': 1203, 'planned': 1204, 'plants': 1205, 'platform': 1206, 'players': 1207, 'plumber': 1208, 'plump': 1209, 'plunger': 1210, 'plush': 1211, 'point': 1212, 'pointed': 1213, 'police': 1214, 'polished': 1215, 'polluted': 1216, 'poppies': 1217, 'porsche': 1218, 'potion': 1219, 'pots': 1220, 'pounded': 1221, 'pounding': 1222, 'poured': 1223, 'pouring': 1224, 'powder': 1225, 'preferred': 1226, 'presents': 1227, 'press': 1228, 'pressed': 1229, 'pretty': 1230, 'products': 1231, 'progress': 1232, 'promise': 1233, 'promises': 1234, 'prosper': 1235, 'protagonist': 1236, 'protein': 1237, 'provolone': 1238, 'puck': 1239, 'pull': 1240, 'pulsing': 1241, 'purple': 1242, 'purplish': 1243, 'put': 1244, 'quick': 1245, 'race': 1246, 'raced': 1247, 'rampant': 1248, 'rapid': 1249, 'rather': 1250, 'rattling': 1251, 'rays': 1252, 'razor': 1253, 'read': 1254, 'realized': 1255, 'reason': 1256, 'reef': 1257, 'reflecting': 1258, 'reflexes': 1259, 'refuge': 1260, 'regret': 1261, 'relief': 1262, 'remove': 1263, 'replied': 1264, 'restaurant': 1265, 'return': 1266, 'revolted': 1267, 'ribbons': 1268, 'riding': 1269, 'rims': 1270, 'ring': 1271, 'rising': 1272, 'rivers': 1273, 'riveted': 1274, 'roared': 1275, 'roger': 1276, 'romantic': 1277, 'roof': 1278, 'roofs': 1279, 'rose': 1280, 'rosettes': 1281, 'rotting': 1282, 'rough': 1283, 'round': 1284, 'rounds': 1285, 'rows': 1286, 'rubble': 1287, 'rumbles': 1288, 'rump': 1289, 'runt': 1290, 'rushing': 1291, 'rustle': 1292, 'ruzz': 1293, 'sad': 1294, 'safety': 1295, 'salina': 1296, 'sand': 1297, 'sapphire': 1298, 'sat': 1299, 'say': 1300, 'scared': 1301, 'scarier': 1302, 'scaring': 1303, 'scent': 1304, 'scheduled': 1305, 'scrabbling': 1306, 'scramble': 1307, 'screamed': 1308, 'screaming': 1309, 'screech': 1310, 'screen': 1311, 'sea': 1312, 'sealskin': 1313, 'seconds': 1314, 'seeds': 1315, 'seeing': 1316, 'seem': 1317, 'semi': 1318, 'send': 1319, 'senior': 1320, 'setting': 1321, 'sewers': 1322, 'sex': 1323, 'shadows': 1324, 'shake': 1325, 'shapes': 1326, 'sharpening': 1327, 'sharply': 1328, 'shattered': 1329, 'shavings': 1330, 'sheathed': 1331, 'sheer': 1332, 'sheets': 1333, 'shelves': 1334, 'shine': 1335, 'shirt': 1336, 'shit': 1337, 'shivering': 1338, 'shook': 1339, 'shops': 1340, 'shore': 1341, 'shot': 1342, 'shout': 1343, 'shouted': 1344, 'shower': 1345, 'shroud': 1346, 'shuf': 1347, 'shut': 1348, 'shuzz': 1349, 'sick': 1350, 'sides': 1351, 'sidewalks': 1352, 'sigh': 1353, 'sighed': 1354, 'sighing': 1355, 'silenced': 1356, 'silent': 1357, 'silently': 1358, 'silly': 1359, 'simmering': 1360, 'simple': 1361, 'sir': 1362, 'sisters': 1363, 'sits': 1364, 'situation': 1365, 'six': 1366, 'sizes': 1367, 'skies': 1368, 'slack': 1369, 'slam': 1370, 'slapping': 1371, 'slash': 1372, 'sleep': 1373, 'sleigh': 1374, 'slept': 1375, 'slice': 1376, 'slip': 1377, 'slowed': 1378, 'slushies': 1379, 'smaller': 1380, 'smiled': 1381, 'smiles': 1382, 'smooth': 1383, 'snuck': 1384, 'snuffed': 1385, 'soggy': 1386, 'sold': 1387, 'sole': 1388, 'somewhere': 1389, 'soon': 1390, 'sorority': 1391, 'sorting': 1392, 'soul': 1393, 'span': 1394, 'spans': 1395, 'spate': 1396, 'speak': 1397, 'speech': 1398, 'speed': 1399, 'spilling': 1400, 'spit': 1401, 'splinter': 1402, 'sports': 1403, 'spun': 1404, 'spurred': 1405, 'squaked': 1406, 'square': 1407, 'squinted': 1408, 'squinting': 1409, 'stacks': 1410, 'staff': 1411, 'staleness': 1412, 'stand': 1413, 'stands': 1414, 'stars': 1415, 'starts': 1416, 'station': 1417, 'stay': 1418, 'steadily': 1419, 'steady': 1420, 'stealth': 1421, 'steam': 1422, 'steeds': 1423, 'steep': 1424, 'step': 1425, 'stiff': 1426, 'stock': 1427, 'stop': 1428, 'stories': 1429, 'story': 1430, 'straight': 1431, 'strawberry': 1432, 'streamers': 1433, 'streetlamps': 1434, 'stripped': 1435, 'struggled': 1436, 'studded': 1437, 'students': 1438, 'stuffed': 1439, 'subject': 1440, 'sucked': 1441, 'sudden': 1442, 'suddenly': 1443, 'suffer': 1444, 'sugar': 1445, 'sunday': 1446, 'sunny': 1447, 'sunshine': 1448, 'suppose': 1449, 'surface': 1450, 'surpassed': 1451, 'surprised': 1452, 'surrender': 1453, 'susie': 1454, 'suzie': 1455, 'swamped': 1456, 'sweaty': 1457, 'sweeping': 1458, 'swiftly': 1459, 'swimmers': 1460, 'swipes': 1461, 'swirled': 1462, 'switched': 1463, 'tables': 1464, 'tackle': 1465, 'taken': 1466, 'talented': 1467, 'taller': 1468, 'talons': 1469, 'tapped': 1470, 'tasted': 1471, 'tears': 1472, 'tensed': 1473, 'terms': 1474, 'text': 1475, 'thatched': 1476, 'thermometers': 1477, 'thickly': 1478, 'thin': 1479, 'thing': 1480, 'think': 1481, 'third': 1482, 'throat': 1483, 'thumbnail': 1484, 'thump': 1485, 'thunder': 1486, 'thunderboy': 1487, 'thundered': 1488, 'tiffany': 1489, 'tiger': 1490, 'tighten': 1491, 'tin': 1492, 'tobacco': 1493, 'toilet': 1494, 'tonight': 1495, 'toolbox': 1496, 'tools': 1497, 'tossed': 1498, 'touched': 1499, 'towards': 1500, 'treats': 1501, 'trees': 1502, 'tried': 1503, 'tripped': 1504, 'troubled': 1505, 'truly': 1506, 'tune': 1507, 'tuning': 1508, 'turbulent': 1509, 'turning': 1510, 'tv': 1511, 'tweed': 1512, 'twinkling': 1513, 'ugly': 1514, 'umbrella': 1515, 'unattached': 1516, 'unsuccessful': 1517, 'unusual': 1518, 'upon': 1519, 'upper': 1520, 'used': 1521, 'vance': 1522, 'various': 1523, 'vegetables': 1524, 'vehicle': 1525, 'veiled': 1526, 'vellum': 1527, 'vest': 1528, 'view': 1529, 'vigorously': 1530, 'vitamins': 1531, 'vivid': 1532, 'voice': 1533, 'wafers': 1534, 'waft': 1535, 'waist': 1536, 'wait': 1537, 'waited': 1538, 'waiting': 1539, 'wanted': 1540, 'warmth': 1541, 'wash': 1542, 'watch': 1543, 'watermelon': 1544, 'weak': 1545, 'wearing': 1546, 'weasley': 1547, 'wedding': 1548, 'weight': 1549, 'weightlifting': 1550, 'welcome': 1551, 'welsh': 1552, 'wet': 1553, 'whenever': 1554, 'whether': 1555, 'whipped': 1556, 'whisperings': 1557, 'whistling': 1558, 'wildly': 1559, 'willed': 1560, 'wilting': 1561, 'windless': 1562, 'windows': 1563, 'winged': 1564, 'wiped': 1565, 'wire': 1566, 'wished': 1567, 'witcham': 1568, 'woke': 1569, 'wondered': 1570, 'wonderful': 1571, 'words': 1572, 'wore': 1573, 'worked': 1574, 'worn': 1575, 'wrenches': 1576, 'wretched': 1577, 'wrist': 1578, 'writing': 1579, 'wrong': 1580, 'yard': 1581, 'yawned': 1582, 'years': 1583, 'yelled': 1584, 'yells': 1585, 'yogurt': 1586, 'york': 1587, 'younger': 1588, 'yowl': 1589})\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.stoi) # 생성된 집합 내 단어들을 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토치텍스트의 테이터로더 생성\n",
    "\n",
    "from torchtext.data import Iterator\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 미니 배치 수 : 13\n",
      "테스트 데이터의 미니 배치 수 : 4\n"
     ]
    }
   ],
   "source": [
    "train_loader = Iterator(dataset=train_data, batch_size = batch_size)\n",
    "test_loader = Iterator(dataset=test_data, batch_size = batch_size)\n",
    "\n",
    "print('훈련 데이터의 미니 배치 수 : {}'.format(len(train_loader)))\n",
    "print('테스트 데이터의 미니 배치 수 : {}'.format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.batch.Batch'>\n"
     ]
    }
   ],
   "source": [
    "print(type(batch)) #미니배치 자료형 확인. 토치텍스늬 데이터로더는  'torchtext.data.batch.Batch'라는 객체를 가져온다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  10,  646,  598,  301,  900, 1045,   88, 1008,  799,   79,  962,  587,\n",
      "          131,  365, 1551,  366,  120,    5,   93,  999],\n",
      "        [  72,  321,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 263,  337,  139,   82,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 274, 1124,  693,  740,  965,  973,  632, 1226, 1073,  279,  947, 1383,\n",
      "         1510,  382,  238,  540,    4, 1498,  644,  825],\n",
      "        [ 358, 1378,  837, 1451, 1249,  679,  444, 1272,  530,  682,  597, 1068,\n",
      "           78,  416,    4,   32,  706,  237,    1,    1],\n",
      "        [ 298,  460,  207, 1559,   52,  217,   12, 1054,   24,   95,  158,  705,\n",
      "          969,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 380,   10,  299,  883,   97,  738,  692,  227, 1182,  274,  786,  797,\n",
      "         1423,  207, 1083,  435,  714,    7, 1181,  382],\n",
      "        [ 264,  192,  860,   66,  458, 1527,   39, 1552,  282,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 936,   63,   20,   85,  216,   24,   17,  890,  476,    5,  782,   18,\n",
      "          478,  136,  772,   74, 1324,    1,    1,    1],\n",
      "        [  58,  142,  447,    9,  616,   34,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 126,  395,   22, 1277,  425,  292,    4,  512,  532,   13,  711,  154,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 416,  208,   31,  470,  189, 1030,  177,  497,  265,  109,   82, 1343,\n",
      "         1428,   13,   76,  470,  970,  248,  444,    1],\n",
      "        [ 117,  368,  773, 1567,  707,   45,  427,  471,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [  29,   44, 1027,   86,   39,  316,  377,  294,  503, 1466,  828,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 493,    2,  238,   68,  128,  144,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 192, 1193,   94,  703,   31,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1]])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader)) # 16개씩 묶어줬음. 첫번째 미니배치에 저장\n",
    "\n",
    "print(batch.text) #첫번째 미니 배치의 text 필드를 호출해서 확인해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 72, 321,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader)) # 첫번째 미니배치\n",
    "print(batch.text[0]) # 첫번째 미니배치 중 첫번째 샘플"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기가지 진행완료!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:09, 12860.97lines/s]\n",
      "120000lines [00:17, 6766.86lines/s]\n",
      "7600lines [00:01, 5807.08lines/s]\n"
     ]
    }
   ],
   "source": [
    "#원본코드인데.. 이미 가공된 데이터셋을 ngrams 처리해서 불러오기 때문에 입력데이터를 dataset에 맞게 수정해야 한다.\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "\n",
    "NGRAMS = 2\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.isdir('./data2'):\n",
    "    os.mkdir('./data2')\n",
    "    \n",
    "    \n",
    "#text_classification.DATASETS의 구조를 보고 결과데이터를 어떻게 생성하는지 분석하거나 이하 학습코드를 분석    \n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./data2', ngrams=NGRAMS, vocab=None) \n",
    "#ref : https://pytorch.org/text/datasets.html#ag-news\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3,\n",
       "  tensor([    131,       5,   23258,      27,    2922,     357,    2688,     769,\n",
       "              814,      14,      32,      15,      16,       6,     131,       7,\n",
       "              230,     293,     452,     836,    6438,      85,       2,      51,\n",
       "            43647,       2,   24372,       4,   60885,      51,  281059,       2,\n",
       "                0,       9,   21969,     115,       2,      51,  108539,       2,\n",
       "            36279,       4,      11,      81,      31,      90,      39,   23258,\n",
       "                6,      27,     357,    4090,    1698,      53,       5,     273,\n",
       "              821,       3,    1507,       7,       3,    1473,    3049,       2,\n",
       "             9821,   53919,  115850,   75043,   30252,  478273,  244818,     822,\n",
       "             4291,      43,      44,      46,     296,    2486,    2022,    8893,\n",
       "            23909,   49141,  381768,    9169,   19041,      89,     122,       0,\n",
       "            43648,   24031,   69185,   94356, 1174101,       0,  450795,       0,\n",
       "                0,  223922,  108592,     126,     122,       0,  178323,   35545,\n",
       "           409428,     735,     174,    3486,    2986,    3563,  119987,  102135,\n",
       "              151,   21657,   24613,  196267,  449408,     568,    9261,   16383,\n",
       "            10610,   12791,   10597,      29,    3937,  125919,   29219])),\n",
       " (3,\n",
       "  tensor([    398,    1371,    4357,     140,       4,     529,   17887,     769,\n",
       "              814,      14,      32,      15,      16,     398,     239,      85,\n",
       "                2,      51,   12202,       2,   36279,      11,      77,    1017,\n",
       "             3918,       6,      27,     621,    1304,       5,    1413,     450,\n",
       "             1535,   20318,    3524,    4357,       9,    1475,       6,    5662,\n",
       "           139374,     140,   17887,   10030,      25,    5818,     295,     374,\n",
       "             3237,     140,       2,   58236,  912220,  247465,    3730,   20914,\n",
       "                0,       0,     822,    4291,      43,      44,      46,   10183,\n",
       "             2133,    4850,      89,     122,       0,   27392,   35545,  132604,\n",
       "              161,  288511,  177353,   56884,     151,   37744,  115817,    1756,\n",
       "             5972,   49904, 1262338,  702497, 1079944,  965345,  128458,  175247,\n",
       "             5939,   33049,       0,  472516,       0,       0,       0,   11287,\n",
       "                0,  127381,  125934,       0,    3595])),\n",
       " (3,\n",
       "  tensor([  1743,   1191,   2678,    398,      5,   1044,   2419,    172,   5807,\n",
       "              14,     32,     15,     16,    506,    198,   7286,      4,      6,\n",
       "            1743,    288,   1118,   1847,      4,    500,    398,    239,     85,\n",
       "               2,      5,    172,     11,     77,     20,      3,    559,      7,\n",
       "               6,     27,   2419,    214,      8,    482,     17,     10,    415,\n",
       "           13976,     12,    592,   5611,    343,      2, 183509,      0, 326295,\n",
       "           29013,  36745,      0,  87826,      0,  15874,     43,     44,     46,\n",
       "           40085,      0, 430407, 103141,     87,  19259,      0, 404669,  43244,\n",
       "           10017,  29826,      0,   2133,   4850,     89,   2071,  10175,  39267,\n",
       "             161,  10293,    154,   5172,   3559,    107,    151, 262534,  14209,\n",
       "            6874,   2580,   6611,     23,   5736, 256315, 145528,  17037, 350126,\n",
       "               0,   4028])),\n",
       " (3,\n",
       "  tensor([  3270, 144492,  18571,    840,    368,      6,    334, 141753,  18571,\n",
       "               7,  62303,    136,     33,    109,   2508,      8,   6705,      4,\n",
       "             368,      4,    125,   5741,   1044,  68572,   2480,      2,      0,\n",
       "               0,      0,      0,  58315,  10751,      0,      0, 109318,      0,\n",
       "               0,   3087,    282,  21461,  18230,  49708,  49983,   5149,   9282,\n",
       "            7808,  38803,      0,      0,      0,  36581])),\n",
       " (3,\n",
       "  tensor([1155350,   17813,   46664,    1367,   46664,    1367,       4,     156,\n",
       "           422148,       4,   16838,      11, 1155350,       5,     633,     333,\n",
       "               34,   38763,       4,     601,    1395,       2,       0,       0,\n",
       "                0,       0,       0,   14019,    1323,       0,       0,       0,\n",
       "            20324,       0,       0,    1215,   34865,   15806,  801893,       0,\n",
       "             4382,   91929,   62739])),\n",
       " (3,\n",
       "  tensor([   9647,     111,       2,   19481,      17,      10,   13103,     339,\n",
       "            57385,   19481,   14857,      25,    2291,    3920,     740,       5,\n",
       "               35,   27554,     339,   45525,     301,      25,  221858,     130,\n",
       "                2,       0,    7864,   51327, 1098538,      23,  275296,       0,\n",
       "                0,   77111,       0,  221568,   63824,       0,       0,     958,\n",
       "             1149,   78427,       0,  152353,       0,   16709,       0,       0,\n",
       "             1272]))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[10:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  tensor([    572,     564,       2,    2326,   49106,     150,      88,       3,\n",
       "             1143,      14,      32,      15,      32,      16,  443749,       4,\n",
       "              572,     499,      17,      10,  741769,       7,  468770,       4,\n",
       "               52,    7019,    1050,     442,       2,   14341,     673,  141447,\n",
       "           326092,   55044,    7887,     411,    9870,  628642,      43,      44,\n",
       "              144,     145,  299709,  443750,   51274,     703,   14312,      23,\n",
       "          1111134,  741770,  411508,  468771,    3779,   86384,  135944,  371666,\n",
       "             4052])),\n",
       " (2,\n",
       "  tensor([  55003,    1474,    1150,    1832,    7559,      14,      32,      15,\n",
       "               32,      16,    1262,    1072,     436,   55003,     131,       4,\n",
       "           142576,      33,       6,    8062,      12,     756,  475640,       9,\n",
       "           991346,    3186,       8,       3,     698,     329,       4,      33,\n",
       "             6764, 1040465,   13979,      11,     278,     483,       7,       3,\n",
       "              172,       2,  659973,  193730, 1237754,  684719,  556644,      43,\n",
       "               44,     144,     145,   77775,   56578,   32382,  782124,   79225,\n",
       "             2908,  140697,  540900,    2031,   31960,   45339,   21562,  936430,\n",
       "          1282186,  578442,  991347,   69671,      26,    9260,  717285,    5378,\n",
       "              597,   27622, 1070413, 1040466,   38669,   27790,  175394,     711,\n",
       "               29,    1404,    1818])),\n",
       " (2,\n",
       "  tensor([    78,      9,    469,   8385,    206,     17,    996,     14,     32,\n",
       "              15,     32,     16,   3654,    600,    124,   3080, 140201,      3,\n",
       "             469,      9,      3,    996,     12,    369,     52,    328, 464765,\n",
       "              48,      3,    397,    172,    149,    113,    301,      3,  18223,\n",
       "               7, 285456,  52106,      2,   5606,  95553, 183737, 180431, 164136,\n",
       "          120431,  30446,     43,     44,    144,    145,  85276,  92253,  10654,\n",
       "          200704, 422054, 213900,   1877,   8549,    152,  12306,   6126,  55468,\n",
       "           63058,   4461, 358165, 464766,    204,   5641,   4893,  59857,   1515,\n",
       "           93728,    995,  83994,  79735, 411437, 460552, 110113])),\n",
       " (2,\n",
       "  tensor([     93,   16478,      78,    2680,      34,    1230,     717,    4562,\n",
       "               14,      32,      15,      32,      16,    1129,      49,    9392,\n",
       "               78,  766148,      34,       3,    1230,    4562,       8,     717,\n",
       "               93,  559272,     947,       6,    1239,    3934,     125, 1178056,\n",
       "                4,      35,      78,     396,      31,      11,     153,       2,\n",
       "           881320,  373252,   12125,   23059,  802619,  258328,  447615,  199864,\n",
       "               43,      44,     144,     145,   40078,   13205,  152671,  373240,\n",
       "          1002054,  766149,     171,    4412,  934044,   47573,    2586,   17490,\n",
       "           881151,  559273,   16579,   17268,   64855,  954751,  699138, 1178057,\n",
       "              733,   26650,   36317,    1670,     177,     435,     949])),\n",
       " (2,\n",
       "  tensor([     78,     124,    7860,       5,    6044,     198,       4,   17139,\n",
       "               27,   36218,       5,      45,     469,      14,     138,      15,\n",
       "              138,      16, 1197667,      61,      78,     124,       4,   31767,\n",
       "             2540,       9,   43425,   25199,       4,    6060,       6,      27,\n",
       "              489,   36218,    5810,     127,     403,     226,       3,      45,\n",
       "              532,     726,       2,     240,   60413,   65219,  104402,  107151,\n",
       "            10156,   48457, 1048623,  262618,  400118,    5800,    4956,   28355,\n",
       "              279,     280,     388,     386,  501493, 1197668,    7397,     240,\n",
       "             3497,   66207, 1235874,   50427,  317844, 1176539,  473765,  216053,\n",
       "            45249,     151,   64260,  745331,  949223,  325090,    4774,   26987,\n",
       "             1139,     243,    8436,    7063,    6238])),\n",
       " (2,\n",
       "  tensor([    206,     208,      53,       4,      55,     479,      94,    3464,\n",
       "               14,      32,      15,      32,      16,     206,    1060,    1902,\n",
       "              379,      11,  800978,    7992,     479,    3464,      12,       3,\n",
       "               94,      21,      78,     124,    3501,     476,     619,  528552,\n",
       "              965,       4,   39403,       6,    2350,     996,      34,     239,\n",
       "           935798,      85,       2,      14,    1141,       2,     495,      15,\n",
       "            11722,   15152,    3178,     118,  231225,  975243,  480985,  131178,\n",
       "               43,      44,     144,     145,    6425,   18789,   63098,   12156,\n",
       "             4606, 1004868,  800979, 1171655,   60005,  396069,      60,    1011,\n",
       "             6986,    2339,     240,   15238,   70524, 1028775,  482706,  528553,\n",
       "             6084,   51256,  101340,   15096,  269456,   27804,   73786,  688832,\n",
       "           935799,      89,     384,   57662,   24791,     932,   17728])),\n",
       " (2,\n",
       "  tensor([  1018,   1675,    582,      8,    415,    113,     14,     36,     15,\n",
       "              36,     16,   2612,      7,      3,    542,     17,     10,   1118,\n",
       "            1018,    172,   3234,   1675,    582,     28,    619,    142,      2,\n",
       "            1049,    189,      8,      3,    415,    113,      5,    619, 534560,\n",
       "               2,  21905,  14818,      4,      3,   1072,     72,   4880,     31,\n",
       "              81,      2, 100737,  98628,   9912,  18657,  15704,  19999,     62,\n",
       "              63,     71,     70,  85239,  16664,     29,    841,   2275,     23,\n",
       "           41434,  91602,  81876,  89964,   8199,  98628,  18613,  37184,   7399,\n",
       "             765,   5717,  77916,   1917,     26,    744,  15704,   4456,   9650,\n",
       "          482769, 534561,  77817, 535975, 288294,     42,  18426,  32381,  72988,\n",
       "           47073,   1132,    549])),\n",
       " (2,\n",
       "  tensor([   1900,    1714,     684,   44443,      48,    2013,      14,    3315,\n",
       "                2,     232,      15,    3315,       2,     232,      16,    1118,\n",
       "              169,   12060,     150,       6,    3972,       8,    1709,       4,\n",
       "                9,      27,     763,      12,    3794,    2462,     582,      92,\n",
       "              113,       4,       3,     133,      31,      81,       4,   15691,\n",
       "                3,     469,      24,    4021,      34,       6,  131682,    6254,\n",
       "                2,  360219,  957280, 1143380,  350974,  266796,  190595,    6596,\n",
       "             3316,     277,     928,    6636,    3316,     277,    1182,  141294,\n",
       "             3592,  438165,   22995,   17293,    5686,  146000,    3246,    7586,\n",
       "              108,    4936,   39342,   17008,   22055,   29322,  108286,   40674,\n",
       "              605,     976,      42,     742,    3807,    1132,     820,   31877,\n",
       "            99516,    1877,    9292,   68651,  380346,     613,  309262,  401039,\n",
       "            45492])),\n",
       " (2,\n",
       "  tensor([   1456,     602,      14,    4054,       2,     232,      15,    4054,\n",
       "                2,     232,      16,      40,   22024,       6,  420149,       2,\n",
       "             1224,       2,       8, 1155430,       4,    8786,  614302,   23842,\n",
       "             1797,       5,     503,      21,       3,     451,     701,      22,\n",
       "                6,    1832,     796,    5847,     436,      22,      35,    1308,\n",
       "             2127,    9247,       7,     619,    3643,       4,     219,       2,\n",
       "             1002,      40,       4,       6,     550,   69660,    4668,      28,\n",
       "               38,   18226,       5,     902,     147,  642157,      91,    1037,\n",
       "             2462,    1370,     341,      38,   22528,       2,      55,       4,\n",
       "               22,    4905,       4,    1879,    1037,      41,       3,  245967,\n",
       "             2827,      34,    1807,    4949,       4,     112,   23842,       2,\n",
       "            83196,  406394,   10956,    5498,     277,     928,   10957,    5498,\n",
       "              277,    1182,    5524,  173711,   87887,  544978,  420150,   11482,\n",
       "             3100,    1107,  864884, 1155431,   85118,  710639,  614303, 1100087,\n",
       "            20363,    2527,   28884,     194,   12465,    6090,   49951,     446,\n",
       "            21914,  684789,    6672,  356699,  127453,    3228,   13757,  583197,\n",
       "           176942,  102821,   20271,  120361,   13387,     223,   10964,   71661,\n",
       "            38040,  143313,      87,   17265,  127414, 1041355,  136947,    5022,\n",
       "           153035,  722611,    6727,  149512,  998456,  642158,  310432,  252583,\n",
       "           622234,   78794,   25784,  153040,  110408,     694,   20536,    1792,\n",
       "           322176,   94783,   85105,  649707,  874340,    1326,  284635,  367875,\n",
       "           285881,  111292,  132111,   59921,    1391, 1121846,  434103])),\n",
       " (2,\n",
       "  tensor([   572,    564,      2,   2326,  49106,    150,     88,      3,   1143,\n",
       "              27,     96,     14,     32,     15,     16, 443749,      4,    572,\n",
       "             499,     17,     10,  29160,   5488,      7, 468770,      4,     52,\n",
       "            7019,   1050,    442,      2,  14341,    673, 141447, 326092,  55044,\n",
       "            7887,    411,   9870, 628732,     97,    276,     43,     44,     46,\n",
       "          299709, 443750,  51274,    703,  14312,     23, 275050, 741752,  29990,\n",
       "          411508, 468771,   3779,  86384, 135944, 371666,   4052]))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7600"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_verif = []\n",
    "data_verif = test_dataset\n",
    "data_verif_len = len(data_verif)\n",
    "data_verif_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7600"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_verif_train = []\n",
    "data_verif_train = train_dataset\n",
    "data_verif_tr_len = len(data_verif_train)\n",
    "data_verif_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'showingTelling.xlsx', 'showingTelling_csv.csv']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('./data')\n",
    "os.listdir('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kimkwangil/Project/01EssayFitAI/showing_telling'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = len(train_dataset.get_labels())\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1308844"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUN_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 25 seconds\n",
      "\tLoss: 0.0260(train)\t|\tAcc: 84.8%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.5%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0118(train)\t|\tAcc: 93.7%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 89.3%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0068(train)\t|\tAcc: 96.4%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.6%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 25 seconds\n",
      "\tLoss: 0.0038(train)\t|\tAcc: 98.2%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 90.8%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0022(train)\t|\tAcc: 99.1%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 91.3%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "N_EPOCHS = 5\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "sub_train_, sub_valid_ = \\\n",
    "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0002(test)\t|\tAcc: 89.3%(test)\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(test_dataset)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "ag_news_label = {1 : \"World\",\n",
    "                 2 : \"Sports\",\n",
    "                 3 : \"Business\",\n",
    "                 4 : \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "vocab = train_dataset.get_vocab()\n",
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
