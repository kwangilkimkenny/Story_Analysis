{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification showing and telling with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The house was creepy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I heard footsteps creeping behind me and it ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>She was my best friend. I could tell her almos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>She hated it there because it smelled bad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>When they embraced she could tell he had been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>2</td>\n",
       "      <td>Her hand reached for the massive, iron door ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>2</td>\n",
       "      <td>The way the door decisively slammed behind her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>2</td>\n",
       "      <td>Dust coated every last surface. He ran his fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2</td>\n",
       "      <td>The lime green patio umbrella flapped happily ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>2</td>\n",
       "      <td>\"And after all the weather was ideal. They cou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>258 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                               text\n",
       "index                                                         \n",
       "0         1                              The house was creepy.\n",
       "1         1  I heard footsteps creeping behind me and it ma...\n",
       "2         1  She was my best friend. I could tell her almos...\n",
       "3         1         She hated it there because it smelled bad.\n",
       "4         1  When they embraced she could tell he had been ...\n",
       "...     ...                                                ...\n",
       "253       2  Her hand reached for the massive, iron door ha...\n",
       "254       2  The way the door decisively slammed behind her...\n",
       "255       2  Dust coated every last surface. He ran his fin...\n",
       "256       2  The lime green patio umbrella flapped happily ...\n",
       "257       2  \"And after all the weather was ideal. They cou...\n",
       "\n",
       "[258 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('data/showingTelling_csv.csv', delimiter = ',')\n",
    "\n",
    "data.index.name = \"index\"\n",
    "data.columns = [\"type\", \"text\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "def shuffle(df, n=1, axis=0): #데이터가 1111 0000이라 잘 섞어주자. \n",
    "    df = data.copy()\n",
    "    for _ in range(n):\n",
    "        df.apply(np.random.shuffle, axis=axis)\n",
    "    return df\n",
    "\n",
    "shuffle(data)\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2</td>\n",
       "      <td>and rain grew louder and seemed closer₩cJoey s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>I climbed the fence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2</td>\n",
       "      <td>She trembled and looked up at him with fear in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>Imagine someone burning the book page by page,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2</td>\n",
       "      <td>the slow, steep walk up the stairs. ₩eEvery da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                               text\n",
       "index                                                         \n",
       "132       2  and rain grew louder and seemed closer₩cJoey s...\n",
       "26        1                               I climbed the fence.\n",
       "180       2  She trembled and looked up at him with fear in...\n",
       "43        1  Imagine someone burning the book page by page,...\n",
       "130       2  the slow, steep walk up the stairs. ₩eEvery da..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head() # 잘 섞였군만! 하지만 데이터 전처리가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1</td>\n",
       "      <td>Elroy drove the boat for fifteen minutes until...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>2</td>\n",
       "      <td>A saw and hammer dangled from his belt and an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2</td>\n",
       "      <td>My mother and I would take walks to watch the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>Bill was frightened. He thought someone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2</td>\n",
       "      <td>run down the stairs, let my dog out, and inhal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                               text\n",
       "index                                                         \n",
       "87        1  Elroy drove the boat for fifteen minutes until...\n",
       "208       2  A saw and hammer dangled from his belt and an ...\n",
       "138       2  My mother and I would take walks to watch the ...\n",
       "21        1            Bill was frightened. He thought someone\n",
       "152       2  run down the stairs, let my dog out, and inhal..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas\n",
    "import numpy\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(review, remove_stopwords=False):\n",
    "        \n",
    "    # 영어가 아닌 특수문자들을 공백(\" \")으로 바꾸기\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review)\n",
    "\n",
    "    # 대문자들을 소문자로 바꾸고 공백단위로 텍스트들 나눠서 리스트로 만든다.\n",
    "    words = review_text.lower().split()\n",
    "\n",
    "    if remove_stopwords: \n",
    "        # 불용어들을 제거\n",
    "    \n",
    "        #영어에 관련된 불용어 불러오기\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        # 불용어가 아닌 단어들로 이루어진 새로운 리스트 생성\n",
    "        words = [w for w in words if not w in stops]\n",
    "        # 단어 리스트를 공백을 넣어서 하나의 글로 합친다.\n",
    "        clean_review = ' '.join(words)\n",
    "\n",
    "    else: # 불용어 제거하지 않을 때\n",
    "        clean_review = ' '.join(words)\n",
    "\n",
    "    return clean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rain grew louder seemed closer cjoey sat still'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_ = []\n",
    "for review in train['text']:\n",
    "    clean_train_.append(preprocessing(review, remove_stopwords=True))\n",
    "\n",
    "# 전처리된 데이터 확인. 잘됨 !! ㅎ\n",
    "clean_train_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rain grew louder seemed closer cjoey sat still',\n",
       " 'climbed fence',\n",
       " 'trembled looked fear eyes',\n",
       " 'imagine someone burning book page page give us incorrect knowledge',\n",
       " 'slow steep walk stairs eevery day fs f thought']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'elroy drove boat fifteen minutes reached canadian waters cut engine began fishing'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_ = []\n",
    "for review in test['text']:\n",
    "    clean_test_.append(preprocessing(review, remove_stopwords=True))\n",
    "    \n",
    "# 전처리된 데이터 확인. 잘됨 !! ㅎ\n",
    "clean_test_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elroy drove boat fifteen minutes reached canadian waters cut engine began fishing',\n",
       " 'saw hammer dangled belt adze hooked one thumbnail black bowed saw several long wood shavings caught curly hair',\n",
       " 'mother would take walks watch glow chapel fs stained glass',\n",
       " 'bill frightened thought someone',\n",
       " 'run stairs let dog inhale breakfast scramble books jacket race front door']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\n",
      "132    2\n",
      "26     1\n",
      "180    2\n",
      "43     1\n",
      "130    2\n",
      "      ..\n",
      "121    2\n",
      "79     1\n",
      "219    2\n",
      "198    2\n",
      "13     1\n",
      "Name: type, Length: 206, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train[\"type\"]) #showing telling 컬럽값을 확인해보고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2</td>\n",
       "      <td>and rain grew louder and seemed closer₩cJoey s...</td>\n",
       "      <td>rain grew louder seemed closer cjoey sat still</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>I climbed the fence.</td>\n",
       "      <td>climbed fence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2</td>\n",
       "      <td>She trembled and looked up at him with fear in...</td>\n",
       "      <td>trembled looked fear eyes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>Imagine someone burning the book page by page,...</td>\n",
       "      <td>imagine someone burning book page page give us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2</td>\n",
       "      <td>the slow, steep walk up the stairs. ₩eEvery da...</td>\n",
       "      <td>slow steep walk stairs eevery day fs f thought</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                               text  \\\n",
       "index                                                            \n",
       "132       2  and rain grew louder and seemed closer₩cJoey s...   \n",
       "26        1                               I climbed the fence.   \n",
       "180       2  She trembled and looked up at him with fear in...   \n",
       "43        1  Imagine someone burning the book page by page,...   \n",
       "130       2  the slow, steep walk up the stairs. ₩eEvery da...   \n",
       "\n",
       "                                            cleaned_text  \n",
       "index                                                     \n",
       "132       rain grew louder seemed closer cjoey sat still  \n",
       "26                                         climbed fence  \n",
       "180                            trembled looked fear eyes  \n",
       "43     imagine someone burning book page page give us...  \n",
       "130       slow steep walk stairs eevery day fs f thought  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['cleaned_text'] = clean_train_ # 이제 전처리된 내용을 한눈에 비교해 볼 수 있다.\n",
    "train[:5] #데이터 앞부분 5개반 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1</td>\n",
       "      <td>Elroy drove the boat for fifteen minutes until...</td>\n",
       "      <td>elroy drove boat fifteen minutes reached canad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>2</td>\n",
       "      <td>A saw and hammer dangled from his belt and an ...</td>\n",
       "      <td>saw hammer dangled belt adze hooked one thumbn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2</td>\n",
       "      <td>My mother and I would take walks to watch the ...</td>\n",
       "      <td>mother would take walks watch glow chapel fs s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>Bill was frightened. He thought someone</td>\n",
       "      <td>bill frightened thought someone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2</td>\n",
       "      <td>run down the stairs, let my dog out, and inhal...</td>\n",
       "      <td>run stairs let dog inhale breakfast scramble b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                               text  \\\n",
       "index                                                            \n",
       "87        1  Elroy drove the boat for fifteen minutes until...   \n",
       "208       2  A saw and hammer dangled from his belt and an ...   \n",
       "138       2  My mother and I would take walks to watch the ...   \n",
       "21        1            Bill was frightened. He thought someone   \n",
       "152       2  run down the stairs, let my dog out, and inhal...   \n",
       "\n",
       "                                            cleaned_text  \n",
       "index                                                     \n",
       "87     elroy drove boat fifteen minutes reached canad...  \n",
       "208    saw hammer dangled belt adze hooked one thumbn...  \n",
       "138    mother would take walks watch glow chapel fs s...  \n",
       "21                       bill frightened thought someone  \n",
       "152    run stairs let dog inhale breakfast scramble b...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['cleaned_text'] = clean_test_\n",
    "test[:5] #test 데이터셋도 전처리된 결과를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2</td>\n",
       "      <td>rain grew louder seemed closer cjoey sat still</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>climbed fence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2</td>\n",
       "      <td>trembled looked fear eyes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>imagine someone burning book page page give us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2</td>\n",
       "      <td>slow steep walk stairs eevery day fs f thought</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                       cleaned_text\n",
       "index                                                         \n",
       "132       2     rain grew louder seemed closer cjoey sat still\n",
       "26        1                                      climbed fence\n",
       "180       2                          trembled looked fear eyes\n",
       "43        1  imagine someone burning book page page give us...\n",
       "130       2     slow steep walk stairs eevery day fs f thought"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train[['type', 'cleaned_text']] #전처리 1차 끝!\n",
    "train_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1</td>\n",
       "      <td>elroy drove boat fifteen minutes reached canad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>2</td>\n",
       "      <td>saw hammer dangled belt adze hooked one thumbn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2</td>\n",
       "      <td>mother would take walks watch glow chapel fs s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>bill frightened thought someone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2</td>\n",
       "      <td>run stairs let dog inhale breakfast scramble b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                       cleaned_text\n",
       "index                                                         \n",
       "87        1  elroy drove boat fifteen minutes reached canad...\n",
       "208       2  saw hammer dangled belt adze hooked one thumbn...\n",
       "138       2  mother would take walks watch glow chapel fs s...\n",
       "21        1                    bill frightened thought someone\n",
       "152       2  run stairs let dog inhale breakfast scramble b..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = test[['type', 'cleaned_text']]  #전처리 1차 끝!\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type            object\n",
       "cleaned_text    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = test_dataset.astype(str)\n",
    "df_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type            object\n",
       "cleaned_text    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = train_dataset.astype(str)\n",
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['cleaned_text', 'type']]\n",
    "df_test = df_test[['cleaned_text', 'type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>rain grew louder seemed closer cjoey sat still</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>climbed fence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>trembled looked fear eyes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>imagine someone burning book page page give us...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>slow steep walk stairs eevery day fs f thought</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            cleaned_text type\n",
       "index                                                        \n",
       "132       rain grew louder seemed closer cjoey sat still    2\n",
       "26                                         climbed fence    1\n",
       "180                            trembled looked fear eyes    2\n",
       "43     imagine someone burning book page page give us...    1\n",
       "130       slow steep walk stairs eevery day fs f thought    2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('datasets/train_datasets.csv', index=False, header=False, sep=',')\n",
    "df_test.to_csv('datasets/test_datasets.csv', index=False, header=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 개수 : 205\n",
      "테스트 샘플의 개수 : 51\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data # torchtext.data 임포트\n",
    "\n",
    "# 필드 정의\n",
    "\n",
    "LABEL = data.Field(sequential=False,\n",
    "                   use_vocab=False,\n",
    "                   batch_first=False,\n",
    "                   is_target=True)\n",
    "\n",
    "TEXT = data.Field(sequential=True,\n",
    "                  use_vocab=True,\n",
    "                  tokenize=str.split,\n",
    "                  lower=True,\n",
    "                  batch_first=True,\n",
    "                  fix_length=20)\n",
    "\n",
    "\n",
    "from torchtext.data import TabularDataset\n",
    "\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "        path='datasets/', train='train_datasets.csv', test='test_datasets.csv', format='csv',\n",
    "        fields=[('text', TEXT), ('label', LABEL)], skip_header=True)\n",
    "\n",
    "\n",
    "print('훈련 샘플의 개수 : {}'.format(len(train_data)))\n",
    "print('테스트 샘플의 개수 : {}'.format(len(test_data)))\n",
    "\n",
    "#ref: https://wikidocs.net/60314"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['imagine', 'someone', 'burning', 'book', 'page', 'page', 'give', 'us', 'incorrect', 'knowledge'], 'label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data[2])) # 성공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 577\n",
      "Token for \"<unk>\": 0\n",
      "Token for \"<pad>\": 1\n"
     ]
    }
   ],
   "source": [
    " #단어장 생성\n",
    "TEXT.build_vocab(train_data)\n",
    "TEXT.build_vocab(test_data)\n",
    "\n",
    "#단어장 생성 확인\n",
    "print('Total vocabulary: {}'.format(len(TEXT.vocab)))\n",
    "print('Token for \"<unk>\": {}'.format(TEXT.vocab.stoi['<unk>']))\n",
    "print('Token for \"<pad>\": {}'.format(TEXT.vocab.stoi['<pad>']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('text', <torchtext.data.field.Field object at 0x1a4ac65850>), ('label', <torchtext.data.field.Field object at 0x1a4ac9bdd0>)])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.fields.items()) # tex, label 로 구분되어 있는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('text', <torchtext.data.field.Field object at 0x1a4ac65850>), ('label', <torchtext.data.field.Field object at 0x1a4ac9bdd0>)])\n"
     ]
    }
   ],
   "source": [
    "print(test_data.fields.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, min_freq=1, max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 1526\n"
     ]
    }
   ],
   "source": [
    "print('단어 집합의 크기 : {}'.format(len(TEXT.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x1a532c0350>>, {'<unk>': 0, '<pad>': 1, 'ft': 2, 'h': 3, 'fs': 4, 'could': 5, 'like': 6, 'one': 7, 'felt': 8, 'house': 9, 'door': 10, 'face': 11, 'red': 12, 'time': 13, 'behind': 14, 'black': 15, 'cold': 16, 'long': 17, 'never': 18, 'see': 19, 'walked': 20, 'back': 21, 'every': 22, 'hands': 23, 'little': 24, 'looked': 25, 'night': 26, 'said': 27, 'stairs': 28, 'window': 29, 'another': 30, 'away': 31, 'c': 32, 'came': 33, 'f': 34, 'garden': 35, 'made': 36, 'old': 37, 'room': 38, 'sun': 39, 'two': 40, 'went': 41, 'white': 42, 'would': 43, 'arms': 44, 'around': 45, 'blood': 46, 'bright': 47, 'castle': 48, 'dark': 49, 'darkness': 50, 'day': 51, 'dress': 52, 'even': 53, 'eyes': 54, 'father': 55, 'feet': 56, 'front': 57, 'gi': 58, 'glass': 59, 'home': 60, 'james': 61, 'knew': 62, 'last': 63, 'late': 64, 'light': 65, 'man': 66, 'nervous': 67, 'reached': 68, 'stood': 69, 'village': 70, 'afraid': 71, 'air': 72, 'boat': 73, 'find': 74, 'first': 75, 'hand': 76, 'head': 77, 'held': 78, 'hot': 79, 'know': 80, 'lights': 81, 'mother': 82, 'open': 83, 'page': 84, 'people': 85, 'really': 86, 'saw': 87, 'slowly': 88, 'smell': 89, 'stepped': 90, 'storm': 91, 'though': 92, 'thought': 93, 'took': 94, 'town': 95, 'wind': 96, 'yellow': 97, 'angry': 98, 'anything': 99, 'bottom': 100, 'branch': 101, 'breath': 102, 'brother': 103, 'crunched': 104, 'decorated': 105, 'dry': 106, 'engine': 107, 'far': 108, 'fast': 109, 'fell': 110, 'fern': 111, 'go': 112, 'going': 113, 'good': 114, 'got': 115, 'green': 116, 'hair': 117, 'heard': 118, 'heart': 119, 'hill': 120, 'horse': 121, 'jim': 122, 'joey': 123, 'kept': 124, 'left': 125, 'lonely': 126, 'looks': 127, 'lost': 128, 'malcolm': 129, 'many': 130, 'metres': 131, 'mr': 132, 'rain': 133, 'ran': 134, 'river': 135, 'running': 136, 'sally': 137, 'side': 138, 'slammed': 139, 'small': 140, 'steps': 141, 'still': 142, 'street': 143, 'summer': 144, 'sweat': 145, 'table': 146, 'thoughts': 147, 'three': 148, 'threw': 149, 'tired': 150, 'top': 151, 'toward': 152, 'turned': 153, 'wall': 154, 'walls': 155, 'want': 156, 'water': 157, 'way': 158, 'weather': 159, 'well': 160, 'across': 161, 'alone': 162, 'asked': 163, 'bad': 164, 'beautiful': 165, 'beneath': 166, 'bit': 167, 'bite': 168, 'blair': 169, 'blue': 170, 'books': 171, 'brick': 172, 'broken': 173, 'carl': 174, 'cheerful': 175, 'cheese': 176, 'city': 177, 'clara': 178, 'clock': 179, 'close': 180, 'clouds': 181, 'cut': 182, 'enough': 183, 'favorite': 184, 'fear': 185, 'filled': 186, 'fingers': 187, 'five': 188, 'floor': 189, 'full': 190, 'give': 191, 'ground': 192, 'hated': 193, 'high': 194, 'hours': 195, 'katie': 196, 'keep': 197, 'kill': 198, 'leaves': 199, 'let': 200, 'life': 201, 'look': 202, 'looking': 203, 'loved': 204, 'mary': 205, 'meet': 206, 'mike': 207, 'minutes': 208, 'mouth': 209, 'must': 210, 'newspaper': 211, 'next': 212, 'oliver': 213, 'outside': 214, 'pepperoni': 215, 'pictures': 216, 'pizza': 217, 'place': 218, 'remember': 219, 'right': 220, 'school': 221, 'second': 222, 'seemed': 223, 'seen': 224, 'shadow': 225, 'shaking': 226, 'sky': 227, 'smile': 228, 'smoking': 229, 'someone': 230, 'spray': 231, 'spread': 232, 'standing': 233, 'streets': 234, 'tell': 235, 'things': 236, 'times': 237, 'today': 238, 'tree': 239, 'twenty': 240, 'warm': 241, 'watched': 242, 'work': 243, 'worked': 244, 'year': 245, 'aaron': 246, 'ached': 247, 'ahead': 248, 'aimed': 249, 'alvin': 250, 'among': 251, 'anxiously': 252, 'archie': 253, 'attractive': 254, 'author': 255, 'ax': 256, 'bag': 257, 'bar': 258, 'bathroom': 259, 'beach': 260, 'became': 261, 'bitter': 262, 'born': 263, 'bowen': 264, 'box': 265, 'bubbled': 266, 'burned': 267, 'burning': 268, 'busy': 269, 'candles': 270, 'canopy': 271, 'cars': 272, 'center': 273, 'chan': 274, 'chapter': 275, 'cheek': 276, 'chest': 277, 'children': 278, 'christmas': 279, 'church': 280, 'cliff': 281, 'climbing': 282, 'clutched': 283, 'coffee': 284, 'college': 285, 'commander': 286, 'complete': 287, 'creditors': 288, 'crowd': 289, 'cynthia': 290, 'damp': 291, 'dazzling': 292, 'dead': 293, 'deep': 294, 'delicious': 295, 'described': 296, 'diagilo': 297, 'dirty': 298, 'doctor': 299, 'drain': 300, 'dresser': 301, 'drew': 302, 'dust': 303, 'ears': 304, 'earth': 305, 'elected': 306, 'elroy': 307, 'emily': 308, 'empty': 309, 'eyed': 310, 'fabric': 311, 'factories': 312, 'fall': 313, 'fallen': 314, 'family': 315, 'fire': 316, 'florentine': 317, 'fm': 318, 'foothold': 319, 'forced': 320, 'forward': 321, 'found': 322, 'frames': 323, 'gate': 324, 'georgie': 325, 'get': 326, 'ggood': 327, 'gibson': 328, 'god': 329, 'gold': 330, 'grandfather': 331, 'grass': 332, 'gripped': 333, 'guards': 334, 'gyou': 335, 'halls': 336, 'handle': 337, 'hard': 338, 'heaps': 339, 'hear': 340, 'heave': 341, 'help': 342, 'hem': 343, 'hockey': 344, 'hoghouse': 345, 'hung': 346, 'ice': 347, 'imagine': 348, 'ink': 349, 'insurance': 350, 'iona': 351, 'ireland': 352, 'isolated': 353, 'jay': 354, 'john': 355, 'knuckles': 356, 'leaving': 357, 'letting': 358, 'library': 359, 'lightning': 360, 'liked': 361, 'lit': 362, 'love': 363, 'lucy': 364, 'luke': 365, 'madam': 366, 'making': 367, 'massive': 368, 'mayor': 369, 'mean': 370, 'michael': 371, 'might': 372, 'milk': 373, 'mind': 374, 'morning': 375, 'mountains': 376, 'moving': 377, 'mozzarella': 378, 'mushrooms': 379, 'music': 380, 'name': 381, 'new': 382, 'noise': 383, 'nose': 384, 'nurse': 385, 'ocean': 386, 'october': 387, 'ophelia': 388, 'pale': 389, 'pancake': 390, 'paper': 391, 'party': 392, 'passed': 393, 'past': 394, 'patio': 395, 'peace': 396, 'peaceful': 397, 'person': 398, 'pigs': 399, 'ping': 400, 'playing': 401, 'pomfrey': 402, 'pong': 403, 'pool': 404, 'popped': 405, 'pulled': 406, 'pulling': 407, 'puppy': 408, 'purple': 409, 'put': 410, 'quickly': 411, 'reach': 412, 'reading': 413, 'ready': 414, 'release': 415, 'roar': 416, 'rock': 417, 'roger': 418, 'rolling': 419, 'run': 420, 'safety': 421, 'sauce': 422, 'sausage': 423, 'say': 424, 'sea': 425, 'seat': 426, 'sent': 427, 'served': 428, 'sewers': 429, 'sharp': 430, 'sheer': 431, 'sheet': 432, 'sheets': 433, 'shiny': 434, 'shirt': 435, 'shrieked': 436, 'signals': 437, 'since': 438, 'single': 439, 'sister': 440, 'sisters': 441, 'smelled': 442, 'smoke': 443, 'soldiers': 444, 'somehow': 445, 'something': 446, 'sometimes': 447, 'sorority': 448, 'speak': 449, 'sports': 450, 'spreading': 451, 'spring': 452, 'stale': 453, 'steadily': 454, 'steam': 455, 'steep': 456, 'step': 457, 'stomach': 458, 'straight': 459, 'straightened': 460, 'strokes': 461, 'strong': 462, 'stumbled': 463, 'sweet': 464, 'sword': 465, 'take': 466, 'taking': 467, 'taste': 468, 'teeth': 469, 'thick': 470, 'think': 471, 'thump': 472, 'tiffany': 473, 'tiny': 474, 'tom': 475, 'tomato': 476, 'trade': 477, 'trembling': 478, 'trying': 479, 'twisted': 480, 'us': 481, 'usurers': 482, 'veins': 483, 'voices': 484, 'waitress': 485, 'war': 486, 'watching': 487, 'waves': 488, 'weak': 489, 'wearing': 490, 'weight': 491, 'whole': 492, 'wide': 493, 'wife': 494, 'wild': 495, 'windows': 496, 'without': 497, 'wood': 498, 'worry': 499, 'wrapped': 500, 'yards': 501, 'years': 502, 'abandoned': 503, 'active': 504, 'admired': 505, 'afterward': 506, 'ago': 507, 'alarm': 508, 'allowed': 509, 'almost': 510, 'already': 511, 'also': 512, 'always': 513, 'amid': 514, 'amount': 515, 'amy': 516, 'animal': 517, 'animals': 518, 'annoying': 519, 'anymore': 520, 'anyone': 521, 'appear': 522, 'appeared': 523, 'apple': 524, 'applying': 525, 'arable': 526, 'arrived': 527, 'ashes': 528, 'atop': 529, 'attention': 530, 'autumn': 531, 'avoided': 532, 'awaken': 533, 'aware': 534, 'backpack': 535, 'backward': 536, 'baking': 537, 'bang': 538, 'barcel': 539, 'barely': 540, 'bark': 541, 'beadle': 542, 'bearings': 543, 'beating': 544, 'beatty': 545, 'becomes': 546, 'bedecked': 547, 'beg': 548, 'belt': 549, 'bench': 550, 'bend': 551, 'beside': 552, 'best': 553, 'better': 554, 'bill': 555, 'biting': 556, 'blackened': 557, 'blackout': 558, 'blank': 559, 'blight': 560, 'blinding': 561, 'blinked': 562, 'bloated': 563, 'blond': 564, 'bloodied': 565, 'blotting': 566, 'blushing': 567, 'board': 568, 'boarded': 569, 'bodies': 570, 'body': 571, 'boiled': 572, 'bone': 573, 'book': 574, 'boots': 575, 'boring': 576, 'bouncing': 577, 'bow': 578, 'boxes': 579, 'brand': 580, 'break': 581, 'breathe': 582, 'breathing': 583, 'breeze': 584, 'brenda': 585, 'brighter': 586, 'bring': 587, 'bringing': 588, 'broadsword': 589, 'broke': 590, 'brought': 591, 'brown': 592, 'brush': 593, 'brushed': 594, 'buck': 595, 'bud': 596, 'buds': 597, 'building': 598, 'bulbs': 599, 'bulging': 600, 'bullied': 601, 'bunches': 602, 'bus': 603, 'butterflies': 604, 'butterfly': 605, 'buzz': 606, 'cab': 607, 'calcium': 608, 'calle': 609, 'calm': 610, 'canadian': 611, 'canal': 612, 'candle': 613, 'cane': 614, 'canuda': 615, 'cappuccino': 616, 'caps': 617, 'captain': 618, 'car': 619, 'cared': 620, 'carefully': 621, 'carpenter': 622, 'carpet': 623, 'carriage': 624, 'carried': 625, 'carry': 626, 'carrying': 627, 'carved': 628, 'cascading': 629, 'cast': 630, 'cat': 631, 'catch': 632, 'cattle': 633, 'cauldron': 634, 'celebrated': 635, 'chain': 636, 'chair': 637, 'champagne': 638, 'changing': 639, 'chapel': 640, 'charming': 641, 'chased': 642, 'chasing': 643, 'cheeks': 644, 'cheerfully': 645, 'chill': 646, 'chilled': 647, 'chocolate': 648, 'choked': 649, 'choking': 650, 'choppy': 651, 'cigarette': 652, 'circle': 653, 'circus': 654, 'clammy': 655, 'clap': 656, 'class': 657, 'classroom': 658, 'clean': 659, 'clear': 660, 'clearly': 661, 'climb': 662, 'climbed': 663, 'clipped': 664, 'cloth': 665, 'cloud': 666, 'clumps': 667, 'cneither': 668, 'coat': 669, 'coated': 670, 'cobblestones': 671, 'cocktail': 672, 'cocktails': 673, 'colds': 674, 'collar': 675, 'collect': 676, 'colorful': 677, 'come': 678, 'comes': 679, 'competes': 680, 'complaining': 681, 'confused': 682, 'connect': 683, 'considering': 684, 'consumed': 685, 'content': 686, 'continued': 687, 'cookies': 688, 'cooking': 689, 'cores': 690, 'correct': 691, 'cottages': 692, 'county': 693, 'course': 694, 'coveralls': 695, 'covered': 696, 'covering': 697, 'covers': 698, 'crack': 699, 'crammed': 700, 'crashed': 701, 'crawling': 702, 'crazy': 703, 'creaking': 704, 'creaky': 705, 'creamy': 706, 'creature': 707, 'creek': 708, 'creeping': 709, 'creepy': 710, 'crepe': 711, 'crept': 712, 'cried': 713, 'crop': 714, 'crossed': 715, 'crossword': 716, 'crow': 717, 'crowded': 718, 'crumpling': 719, 'crying': 720, 'crystal': 721, 'cupboard': 722, 'curbing': 723, 'curse': 724, 'curses': 725, 'cursing': 726, 'curtain': 727, 'customers': 728, 'dairy': 729, 'daisy': 730, 'dashed': 731, 'dave': 732, 'dawn': 733, 'dearly': 734, 'deathly': 735, 'debated': 736, 'decaying': 737, 'decent': 738, 'decide': 739, 'decided': 740, 'decisively': 741, 'deepening': 742, 'delicately': 743, 'demanding': 744, 'descended': 745, 'desert': 746, 'desperately': 747, 'destroyed': 748, 'different': 749, 'diligent': 750, 'diligently': 751, 'dim': 752, 'dimly': 753, 'dirtier': 754, 'discerned': 755, 'distance': 756, 'distorted': 757, 'dog': 758, 'dominique': 759, 'donuts': 760, 'doors': 761, 'downstairs': 762, 'drag': 763, 'drank': 764, 'drawer': 765, 'drenched': 766, 'dressed': 767, 'dressing': 768, 'drink': 769, 'drinker': 770, 'drinks': 771, 'drive': 772, 'driver': 773, 'dropped': 774, 'drowning': 775, 'ducked': 776, 'due': 777, 'dug': 778, 'dull': 779, 'dung': 780, 'dusted': 781, 'dye': 782, 'ear': 783, 'early': 784, 'easily': 785, 'eerily': 786, 'eevery': 787, 'effort': 788, 'eh': 789, 'ei': 790, 'eight': 791, 'elephant': 792, 'embarrassed': 793, 'embrace': 794, 'embraced': 795, 'emitted': 796, 'end': 797, 'energy': 798, 'eno': 799, 'entered': 800, 'entirely': 801, 'enveloped': 802, 'especially': 803, 'ethump': 804, 'evening': 805, 'everyone': 806, 'everything': 807, 'evinrude': 808, 'exhausted': 809, 'exhaustion': 810, 'expelled': 811, 'expensive': 812, 'explosion': 813, 'faint': 814, 'faintly': 815, 'faithlessness': 816, 'false': 817, 'familiar': 818, 'fan': 819, 'fancy': 820, 'farther': 821, 'faster': 822, 'fd': 823, 'feature': 824, 'feel': 825, 'feeling': 826, 'fence': 827, 'fetid': 828, 'fifteen': 829, 'fifty': 830, 'figuring': 831, 'filmed': 832, 'filthy': 833, 'finger': 834, 'fingernail': 835, 'fiona': 836, 'fireplace': 837, 'fish': 838, 'fishtail': 839, 'flaking': 840, 'flame': 841, 'flapped': 842, 'flares': 843, 'flat': 844, 'flesh': 845, 'flew': 846, 'flickered': 847, 'fll': 848, 'floating': 849, 'flooding': 850, 'floorboard': 851, 'flower': 852, 'flowers': 853, 'flowing': 854, 'flying': 855, 'fog': 856, 'fogged': 857, 'folded': 858, 'football': 859, 'footsteps': 860, 'forehead': 861, 'foreign': 862, 'foreigner': 863, 'forest': 864, 'form': 865, 'formed': 866, 'fought': 867, 'four': 868, 'friend': 869, 'frightened': 870, 'frigid': 871, 'frost': 872, 'frothy': 873, 'frozen': 874, 'fueled': 875, 'fullness': 876, 'fully': 877, 'fur': 878, 'fury': 879, 'fve': 880, 'ga': 881, 'gabilan': 882, 'game': 883, 'gardener': 884, 'gardens': 885, 'garlands': 886, 'gave': 887, 'gdarlene': 888, 'gdo': 889, 'general': 890, 'gentle': 891, 'getting': 892, 'gget': 893, 'gholy': 894, 'ginny': 895, 'girl': 896, 'girlfriend': 897, 'girls': 898, 'given': 899, 'giving': 900, 'glances': 901, 'glistened': 902, 'glistening': 903, 'glowing': 904, 'glumly': 905, 'glutinous': 906, 'gnice': 907, 'gnot': 908, 'goh': 909, 'gotten': 910, 'gpoint': 911, 'grabbed': 912, 'grabbing': 913, 'graceful': 914, 'graciously': 915, 'grasping': 916, 'gravity': 917, 'gray': 918, 'great': 919, 'grimace': 920, 'grizzly': 921, 'groaned': 922, 'grounds': 923, 'grow': 924, 'gsit': 925, 'gslice': 926, 'gstop': 927, 'guessed': 928, 'gushing': 929, 'gutter': 930, 'hail': 931, 'half': 932, 'hallway': 933, 'halted': 934, 'hammering': 935, 'hanging': 936, 'hansom': 937, 'happily': 938, 'haunted': 939, 'haze': 940, 'healthy': 941, 'heaven': 942, 'heavy': 943, 'helmets': 944, 'hidden': 945, 'higher': 946, 'hills': 947, 'hinges': 948, 'hit': 949, 'holt': 950, 'homework': 951, 'honest': 952, 'hood': 953, 'hoofbeats': 954, 'hopped': 955, 'horsepower': 956, 'houses': 957, 'huddled': 958, 'hugged': 959, 'human': 960, 'humid': 961, 'hungarian': 962, 'hungry': 963, 'hunk': 964, 'hurled': 965, 'hurried': 966, 'husband': 967, 'iced': 968, 'ideal': 969, 'ignited': 970, 'ill': 971, 'immediately': 972, 'impenetrable': 973, 'impossible': 974, 'impregnated': 975, 'impression': 976, 'incorrect': 977, 'ingredient': 978, 'inside': 979, 'instantly': 980, 'instead': 981, 'intensity': 982, 'interrupted': 983, 'invisible': 984, 'iron': 985, 'itched': 986, 'jack': 987, 'jerked': 988, 'jews': 989, 'job': 990, 'joy': 991, 'julain': 992, 'juli': 993, 'jump': 994, 'jumped': 995, 'junkyard': 996, 'key': 997, 'kicked': 998, 'kind': 999, 'kindergarten': 1000, 'kinds': 1001, 'kiosk': 1002, 'kiss': 1003, 'kitchen': 1004, 'kitchenware': 1005, 'knee': 1006, 'knight': 1007, 'knowing': 1008, 'knowledge': 1009, 'lady': 1010, 'languages': 1011, 'lash': 1012, 'latin': 1013, 'lawns': 1014, 'lay': 1015, 'layered': 1016, 'layers': 1017, 'leaden': 1018, 'leaned': 1019, 'leather': 1020, 'leg': 1021, 'lemonade': 1022, 'lifetime': 1023, 'lift': 1024, 'lifting': 1025, 'lifts': 1026, 'lightly': 1027, 'lime': 1028, 'lip': 1029, 'lips': 1030, 'living': 1031, 'london': 1032, 'lookin': 1033, 'lots': 1034, 'loud': 1035, 'lovely': 1036, 'low': 1037, 'lower': 1038, 'lurches': 1039, 'luscious': 1040, 'machines': 1041, 'mactalde': 1042, 'mad': 1043, 'madness': 1044, 'make': 1045, 'managed': 1046, 'matters': 1047, 'maw': 1048, 'may': 1049, 'meal': 1050, 'medicine': 1051, 'medieval': 1052, 'melancholy': 1053, 'melted': 1054, 'men': 1055, 'mesmerized': 1056, 'mess': 1057, 'met': 1058, 'metal': 1059, 'metallic': 1060, 'middle': 1061, 'mince': 1062, 'mine': 1063, 'minute': 1064, 'misbehaving': 1065, 'miss': 1066, 'missed': 1067, 'missing': 1068, 'moishe': 1069, 'mold': 1070, 'molly': 1071, 'mom': 1072, 'moment': 1073, 'monotonously': 1074, 'montag': 1075, 'moon': 1076, 'mornings': 1077, 'moths': 1078, 'motor': 1079, 'mounted': 1080, 'moved': 1081, 'movement': 1082, 'movements': 1083, 'mowing': 1084, 'mrs': 1085, 'much': 1086, 'muddy': 1087, 'n': 1088, 'named': 1089, 'narrow': 1090, 'narrowing': 1091, 'navigated': 1092, 'nearer': 1093, 'needed': 1094, 'needs': 1095, 'neighborhood': 1096, 'nice': 1097, 'ninja': 1098, 'noises': 1099, 'north': 1100, 'nostrils': 1101, 'nothing': 1102, 'notion': 1103, 'notions': 1104, 'oath': 1105, 'occured': 1106, 'odours': 1107, 'older': 1108, 'ones': 1109, 'onto': 1110, 'oooh': 1111, 'opaque': 1112, 'opened': 1113, 'opera': 1114, 'opponent': 1115, 'opulent': 1116, 'orange': 1117, 'orchestra': 1118, 'ordered': 1119, 'ordinary': 1120, 'orias': 1121, 'others': 1122, 'outboard': 1123, 'outfits': 1124, 'outline': 1125, 'outward': 1126, 'owl': 1127, 'oyster': 1128, 'pain': 1129, 'painful': 1130, 'paint': 1131, 'palms': 1132, 'pancakes': 1133, 'papa': 1134, 'pardon': 1135, 'parked': 1136, 'part': 1137, 'parties': 1138, 'pass': 1139, 'passenger': 1140, 'passionate': 1141, 'patches': 1142, 'path': 1143, 'pavements': 1144, 'pay': 1145, 'peeled': 1146, 'pepperup': 1147, 'perched': 1148, 'percy': 1149, 'perfect': 1150, 'permeate': 1151, 'petals': 1152, 'philosophies': 1153, 'picking': 1154, 'picks': 1155, 'pies': 1156, 'pig': 1157, 'piled': 1158, 'piles': 1159, 'pink': 1160, 'piss': 1161, 'piston': 1162, 'pitcher': 1163, 'pitches': 1164, 'places': 1165, 'plane': 1166, 'planned': 1167, 'plants': 1168, 'platform': 1169, 'players': 1170, 'plumber': 1171, 'plump': 1172, 'plunger': 1173, 'plush': 1174, 'point': 1175, 'pointed': 1176, 'police': 1177, 'polished': 1178, 'polluted': 1179, 'poppies': 1180, 'porsche': 1181, 'potion': 1182, 'pounded': 1183, 'pouring': 1184, 'powder': 1185, 'preferred': 1186, 'presents': 1187, 'press': 1188, 'pressed': 1189, 'pretty': 1190, 'products': 1191, 'progress': 1192, 'promise': 1193, 'promises': 1194, 'prosper': 1195, 'protagonist': 1196, 'protein': 1197, 'provolone': 1198, 'public': 1199, 'puck': 1200, 'pull': 1201, 'pulsing': 1202, 'purplish': 1203, 'pushed': 1204, 'puzzle': 1205, 'quick': 1206, 'raced': 1207, 'rather': 1208, 'rattled': 1209, 'rattling': 1210, 'razor': 1211, 'read': 1212, 'readers': 1213, 'realized': 1214, 'reason': 1215, 'reef': 1216, 'reflected': 1217, 'reflecting': 1218, 'reflexes': 1219, 'refuge': 1220, 'regent': 1221, 'relief': 1222, 'remaining': 1223, 'remove': 1224, 'replied': 1225, 'restaurant': 1226, 'revolted': 1227, 'ribbons': 1228, 'riding': 1229, 'rifles': 1230, 'rims': 1231, 'ring': 1232, 'rivers': 1233, 'roared': 1234, 'romantic': 1235, 'roof': 1236, 'roofs': 1237, 'rooms': 1238, 'rose': 1239, 'rosettes': 1240, 'rotten': 1241, 'rotting': 1242, 'round': 1243, 'rounds': 1244, 'rows': 1245, 'rubble': 1246, 'rumbles': 1247, 'runt': 1248, 'rushing': 1249, 'rustle': 1250, 'salina': 1251, 'sand': 1252, 'sapphire': 1253, 'says': 1254, 'scared': 1255, 'scarier': 1256, 'scaring': 1257, 'scent': 1258, 'scheduled': 1259, 'scrabbling': 1260, 'screamed': 1261, 'screaming': 1262, 'screech': 1263, 'screen': 1264, 'scribbled': 1265, 'sealskin': 1266, 'seconds': 1267, 'seeds': 1268, 'seem': 1269, 'semi': 1270, 'senior': 1271, 'sentence': 1272, 'serving': 1273, 'set': 1274, 'setting': 1275, 'several': 1276, 'sex': 1277, 'shadows': 1278, 'shake': 1279, 'shapes': 1280, 'sharpening': 1281, 'sharply': 1282, 'shattered': 1283, 'sheathed': 1284, 'shelves': 1285, 'shifted': 1286, 'shine': 1287, 'shit': 1288, 'shivering': 1289, 'shook': 1290, 'shops': 1291, 'shore': 1292, 'shot': 1293, 'shout': 1294, 'shouted': 1295, 'show': 1296, 'shower': 1297, 'shroud': 1298, 'shut': 1299, 'sides': 1300, 'sigh': 1301, 'sighed': 1302, 'sighet': 1303, 'silenced': 1304, 'silent': 1305, 'silently': 1306, 'silly': 1307, 'silver': 1308, 'simmering': 1309, 'simple': 1310, 'sir': 1311, 'sits': 1312, 'situation': 1313, 'six': 1314, 'sizes': 1315, 'skies': 1316, 'skin': 1317, 'slack': 1318, 'slam': 1319, 'slapping': 1320, 'slash': 1321, 'sleep': 1322, 'sleepy': 1323, 'slept': 1324, 'slice': 1325, 'slick': 1326, 'slip': 1327, 'slow': 1328, 'slushies': 1329, 'smaller': 1330, 'smallest': 1331, 'smelling': 1332, 'smooth': 1333, 'snuck': 1334, 'snuffed': 1335, 'soggy': 1336, 'sold': 1337, 'sole': 1338, 'somewhere': 1339, 'soon': 1340, 'soul': 1341, 'sound': 1342, 'spaces': 1343, 'span': 1344, 'spans': 1345, 'spat': 1346, 'spate': 1347, 'speech': 1348, 'speed': 1349, 'spilled': 1350, 'spilling': 1351, 'spit': 1352, 'sport': 1353, 'spun': 1354, 'spurred': 1355, 'squaked': 1356, 'squinted': 1357, 'squinting': 1358, 'stacks': 1359, 'staff': 1360, 'stains': 1361, 'staircase': 1362, 'staleness': 1363, 'stand': 1364, 'stars': 1365, 'starts': 1366, 'state': 1367, 'station': 1368, 'stay': 1369, 'stealth': 1370, 'stepping': 1371, 'stiff': 1372, 'stock': 1373, 'stone': 1374, 'stop': 1375, 'strawberry': 1376, 'streamers': 1377, 'streetlamps': 1378, 'stripped': 1379, 'stroke': 1380, 'struggled': 1381, 'studded': 1382, 'students': 1383, 'stuffed': 1384, 'sucked': 1385, 'sudden': 1386, 'suddenly': 1387, 'suffer': 1388, 'sugar': 1389, 'sunshine': 1390, 'suppose': 1391, 'sure': 1392, 'surface': 1393, 'surprised': 1394, 'surrender': 1395, 'susie': 1396, 'suzie': 1397, 'sweaty': 1398, 'sweeping': 1399, 'swiftly': 1400, 'swimmers': 1401, 'swipes': 1402, 'switched': 1403, 'tables': 1404, 'taken': 1405, 'talented': 1406, 'tall': 1407, 'taller': 1408, 'talons': 1409, 'tasted': 1410, 'tears': 1411, 'temperature': 1412, 'ten': 1413, 'tensed': 1414, 'terms': 1415, 'terribly': 1416, 'text': 1417, 'thatched': 1418, 'thickly': 1419, 'thin': 1420, 'thing': 1421, 'third': 1422, 'throat': 1423, 'throttle': 1424, 'thunder': 1425, 'thunderboy': 1426, 'thundered': 1427, 'tiger': 1428, 'tissue': 1429, 'tobacco': 1430, 'told': 1431, 'tonight': 1432, 'toolbox': 1433, 'tools': 1434, 'tossed': 1435, 'tough': 1436, 'treats': 1437, 'trembled': 1438, 'tried': 1439, 'tripped': 1440, 'truly': 1441, 'tune': 1442, 'turbulent': 1443, 'turning': 1444, 'tv': 1445, 'twinkling': 1446, 'ugly': 1447, 'umbrella': 1448, 'uneven': 1449, 'unnatural': 1450, 'unsuccessful': 1451, 'upcoming': 1452, 'upon': 1453, 'upper': 1454, 'upstream': 1455, 'used': 1456, 'usual': 1457, 'vance': 1458, 'various': 1459, 'vast': 1460, 'vegetables': 1461, 'vehicle': 1462, 'veiled': 1463, 'vellum': 1464, 'vest': 1465, 'view': 1466, 'vitamins': 1467, 'vivid': 1468, 'voice': 1469, 'wafers': 1470, 'waft': 1471, 'waist': 1472, 'wait': 1473, 'waiting': 1474, 'walk': 1475, 'wanted': 1476, 'warmed': 1477, 'warmth': 1478, 'wash': 1479, 'watch': 1480, 'watermelon': 1481, 'waters': 1482, 'wearin': 1483, 'weasley': 1484, 'wedding': 1485, 'weightlifting': 1486, 'welcome': 1487, 'welsh': 1488, 'west': 1489, 'wet': 1490, 'whenever': 1491, 'whether': 1492, 'whisperings': 1493, 'whistling': 1494, 'wildly': 1495, 'willed': 1496, 'wilting': 1497, 'windless': 1498, 'winged': 1499, 'wipe': 1500, 'wiped': 1501, 'wished': 1502, 'witcham': 1503, 'woke': 1504, 'women': 1505, 'wondered': 1506, 'wonderful': 1507, 'wooden': 1508, 'words': 1509, 'wore': 1510, 'working': 1511, 'worn': 1512, 'wrenches': 1513, 'wretched': 1514, 'writing': 1515, 'wrong': 1516, 'yard': 1517, 'yawned': 1518, 'yelled': 1519, 'yells': 1520, 'yogurt': 1521, 'york': 1522, 'young': 1523, 'younger': 1524, 'yowl': 1525})\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.stoi) # 생성된 집합 내 단어들을 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토치텍스트의 테이터로더 생성\n",
    "\n",
    "from torchtext.data import Iterator\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 미니 배치 수 : 13\n",
      "테스트 데이터의 미니 배치 수 : 4\n"
     ]
    }
   ],
   "source": [
    "train_loader = Iterator(dataset=train_data, batch_size = batch_size)\n",
    "test_loader = Iterator(dataset=test_data, batch_size = batch_size)\n",
    "\n",
    "print('훈련 데이터의 미니 배치 수 : {}'.format(len(train_loader)))\n",
    "print('테스트 데이터의 미니 배치 수 : {}'.format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.batch.Batch'>\n"
     ]
    }
   ],
   "source": [
    "print(type(batch)) #미니배치 자료형 확인. 토치텍스늬 데이터로더는  'torchtext.data.batch.Batch'라는 객체를 가져온다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 264, 1026,  838, 1226,  211, 1508,  154,  265,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 199,  104,  166,   56,  623,   12,  330,  737,  592,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 217,  295,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 814, 1258,  453,  652,  443, 1058, 1101,  407,   11,  818,  920,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 432,  391,  559, 1454,  138, 1515,  660, 1203,   15,  349, 1038, 1013,\n",
      "         1417,  338, 1212, 1052,   76,    1,    1,    1],\n",
      "        [   7,   51,  862,  989,  811, 1303, 1069,  542,  863,  700,  633,  272,\n",
      "          962, 1177,  713, 1306,  233, 1368, 1169,  720],\n",
      "        [ 255,  354, 1458,  296, 1196,  122,  881,  126,  353,  960,  668,  201,\n",
      "          392,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [  47,   12, 1228,  753,  362,  270,  116,  665,  105,  279,  146,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 290,  436,  406, 1495,  117,  139,   10,  357,   24,  103,  162,  682,\n",
      "          933,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [  39, 1274,  805,  227,  129,   88,  153,   20,  152,   60, 1305,  142,\n",
      "           29,    5,   19, 1108,  103,   61,  487,  859],\n",
      "        [ 371, 1416,   71,   49,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 643,   73,  125, 1503,  143,  136,  109,  157,  136,  822,   73,  407,\n",
      "          248,  118,  742,  416,   87,  830,  501,  821],\n",
      "        [1510,  695,  625, 1173, 1059, 1433, 1513, 1459, 1315,  346, 1020,  549,\n",
      "           45, 1472,  911,   77,    3,   27,    1,    1],\n",
      "        [  37,  132,  274, 1456, 1429, 1500,  145,   11,  115,  414,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 383,  277,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [1130,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1]])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader)) # 16개씩 묶어줬음. 첫번째 미니배치에 저장\n",
    "\n",
    "print(batch.text) #첫번째 미니 배치의 text 필드를 호출해서 확인해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 473,    8,   71, 1183,   10,  744,  200,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader)) # 첫번째 미니배치\n",
    "print(batch.text[0]) # 첫번째 미니배치 중 첫번째 샘플"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:09, 12860.97lines/s]\n",
      "120000lines [00:17, 6766.86lines/s]\n",
      "7600lines [00:01, 5807.08lines/s]\n"
     ]
    }
   ],
   "source": [
    "#원본코드인데.. 이미 가공된 데이터셋을 ngrams 처리해서 불러오기 때문에 입력데이터를 dataset에 맞게 수정해야 한다.\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "NGRAMS = 2\n",
    "import os\n",
    "if not os.path.isdir('./data2'):\n",
    "    os.mkdir('./data2')\n",
    "    \n",
    "    \n",
    "#text_classification.DATASETS의 구조를 보고 결과데이터를 어떻게 생성하는지 분석하거나 이하 학습코드를 분석하자    \n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./data2', ngrams=NGRAMS, vocab=None) \n",
    "#ref : https://pytorch.org/text/datasets.html#ag-news\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3,\n",
       "  tensor([    131,       5,   23258,      27,    2922,     357,    2688,     769,\n",
       "              814,      14,      32,      15,      16,       6,     131,       7,\n",
       "              230,     293,     452,     836,    6438,      85,       2,      51,\n",
       "            43647,       2,   24372,       4,   60885,      51,  281059,       2,\n",
       "                0,       9,   21969,     115,       2,      51,  108539,       2,\n",
       "            36279,       4,      11,      81,      31,      90,      39,   23258,\n",
       "                6,      27,     357,    4090,    1698,      53,       5,     273,\n",
       "              821,       3,    1507,       7,       3,    1473,    3049,       2,\n",
       "             9821,   53919,  115850,   75043,   30252,  478273,  244818,     822,\n",
       "             4291,      43,      44,      46,     296,    2486,    2022,    8893,\n",
       "            23909,   49141,  381768,    9169,   19041,      89,     122,       0,\n",
       "            43648,   24031,   69185,   94356, 1174101,       0,  450795,       0,\n",
       "                0,  223922,  108592,     126,     122,       0,  178323,   35545,\n",
       "           409428,     735,     174,    3486,    2986,    3563,  119987,  102135,\n",
       "              151,   21657,   24613,  196267,  449408,     568,    9261,   16383,\n",
       "            10610,   12791,   10597,      29,    3937,  125919,   29219])),\n",
       " (3,\n",
       "  tensor([    398,    1371,    4357,     140,       4,     529,   17887,     769,\n",
       "              814,      14,      32,      15,      16,     398,     239,      85,\n",
       "                2,      51,   12202,       2,   36279,      11,      77,    1017,\n",
       "             3918,       6,      27,     621,    1304,       5,    1413,     450,\n",
       "             1535,   20318,    3524,    4357,       9,    1475,       6,    5662,\n",
       "           139374,     140,   17887,   10030,      25,    5818,     295,     374,\n",
       "             3237,     140,       2,   58236,  912220,  247465,    3730,   20914,\n",
       "                0,       0,     822,    4291,      43,      44,      46,   10183,\n",
       "             2133,    4850,      89,     122,       0,   27392,   35545,  132604,\n",
       "              161,  288511,  177353,   56884,     151,   37744,  115817,    1756,\n",
       "             5972,   49904, 1262338,  702497, 1079944,  965345,  128458,  175247,\n",
       "             5939,   33049,       0,  472516,       0,       0,       0,   11287,\n",
       "                0,  127381,  125934,       0,    3595])),\n",
       " (3,\n",
       "  tensor([  1743,   1191,   2678,    398,      5,   1044,   2419,    172,   5807,\n",
       "              14,     32,     15,     16,    506,    198,   7286,      4,      6,\n",
       "            1743,    288,   1118,   1847,      4,    500,    398,    239,     85,\n",
       "               2,      5,    172,     11,     77,     20,      3,    559,      7,\n",
       "               6,     27,   2419,    214,      8,    482,     17,     10,    415,\n",
       "           13976,     12,    592,   5611,    343,      2, 183509,      0, 326295,\n",
       "           29013,  36745,      0,  87826,      0,  15874,     43,     44,     46,\n",
       "           40085,      0, 430407, 103141,     87,  19259,      0, 404669,  43244,\n",
       "           10017,  29826,      0,   2133,   4850,     89,   2071,  10175,  39267,\n",
       "             161,  10293,    154,   5172,   3559,    107,    151, 262534,  14209,\n",
       "            6874,   2580,   6611,     23,   5736, 256315, 145528,  17037, 350126,\n",
       "               0,   4028])),\n",
       " (3,\n",
       "  tensor([  3270, 144492,  18571,    840,    368,      6,    334, 141753,  18571,\n",
       "               7,  62303,    136,     33,    109,   2508,      8,   6705,      4,\n",
       "             368,      4,    125,   5741,   1044,  68572,   2480,      2,      0,\n",
       "               0,      0,      0,  58315,  10751,      0,      0, 109318,      0,\n",
       "               0,   3087,    282,  21461,  18230,  49708,  49983,   5149,   9282,\n",
       "            7808,  38803,      0,      0,      0,  36581])),\n",
       " (3,\n",
       "  tensor([1155350,   17813,   46664,    1367,   46664,    1367,       4,     156,\n",
       "           422148,       4,   16838,      11, 1155350,       5,     633,     333,\n",
       "               34,   38763,       4,     601,    1395,       2,       0,       0,\n",
       "                0,       0,       0,   14019,    1323,       0,       0,       0,\n",
       "            20324,       0,       0,    1215,   34865,   15806,  801893,       0,\n",
       "             4382,   91929,   62739])),\n",
       " (3,\n",
       "  tensor([   9647,     111,       2,   19481,      17,      10,   13103,     339,\n",
       "            57385,   19481,   14857,      25,    2291,    3920,     740,       5,\n",
       "               35,   27554,     339,   45525,     301,      25,  221858,     130,\n",
       "                2,       0,    7864,   51327, 1098538,      23,  275296,       0,\n",
       "                0,   77111,       0,  221568,   63824,       0,       0,     958,\n",
       "             1149,   78427,       0,  152353,       0,   16709,       0,       0,\n",
       "             1272]))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[10:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  tensor([    572,     564,       2,    2326,   49106,     150,      88,       3,\n",
       "             1143,      14,      32,      15,      32,      16,  443749,       4,\n",
       "              572,     499,      17,      10,  741769,       7,  468770,       4,\n",
       "               52,    7019,    1050,     442,       2,   14341,     673,  141447,\n",
       "           326092,   55044,    7887,     411,    9870,  628642,      43,      44,\n",
       "              144,     145,  299709,  443750,   51274,     703,   14312,      23,\n",
       "          1111134,  741770,  411508,  468771,    3779,   86384,  135944,  371666,\n",
       "             4052])),\n",
       " (2,\n",
       "  tensor([  55003,    1474,    1150,    1832,    7559,      14,      32,      15,\n",
       "               32,      16,    1262,    1072,     436,   55003,     131,       4,\n",
       "           142576,      33,       6,    8062,      12,     756,  475640,       9,\n",
       "           991346,    3186,       8,       3,     698,     329,       4,      33,\n",
       "             6764, 1040465,   13979,      11,     278,     483,       7,       3,\n",
       "              172,       2,  659973,  193730, 1237754,  684719,  556644,      43,\n",
       "               44,     144,     145,   77775,   56578,   32382,  782124,   79225,\n",
       "             2908,  140697,  540900,    2031,   31960,   45339,   21562,  936430,\n",
       "          1282186,  578442,  991347,   69671,      26,    9260,  717285,    5378,\n",
       "              597,   27622, 1070413, 1040466,   38669,   27790,  175394,     711,\n",
       "               29,    1404,    1818])),\n",
       " (2,\n",
       "  tensor([    78,      9,    469,   8385,    206,     17,    996,     14,     32,\n",
       "              15,     32,     16,   3654,    600,    124,   3080, 140201,      3,\n",
       "             469,      9,      3,    996,     12,    369,     52,    328, 464765,\n",
       "              48,      3,    397,    172,    149,    113,    301,      3,  18223,\n",
       "               7, 285456,  52106,      2,   5606,  95553, 183737, 180431, 164136,\n",
       "          120431,  30446,     43,     44,    144,    145,  85276,  92253,  10654,\n",
       "          200704, 422054, 213900,   1877,   8549,    152,  12306,   6126,  55468,\n",
       "           63058,   4461, 358165, 464766,    204,   5641,   4893,  59857,   1515,\n",
       "           93728,    995,  83994,  79735, 411437, 460552, 110113])),\n",
       " (2,\n",
       "  tensor([     93,   16478,      78,    2680,      34,    1230,     717,    4562,\n",
       "               14,      32,      15,      32,      16,    1129,      49,    9392,\n",
       "               78,  766148,      34,       3,    1230,    4562,       8,     717,\n",
       "               93,  559272,     947,       6,    1239,    3934,     125, 1178056,\n",
       "                4,      35,      78,     396,      31,      11,     153,       2,\n",
       "           881320,  373252,   12125,   23059,  802619,  258328,  447615,  199864,\n",
       "               43,      44,     144,     145,   40078,   13205,  152671,  373240,\n",
       "          1002054,  766149,     171,    4412,  934044,   47573,    2586,   17490,\n",
       "           881151,  559273,   16579,   17268,   64855,  954751,  699138, 1178057,\n",
       "              733,   26650,   36317,    1670,     177,     435,     949])),\n",
       " (2,\n",
       "  tensor([     78,     124,    7860,       5,    6044,     198,       4,   17139,\n",
       "               27,   36218,       5,      45,     469,      14,     138,      15,\n",
       "              138,      16, 1197667,      61,      78,     124,       4,   31767,\n",
       "             2540,       9,   43425,   25199,       4,    6060,       6,      27,\n",
       "              489,   36218,    5810,     127,     403,     226,       3,      45,\n",
       "              532,     726,       2,     240,   60413,   65219,  104402,  107151,\n",
       "            10156,   48457, 1048623,  262618,  400118,    5800,    4956,   28355,\n",
       "              279,     280,     388,     386,  501493, 1197668,    7397,     240,\n",
       "             3497,   66207, 1235874,   50427,  317844, 1176539,  473765,  216053,\n",
       "            45249,     151,   64260,  745331,  949223,  325090,    4774,   26987,\n",
       "             1139,     243,    8436,    7063,    6238])),\n",
       " (2,\n",
       "  tensor([    206,     208,      53,       4,      55,     479,      94,    3464,\n",
       "               14,      32,      15,      32,      16,     206,    1060,    1902,\n",
       "              379,      11,  800978,    7992,     479,    3464,      12,       3,\n",
       "               94,      21,      78,     124,    3501,     476,     619,  528552,\n",
       "              965,       4,   39403,       6,    2350,     996,      34,     239,\n",
       "           935798,      85,       2,      14,    1141,       2,     495,      15,\n",
       "            11722,   15152,    3178,     118,  231225,  975243,  480985,  131178,\n",
       "               43,      44,     144,     145,    6425,   18789,   63098,   12156,\n",
       "             4606, 1004868,  800979, 1171655,   60005,  396069,      60,    1011,\n",
       "             6986,    2339,     240,   15238,   70524, 1028775,  482706,  528553,\n",
       "             6084,   51256,  101340,   15096,  269456,   27804,   73786,  688832,\n",
       "           935799,      89,     384,   57662,   24791,     932,   17728])),\n",
       " (2,\n",
       "  tensor([  1018,   1675,    582,      8,    415,    113,     14,     36,     15,\n",
       "              36,     16,   2612,      7,      3,    542,     17,     10,   1118,\n",
       "            1018,    172,   3234,   1675,    582,     28,    619,    142,      2,\n",
       "            1049,    189,      8,      3,    415,    113,      5,    619, 534560,\n",
       "               2,  21905,  14818,      4,      3,   1072,     72,   4880,     31,\n",
       "              81,      2, 100737,  98628,   9912,  18657,  15704,  19999,     62,\n",
       "              63,     71,     70,  85239,  16664,     29,    841,   2275,     23,\n",
       "           41434,  91602,  81876,  89964,   8199,  98628,  18613,  37184,   7399,\n",
       "             765,   5717,  77916,   1917,     26,    744,  15704,   4456,   9650,\n",
       "          482769, 534561,  77817, 535975, 288294,     42,  18426,  32381,  72988,\n",
       "           47073,   1132,    549])),\n",
       " (2,\n",
       "  tensor([   1900,    1714,     684,   44443,      48,    2013,      14,    3315,\n",
       "                2,     232,      15,    3315,       2,     232,      16,    1118,\n",
       "              169,   12060,     150,       6,    3972,       8,    1709,       4,\n",
       "                9,      27,     763,      12,    3794,    2462,     582,      92,\n",
       "              113,       4,       3,     133,      31,      81,       4,   15691,\n",
       "                3,     469,      24,    4021,      34,       6,  131682,    6254,\n",
       "                2,  360219,  957280, 1143380,  350974,  266796,  190595,    6596,\n",
       "             3316,     277,     928,    6636,    3316,     277,    1182,  141294,\n",
       "             3592,  438165,   22995,   17293,    5686,  146000,    3246,    7586,\n",
       "              108,    4936,   39342,   17008,   22055,   29322,  108286,   40674,\n",
       "              605,     976,      42,     742,    3807,    1132,     820,   31877,\n",
       "            99516,    1877,    9292,   68651,  380346,     613,  309262,  401039,\n",
       "            45492])),\n",
       " (2,\n",
       "  tensor([   1456,     602,      14,    4054,       2,     232,      15,    4054,\n",
       "                2,     232,      16,      40,   22024,       6,  420149,       2,\n",
       "             1224,       2,       8, 1155430,       4,    8786,  614302,   23842,\n",
       "             1797,       5,     503,      21,       3,     451,     701,      22,\n",
       "                6,    1832,     796,    5847,     436,      22,      35,    1308,\n",
       "             2127,    9247,       7,     619,    3643,       4,     219,       2,\n",
       "             1002,      40,       4,       6,     550,   69660,    4668,      28,\n",
       "               38,   18226,       5,     902,     147,  642157,      91,    1037,\n",
       "             2462,    1370,     341,      38,   22528,       2,      55,       4,\n",
       "               22,    4905,       4,    1879,    1037,      41,       3,  245967,\n",
       "             2827,      34,    1807,    4949,       4,     112,   23842,       2,\n",
       "            83196,  406394,   10956,    5498,     277,     928,   10957,    5498,\n",
       "              277,    1182,    5524,  173711,   87887,  544978,  420150,   11482,\n",
       "             3100,    1107,  864884, 1155431,   85118,  710639,  614303, 1100087,\n",
       "            20363,    2527,   28884,     194,   12465,    6090,   49951,     446,\n",
       "            21914,  684789,    6672,  356699,  127453,    3228,   13757,  583197,\n",
       "           176942,  102821,   20271,  120361,   13387,     223,   10964,   71661,\n",
       "            38040,  143313,      87,   17265,  127414, 1041355,  136947,    5022,\n",
       "           153035,  722611,    6727,  149512,  998456,  642158,  310432,  252583,\n",
       "           622234,   78794,   25784,  153040,  110408,     694,   20536,    1792,\n",
       "           322176,   94783,   85105,  649707,  874340,    1326,  284635,  367875,\n",
       "           285881,  111292,  132111,   59921,    1391, 1121846,  434103])),\n",
       " (2,\n",
       "  tensor([   572,    564,      2,   2326,  49106,    150,     88,      3,   1143,\n",
       "              27,     96,     14,     32,     15,     16, 443749,      4,    572,\n",
       "             499,     17,     10,  29160,   5488,      7, 468770,      4,     52,\n",
       "            7019,   1050,    442,      2,  14341,    673, 141447, 326092,  55044,\n",
       "            7887,    411,   9870, 628732,     97,    276,     43,     44,     46,\n",
       "          299709, 443750,  51274,    703,  14312,     23, 275050, 741752,  29990,\n",
       "          411508, 468771,   3779,  86384, 135944, 371666,   4052]))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7600"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_verif = []\n",
    "data_verif = test_dataset\n",
    "data_verif_len = len(data_verif)\n",
    "data_verif_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'showingTelling.xlsx', 'showingTelling_csv.csv']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('./data')\n",
    "os.listdir('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kimkwangil/Project/01EssayFitAI/showing_telling'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = len(train_dataset.get_labels())\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1308844"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUN_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 25 seconds\n",
      "\tLoss: 0.0260(train)\t|\tAcc: 84.8%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.5%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0118(train)\t|\tAcc: 93.7%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 89.3%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0068(train)\t|\tAcc: 96.4%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.6%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 25 seconds\n",
      "\tLoss: 0.0038(train)\t|\tAcc: 98.2%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 90.8%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0022(train)\t|\tAcc: 99.1%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 91.3%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "N_EPOCHS = 5\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "sub_train_, sub_valid_ = \\\n",
    "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0002(test)\t|\tAcc: 89.3%(test)\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(test_dataset)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "ag_news_label = {1 : \"World\",\n",
    "                 2 : \"Sports\",\n",
    "                 3 : \"Business\",\n",
    "                 4 : \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "vocab = train_dataset.get_vocab()\n",
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
