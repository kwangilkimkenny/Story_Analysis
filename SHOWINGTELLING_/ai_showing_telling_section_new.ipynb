{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "#cuda 메모리에 여유를 주기 위해서 잠시 딜레이를 시키자\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#입력된 전체 문장을 개별문장으로 분리하여 전처리 처리함\n",
    "def sentence_to_df(input_sentence):\n",
    "\n",
    "    input_text_df = nltk.tokenize.sent_tokenize(input_sentence)\n",
    "    test = []\n",
    "\n",
    "    for i in range(0,len(input_text_df)):\n",
    "        new_label = np.random.randint(0,2)  # 개별문장(input_text_df) 수만큼 0 또는 1 난수 생성\n",
    "        data = [new_label, input_text_df[i]]\n",
    "        test.append(data)\n",
    "\n",
    "    #print(test)\n",
    "    dataf = pd.DataFrame(test, columns=['label', 'text'])\n",
    "    #print(dataf)\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDataset(Dataset):\n",
    "    ''' Showing Telling Corpus Dataset '''\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx, 1]\n",
    "        label = int(self.df.iloc[idx, 0])\n",
    "        return text, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########입력받은 데이터 처리 실행하는 메소드 showtell_classfy() ###############\n",
    "#result_all.html에서 입력받을 text를 contents에 넣고 전처리 후 데이터프레임에 넣어줌\n",
    "def showtell_classfy(text):\n",
    "    contents = str(text)\n",
    "    preprossed_contents_df = sentence_to_df(contents)\n",
    "\n",
    "    preprossed_contents_df.dropna(inplace=True)\n",
    "    #전처리된 데이터를 확인(데이터프레임으로 가공됨)\n",
    "    preprossed_contents_df__ = preprossed_contents_df.sample(frac=1, random_state=999)\n",
    "    \n",
    "\n",
    "    #파이토치에 입력하기 위해서 로딩...\n",
    "    ST_test_dataset = STDataset(preprossed_contents_df__)\n",
    "    test_loader = DataLoader(ST_test_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "    #로딩되는지 확인\n",
    "    ST_test_dataset.__getitem__(1)\n",
    "\n",
    "\n",
    "    #check whether cuda is available\n",
    "    #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "    device = torch.device(\"cpu\")  \n",
    "    #device = torch.device(\"cuda\")\n",
    "    #tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    # for text, label in test_loader :\n",
    "    #     print(\"text:\",text)\n",
    "    #     print(\"label:\",label)\n",
    "\n",
    "\n",
    "    #저장된 모델을 불러온다.\n",
    "    #J:\\Django\\EssayFit_Django\\essayfitaiproject\\essayfitapp\\model.pt\n",
    "    time.sleep(1)\n",
    "    #model = torch.load(\"/Users/jongphilkim/Desktop/Django_WEB/essayfitaiproject/essayai/model.pt\", map_location=torch.device('cpu'))\n",
    "    model = torch.load(\"/Users/kimkwangil/Documents/001_ESSAYFITAI/essayfit_project/ai_server_backup/essayfit/essayai/data/model.pt\", map_location=torch.device('cpu'))\n",
    "    print(\"model loadling~\")\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pred_loader = test_loader\n",
    "\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_showing__ = 0\n",
    "    total_telling__ = 0\n",
    "\n",
    "    print(\"check!\")\n",
    "    for text, label in pred_loader:\n",
    "        print(\"text:\",text)\n",
    "        print(\"label:\",label)\n",
    "        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text] #text to tokenize\n",
    "        padded_list =  [e + [0] * (512-len(e)) for e in encoded_list] #padding\n",
    "        sample = torch.tensor(padded_list) #torch tensor로 변환\n",
    "        sample, label = sample.to(device), label.to(device) #tokenized text에 label을 넣어서 Device(gpu/cpu)에 넣기 위해 준비\n",
    "        labels = torch.tensor(label) #레이블을 텐서로 변환\n",
    "\n",
    "        outputs = model(sample,labels=labels) #모델을 통해서 샘플텍스트와 레이블 입력데이터를 출력 output에 넣음\n",
    "        #시간 딜레이를 주자\n",
    "\n",
    "        _, logits = outputs #outputs를 로짓에 넣음 이것을 softmax에 넣으면 0~1 사이로 결과가 출력됨\n",
    "        \n",
    "        pred = torch.argmax(F.softmax(logits), dim=1) #드디어 예측한다. argmax는 리스트(계산된 값)에서 가장 큰 값을 추출하여 pred에 넣는다. 0 ~1 사이의 값이 나올거임\n",
    "        # correct = pred.eq(labels) \n",
    "        showing__ = pred.eq(1) # 예측한 결과가 1과 같으면 showing이다. 그렇지 않으면 telling\n",
    "        telling__ = pred.eq(0)\n",
    "        total_showing__ += showing__.sum().item()\n",
    "        total_telling__ += telling__.sum().item()\n",
    "        # total_correct += correct.sum().item() #그 다음에는 계산을 하면 끝!\n",
    "        total_len += len(pred)\n",
    "        # total_len += len(labels)\n",
    "\n",
    "\n",
    "\n",
    "    #계산 결과를 웹페이지 result_all.html 페이지에 적용\n",
    "    result_showing = round(float(total_showing__/total_len),3)*100\n",
    "    result_telling = round(float(total_telling__/total_len),3)*100\n",
    "\n",
    "    result = [result_showing, result_telling]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lacking ideal overboard 계산\n",
    "\n",
    "def cal_laking_ideal_overboard(one_ps_char_desc, ideal_mean):\n",
    "    min_ = int(ideal_mean-ideal_mean*0.6)\n",
    "    print('min_', min_)\n",
    "    max_ = int(ideal_mean+ideal_mean*0.6)\n",
    "    print('max_: ', max_)\n",
    "    div_ = int(((ideal_mean+ideal_mean*0.6)-(ideal_mean-ideal_mean*0.6))/3)\n",
    "    print('div_:', div_)\n",
    "\n",
    "\n",
    "    cal_abs = abs(ideal_mean - one_ps_char_desc) # 개인 - 단체 값의 절대값계산\n",
    "\n",
    "    print('cal_abs 절대값 :', cal_abs)\n",
    "    compare = (one_ps_char_desc + ideal_mean)/7\n",
    "    print('compare :', compare)\n",
    "\n",
    "    if one_ps_char_desc > ideal_mean: # 개인점수가 평균보다 클 경우는 overboard\n",
    "        if cal_abs > compare: # 개인점수가 개인평균차의 절대값보다 클 경우, 즉 차이가 많이 날경우\n",
    "            print(\"Overboard\")\n",
    "            result = 2\n",
    "        else: #차이가 많이 안나면\n",
    "            print(\"Ideal\")\n",
    "            result = 1\n",
    "            \n",
    "        \n",
    "    elif one_ps_char_desc < ideal_mean: # 개인점수가 평균보다 작을 경우 lacking\n",
    "        if cal_abs > compare: #차이가 많이나면 # 개인점수가  평균보다 작을 경우 Lacking이고 \n",
    "            print(\"Lacking\")\n",
    "            result = 0\n",
    "        else: #차이가 많이 안나면\n",
    "            print (\"Ideal\")\n",
    "            result = 1\n",
    "            \n",
    "    else:\n",
    "        print(\"Ideal\")\n",
    "        result = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "########### 실행함수!!!!!!     이것을 실행하면 모든 것이 처리됨!!!   ####################################\n",
    "def ai_show_telling_analysis(input_text):\n",
    "\n",
    "    st_result = showtell_classfy(input_text)\n",
    "\n",
    "    out_show = st_result[0] # showing result\n",
    "    print(\"1명의 에세이 Showing:\", out_show)\n",
    "\n",
    "    # 1000명 통계 고정값(미리계산적용한 값)\n",
    "    show_ideal_mean = 50 # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 1000 ea dataset mean value\n",
    "\n",
    "    out_tell = st_result[1] # telling result\n",
    "    print(\"1명의 에세이 Telling :\", out_tell)\n",
    "\n",
    "    # 1000명 통계 고정값(미리계산적용한 값)\n",
    "    tell_idel_mean = 50 # # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 1000 ea dataset mean value\n",
    "\n",
    "    re_showing_ = cal_laking_ideal_overboard(out_show, show_ideal_mean) # 각각의 값 계산 (showing)\n",
    "    re_telling_ = cal_laking_ideal_overboard(out_tell, tell_idel_mean) # 각각의 값 계산 (telling)\n",
    "\n",
    " \n",
    "    \n",
    "    data = {\n",
    "        \"showing\":re_showing_,\n",
    "        \"telling\":re_telling_\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학생 #2 에세이\n",
    "# -거의 Telling <<<<<<<<<<<< 확인할 것 결과는 Telling : Overboard  2 로 나와야 함\n",
    "# -성적/활동 낮음\n",
    "# -캐릭더 수 없음 (나만, I)\n",
    "# -Setting 갯수 별로 없음 / Setting 설명 없음\n",
    "# -Conflict 거의 없음 (neutral)\n",
    "# -Emotion (거의 realization 쪽으로... intellectual/academic 하게)\n",
    "# -Essay Prompt #3 (Intellectual) - Theme 맞는지  (각 prompt 별로 emotion / keyword 조건 확인)\n",
    "\n",
    "input_text = \"\"\"John was sad to see his girlfriend leave. John wiped tears down his face as he watched his girlfriend board the plane. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "model loadling~\n",
      "check!\n",
      "text: ('John was sad to see his girlfriend leave.',)\n",
      "label: tensor([0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: ('John wiped tears down his face as he watched his girlfriend board the plane.',)\n",
      "label: tensor([0])\n",
      "1명의 에세이 Showing: 50.0\n",
      "1명의 에세이 Telling : 50.0\n",
      "min_ 20\n",
      "max_:  80\n",
      "div_: 20\n",
      "cal_abs 절대값 : 0.0\n",
      "compare : 14.285714285714286\n",
      "Ideal\n",
      "min_ 20\n",
      "max_:  80\n",
      "div_: 20\n",
      "cal_abs 절대값 : 0.0\n",
      "compare : 14.285714285714286\n",
      "Ideal\n",
      "SHOW TELL RESULT 0:telling, 1:showing ====  {'showing': 1, 'telling': 1}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"==================================================\")\n",
    "print (\"SHOW TELL RESULT 0:telling, 1:showing ==== \" , ai_show_telling_analysis(input_text))\n",
    "print(\"==================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
