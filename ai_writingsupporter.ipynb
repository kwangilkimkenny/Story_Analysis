{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cacki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\cacki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#conflict\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "from mpld3 import plugins, fig_to_html, save_html, fig_to_dict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "#character, setting\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import multiprocessing\n",
    "import os\n",
    "from pathlib import Path\n",
    "import io\n",
    "from gensim.models import Phrases\n",
    "from textblob import TextBlob\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = \"\"\"Share an essay on any topic of your choice. It can be one you've already written, one that responds to a different prompt \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aiwriter(input_text):\n",
    "    \n",
    "    #데이터 1000의 문장을. 로 구분하여 리스테 넣어놓기\n",
    "    #중간연사~~~ 삐리릭 삐삐~\n",
    "    #결과값을 토대로 문장을 가져와서 출력하기\n",
    "    \n",
    "    documents = [\"Some students have a background, identity, interest, or talent that is so meaningful they believe their application would be incomplete without it. If this sounds like you, then please share your story.\",\n",
    "                \"The lessons we take from obstacles we encounter can be fundamental to later success. Recount a time when you faced a challenge, setback, or failure. How did it affect you, and what did you learn from the experience?\",\n",
    "                \"Reflect on a time when you questioned or challenged a belief or idea. What prompted your thinking? What was the outcome?\",\n",
    "                \"Describe a problem you've solved or a problem you'd like to solve. It can be an intellectual challenge, a research query, an ethical dilemma - anything that is of personal importance, no matter the scale. Explain its significance to you and what steps you took or could be taken to identify a solution.\",\n",
    "                \"Discuss an accomplishment, event, or realization that sparked a period of personal growth and a new understanding of yourself or others.\",\n",
    "                \"Describe a topic, idea, or concept you find so engaging that it makes you lose all track of time. Why does it captivate you? What or who do you turn to when you want to learn more? \",\n",
    "                \"Share an essay on any topic of your choice. It can be one you've already written, one that responds to a different prompt, or one of your own design.\"]\n",
    "\n",
    "    df = pd.DataFrame(documents, columns =['sentences'])\n",
    "    \n",
    "    # remove common words and tokenize them... 100ea. upto 1000ea\n",
    "    stoplist = set('''for a of the and to in a about all also and as at be because but by\n",
    "                    can come could day do even find first for from get give go have he her here him his how I if in into it its\n",
    "                    just know like look make man many me more my new no not now of on one only or other our out people say see she\n",
    "                    so some take tell than that the their them then there these they thing think this those time to two up use very\n",
    "                    want way we well what when which who will with would year you your'''.split())\n",
    "\n",
    "    texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\n",
    "\n",
    "    # remove words those appear only once\n",
    "    all_tokens = sum(texts, [])\n",
    "\n",
    "    tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) ==1)\n",
    "    texts = [[word for word in text if word not in tokens_once]\n",
    "            for text in texts]\n",
    "    dictionary = corpora.Dictionary(texts)#make dictionary  문장을 수치화 하여 사전으로 만듬\n",
    "\n",
    "    dictionary.save('similerlist.dict')  # save as binary file at the dictionary at local directory 일단 사전을 만든다. \n",
    "    dictionary.save_as_text('similerlist_text.dict')  # save as text file at the local directory\n",
    "\n",
    "\n",
    "\n",
    "    #input answer\n",
    "    text_input = input_text #문장입력....\n",
    " \n",
    "    new_vec = dictionary.doc2bow(text_input.lower().split()) # return \"word-ID : Frequency of appearance\"\" 소문자, 빈칸구분, 빈도수 수치화 하여 \n",
    "    corpus = [dictionary.doc2bow(text) for text in texts] #문장을 수치화하여 코퍼스 리스트로 만듬\n",
    "    corpora.MmCorpus.serialize('similerlist.mm', corpus) # save corpus at local directory Matrix Market format으로 저장\n",
    "    corpus = corpora.MmCorpus('similerlist.mm') # try to load the saved corpus from local 다시 로딩\n",
    "    dictionary = corpora.Dictionary.load('similerlist.dict') # try to load saved dic.from local\n",
    "    tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model 초기화\n",
    "    corpus_tfidf = tfidf[corpus]  # 코퍼스 모델을 tfidf공간에 맵핑\n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) # LSI 초기화\n",
    "    corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus\n",
    "    topic = lsi.print_topics(2)\n",
    "    lsi.save('model.lsi')  # save output model at local directory\n",
    "    lsi = models.LsiModel.load('model.lsi') # try to load above saved model\n",
    "\n",
    "    doc = text_input\n",
    "\n",
    "    vec_bow = dictionary.doc2bow(doc.lower().split())  # put newly obtained document to existing dictionary object\n",
    "    vec_lsi = lsi[vec_bow] # convert new document (henceforth, call it \"query\") to LSI space\n",
    "    index = similarities.MatrixSimilarity(lsi[corpus]) # transform corpus to LSI space and indexize it\n",
    "    index.save('similerlist.index') # save index object at local directory\n",
    "    index = similarities.MatrixSimilarity.load('similerlist.index')\n",
    "    sims = index[vec_lsi] # calculate degree of similarity of the query to existing corpus\n",
    "\n",
    "    #print(list(enumerate(sims))) # output (document_number , document similarity)\n",
    "\n",
    "    #sims = sorted(enumerate(sims), key=lambda item: -item[1])  # sort output object as per similarity ( largest similarity document comes first )\n",
    "    #print(sims) # 가장 질문에 대한 답변이 적합한 순서대로 출력\n",
    "    \n",
    "    #df2 = pd.DataFrame(sims)\n",
    "    #return df, df2\n",
    "\n",
    "    df2 = pd.DataFrame(sims ,columns =['similarity'])\n",
    "    df3 = pd.concat([df,df2], axis=1) #문장과 유사도 계산 결과를 데이터프레임으로 합치고\n",
    "    df4 =  df3.sort_values(by=['similarity'], axis=0, ascending=False)\n",
    "    return df4[:3] #가장 비슷한 문장 3개를 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Share an essay on any topic of your choice. It...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Describe a problem you've solved or a problem ...</td>\n",
       "      <td>0.982750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Discuss an accomplishment, event, or realizati...</td>\n",
       "      <td>0.974974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences  similarity\n",
       "6  Share an essay on any topic of your choice. It...    1.000000\n",
       "3  Describe a problem you've solved or a problem ...    0.982750\n",
       "4  Discuss an accomplishment, event, or realizati...    0.974974"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiwriter(text_input) # 결과에서 6,3,4를 유사한 문장으로 추천해 줘야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
