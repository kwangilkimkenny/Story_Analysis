{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "owned-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data  \n",
    "\n",
    "# Yale 교수 전공 및 general 모든 정보 link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "romantic-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "\n",
    "pd.options.display.max_rows = 999 # 데이터 프레임 표시 최대 열수를 999로 지정\n",
    "pd.set_option('display.max_columns',999) # 데이터 프레임 표시 최대 행수를 999로 지정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "going-arthritis",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>major_title</th>\n",
       "      <th>info_type</th>\n",
       "      <th>Link</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>African and African-American Studies</td>\n",
       "      <td>general_keywords</td>\n",
       "      <td>https://afas.wustl.edu/undergraduate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>African and African-American Studies</td>\n",
       "      <td>general_keywords</td>\n",
       "      <td>https://afas.wustl.edu/undergraduate#primary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>African and African-American Studies</td>\n",
       "      <td>general_keywords</td>\n",
       "      <td>https://bulletin.wustl.edu/undergrad/artsci/af...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>African and African-American Studies</td>\n",
       "      <td>courses_concentrations</td>\n",
       "      <td>https://bulletin.wustl.edu/undergrad/artsci/af...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>African and African-American Studies</td>\n",
       "      <td>courses_concentrations</td>\n",
       "      <td>https://afas.wustl.edu/sites/courses?level=all</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1015 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               major_title               info_type  \\\n",
       "0     African and African-American Studies        general_keywords   \n",
       "1     African and African-American Studies        general_keywords   \n",
       "2     African and African-American Studies        general_keywords   \n",
       "3     African and African-American Studies  courses_concentrations   \n",
       "4     African and African-American Studies  courses_concentrations   \n",
       "...                                    ...                     ...   \n",
       "1010                                   NaN                     NaN   \n",
       "1011                                   NaN                     NaN   \n",
       "1012                                   NaN                     NaN   \n",
       "1013                                   NaN                     NaN   \n",
       "1014                                   NaN                     NaN   \n",
       "\n",
       "                                                   Link Unnamed: 3  Unnamed: 4  \n",
       "0                  https://afas.wustl.edu/undergraduate        NaN         NaN  \n",
       "1          https://afas.wustl.edu/undergraduate#primary        NaN         NaN  \n",
       "2     https://bulletin.wustl.edu/undergrad/artsci/af...        NaN         NaN  \n",
       "3     https://bulletin.wustl.edu/undergrad/artsci/af...        NaN         NaN  \n",
       "4        https://afas.wustl.edu/sites/courses?level=all        NaN         NaN  \n",
       "...                                                 ...        ...         ...  \n",
       "1010                                                NaN        NaN         NaN  \n",
       "1011                                                NaN        NaN         NaN  \n",
       "1012                                                NaN        NaN         NaN  \n",
       "1013                                                NaN        NaN         NaN  \n",
       "1014                                                NaN        NaN         NaN  \n",
       "\n",
       "[1015 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get excel data to dataframe\n",
    "file_name = './college_datasets/WashU_St_Louis/WashU-St_Louis_major_detail_inof.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "christian-evanescence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1015"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "electronic-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# major_title + info_type 으로 파일명을 만들고 링크타고 데이터 추출후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conservative-robertson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'African and African-American Studies'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mjr_title = df.iloc[0]['major_title']\n",
    "mjr_title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "prescription-turkish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'general_keywords'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_type = df.iloc[0]['info_type']\n",
    "info_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alike-awareness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://afas.wustl.edu/undergraduate'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = df.iloc[0]['Link']\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pleased-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_file_full_name = \"WashU_St_Louis\" + \"_\" +  mjr_title + \"_\" + info_type + \"_info\" + \".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "engaged-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe에서 데이터 추출하고 파일명으로 저장하기\n",
    "md_file = open(mk_file_full_name, 'w')\n",
    "# Yale_African American Studies_general_keywords_info.txt\n",
    "md_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "earlier-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 링크타고 들어가서 키워드 크롤링\n",
    "# 검색어로 크롬 실행, 실행결과의 링크 추출, 링크를 타고 각 페이지 접속, 페이지 내용 전체 크롤링, text 추출하고, 단어 리스트로 만드는 코드\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from urllib.parse import ParseResultBytes, quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('window-size=1920x1080')\n",
    "options.add_argument(\"disable-gpu\")\n",
    "# 혹은 options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# UserAgent값을 바꿔줍시다! 서버가 인식하지 못하도록 가상으로 headless 값 추가함ㅠ\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "presidential-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linked_search_result(linked_page):\n",
    "#     baseUrl = 'https://www.google.com/search?q='\n",
    "\n",
    "#     #plusUrl = input('무엇을 검색할까요? :')\n",
    "#     plusUrl = input_word\n",
    "\n",
    "#     # url = baseUrl + quote_plus(plusUrl)\n",
    "#     url = baseUrl + plusUrl\n",
    "#     # 한글을 사용할 경우 :  quote_plus 적용 - URL에 막 %CE%GD%EC 이런 거 생성해줌\n",
    "\n",
    "    driver = webdriver.Chrome(executable_path= r'./data/chromedriver_mac_ver_90', chrome_options=options)\n",
    "    driver.get(linked_page)\n",
    "\n",
    "#     html = driver.page_source\n",
    "#     soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "#     v = soup.select('.yuRUbf')\n",
    "\n",
    "#     search_title_result = []\n",
    "#     search_linked_contents_result = []\n",
    "#     for i in v:\n",
    "#         #print(i.select_one('.LC20lb.DKV0Md').text)\n",
    "#         search_title_result.append(i.select_one('.LC20lb.DKV0Md').text)\n",
    "#         #print(i.a.attrs['href'])\n",
    "#         search_linked_contents_result.append(i.a.attrs['href'])\n",
    "#         #print()\n",
    "\n",
    "\n",
    "    # search_linked_contents_result 의 각 링크로 접속하여 해당 내용을 모두 text로 크롤링한 후, 단어만 추리고, 다시 주요키워드를 추출한다.\n",
    "    \n",
    "    get_all_linked_web_data = []\n",
    "    \n",
    "    driver.get(linked_page)\n",
    "    html = driver.page_source\n",
    "    get_all_data = BeautifulSoup(html, features=\"html.parser\")\n",
    "    get_all_linked_web_data.append(get_all_data)\n",
    "        \n",
    "    body = re.search('<body.*/body>', html, re.I|re.S)\n",
    "    if (body is None):\n",
    "        print (\"No <body> in html\")\n",
    "        exit()\n",
    "            \n",
    "    body = body.group()\n",
    "    #print(body)\n",
    "    \n",
    "    # 추출된 정보 클린징\n",
    "    korean = re.compile('[\\u3131-\\u3163\\uac00-\\ud7a3]+')#한글제거\n",
    "\n",
    "    item_extract = str(body).replace('\\n', ' ')\n",
    "    item_extract = re.sub('<span.*?>.*?</span>', ' ', item_extract)\n",
    "    item_extract = re.sub('<b>.*?</b>', ' ', item_extract)    \n",
    "    item_extract = re.sub('<.*?>', ' ', item_extract)        \n",
    "    item_extract = item_extract.replace('\\t', ' ')\n",
    "    item_extract = re.sub(korean, '', item_extract)\n",
    "    item_extract = re.sub('[-=.#/?:$}]', ' ', item_extract)\n",
    "    item_extract = re.sub(\"[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』;{}()'\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]\", ' ', item_extract)\n",
    "    #print (item_extract)\n",
    "    \n",
    "    driver.close()\n",
    "\n",
    "    return item_extract\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "appreciated-regard",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: use options instead of chrome_options\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "get_result = linked_search_result('https://afamstudies.yale.edu/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "robust-offense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    omega media query dummy   position  relative  z index   1       omega media query dummy   z index  2       omega media query dummy   z index  0      omega media query dummy   z index  1      omega media query dummy   z index  2               Skip to main content                                                                                                                                                                                                                                                                                                                                                                                                                 Department of African American Studies nbsp                                                                                                                                                                         Graduate Program     Undergraduate Major     Research  amp  Collections     Media Gallery     People     Contact Us                                                                                                                        Welcome                                                                   Welcome to the Department of African American Studies at Yale  nbsp The African American Studies Department examines  from numerous disciplinary perspectives  the experiences of people of African descent in Black Atlantic societies  including the United States  the Caribbean  and Latin America     Learn More                                       From the nbsp Collection                                                                                                                                   The Rev  Dr  Martin Luther King  Jr   shown here with former Yale trustee the Rev  Gardiner M  Day  received an honorary LL D degree at Yale’s 263rd commencement in 1964                                                                        Ella Fitzgerald in 1940  photographed by Carl Van Vechten  who documented much of the Harlem Renaissance Credit   Carl Van Vechten © Van Vechten Trust                                                                       Walter Evans Collection of Frederick Douglass and Douglass Family Papers                                                                       Half length group portrait  family group of six  father  mother  four children                                      1    2    3    4      Previous    Next                                                                          News                                                                                                    Elizabeth Hinton’s new book recounts the history of police oppression and how it fuels protests by Black citizens                                                                                                               Hazel Carby   DeVane Medals recognize stellar teaching and scholarship                                                                                                               Guggenheim’s first full time Black curator earned her doctorate at Yale — and curated a Yale Art Gallery exhibit while in graduate school                                                                                                 More news                                                                                       African American Studies  amp  the Crises of Our Time    African American Studies  Support for the Scholars  Strike             Click here for the latest information on Yale s COVID 19 policies and procedures     African American Studies Response to COVID 19     nbsp  nbsp                                                    Join in Our Academic Work    The Combined Ph D  in African American Studies  nbsp  Important Update     Choose Our Undergraduate Program     Questions  nbsp    Contact the Registrar nbsp         nbsp                                    Events                                                                  More events                              Follow us for more news  amp  events         nbsp  nbsp                                                     New from African American Studies                                                                                                                      Elizabeth Hinton                                                                       Daphne A  Brooks                                                                       Hazel Carby                                                                       Tavia Nyong o                                                                       Edward Rugemer                                                                       David Blight                                                                      More publications                                                                                              Yale University Art Gallery Copyright Whitfield Lovell DC Moore Gallery                                                     Engage with Our Ideas                        “Style With Soul  How The World’s Most Iconic Black Women Singers Expressed Themselves Through Fashion ” Tavia Nyong’o  Vogue      “White Supermacy as Daily Pratice ” Micah Jones   Historians’ Watch      “José Muñoz  Then and There  nbsp On the afterlives of the pioneering queer theorist ” nbsp Tavia Nyong’o   thebaffler com                                                                    Kindren  quilt  Bisa Butler                                                                                                Accessibility at Yale  ·        Privacy policy          Copyright © 2021 Yale University · All rights reserved                                            About Us     Contact Us                                                                                                                                                                                            '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-burst",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-pattern",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-firewall",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "latter-mailing",
   "metadata": {},
   "source": [
    "## test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "entitled-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "\n",
    "pd.options.display.max_rows = 999 # 데이터 프레임 표시 최대 열수를 999로 지정\n",
    "pd.set_option('display.max_columns',999) # 데이터 프레임 표시 최대 행수를 999로 지정\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import openpyxl\n",
    "from urllib.parse import ParseResultBytes, quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('window-size=1920x1080')\n",
    "options.add_argument(\"disable-gpu\")\n",
    "# 혹은 options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# UserAgent값을 바꿔줍시다! 서버가 인식하지 못하도록 가상으로 headless 값 추가함ㅠ\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "blessed-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(input_data):\n",
    "    remove = input_data.replace(\"  \",\" \") # 변환\n",
    "    remove_ = re.sub(r\"\\t\", \" \", remove) # 제거\n",
    "    remove__ = re.sub(r\"\\n\", \" \", remove_) # 제거\n",
    "    remove__ = remove__.replace(\"   \", \" \")\n",
    "    remove__ = remove__.replace(\"  \", \" \")\n",
    "    remove__ = remove__.replace(\" \", \",\")\n",
    "    remove__ = remove__.replace(\"…/\", \" \")\n",
    "    remove__ = remove__.replace(\"…\", \" \")\n",
    "    remove__ = remove__.replace(\"/\", \" \")\n",
    "    remove__ = remove__.replace(\" \", \",\")\n",
    "    remove__ = remove__.replace(\")\", \",\")\n",
    "    remove__ = remove__.replace(\"(\", \",\")\n",
    "    preprossed = remove__.split(\",\") # 단어를 리스트로 변환\n",
    "    #print(preprossed)\n",
    "    \n",
    "    # 표제어 추출, 동사는 현재형으로 변환\n",
    "    lemma_list =[]\n",
    "    for i in preprossed:\n",
    "        lema_re = lemmatizer.lemmatize(i, pos='v') #표제어 추출, 동사는 현재형으로 변환\n",
    "        lemma_list.append(lema_re)\n",
    "    \n",
    "    # 표제어 추출\n",
    "    ext_lema = [lemmatizer.lemmatize(w) for w in preprossed]\n",
    "    # 중복값을 제거하고\n",
    "    rm_dupli = set(ext_lema)\n",
    "    # 다시 리스트로 만들고\n",
    "    re_li = list(rm_dupli)\n",
    "    # 빈 값은 제거하고\n",
    "    get_wd =list(filter(None, re_li))\n",
    "    # 소문자로 모두 변환\n",
    "    lower_wd = [i.lower() for i in get_wd]\n",
    "    \n",
    "    result = []\n",
    "    for w in lower_wd: \n",
    "        if w not in stop_words: \n",
    "            result.append(w)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "complex-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linked_search_result(linked_page):\n",
    "#     baseUrl = 'https://www.google.com/search?q='\n",
    "\n",
    "#     #plusUrl = input('무엇을 검색할까요? :')\n",
    "#     plusUrl = input_word\n",
    "\n",
    "#     # url = baseUrl + quote_plus(plusUrl)\n",
    "#     url = baseUrl + plusUrl\n",
    "#     # 한글을 사용할 경우 :  quote_plus 적용 - URL에 막 %CE%GD%EC 이런 거 생성해줌\n",
    "\n",
    "    driver = webdriver.Chrome(executable_path= r'./data/chromedriver_mac_ver_90', chrome_options=options)\n",
    "    \n",
    "    try:\n",
    "        driver.get(linked_page)\n",
    "        \n",
    "    #     html = driver.page_source\n",
    "    #     soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "    #     v = soup.select('.yuRUbf')\n",
    "\n",
    "    #     search_title_result = []\n",
    "    #     search_linked_contents_result = []\n",
    "    #     for i in v:\n",
    "    #         #print(i.select_one('.LC20lb.DKV0Md').text)\n",
    "    #         search_title_result.append(i.select_one('.LC20lb.DKV0Md').text)\n",
    "    #         #print(i.a.attrs['href'])\n",
    "    #         search_linked_contents_result.append(i.a.attrs['href'])\n",
    "    #         #print()\n",
    "\n",
    "\n",
    "        # search_linked_contents_result 의 각 링크로 접속하여 해당 내용을 모두 text로 크롤링한 후, 단어만 추리고, 다시 주요키워드를 추출한다.\n",
    "\n",
    "        get_all_linked_web_data = []\n",
    "\n",
    "        driver.get(linked_page)\n",
    "        html = driver.page_source\n",
    "        get_all_data = BeautifulSoup(html, features=\"html.parser\")\n",
    "        get_all_linked_web_data.append(get_all_data)\n",
    "\n",
    "        body = re.search('<body.*/body>', html, re.I|re.S)\n",
    "        if (body is None):\n",
    "            print (\"No <body> in html\")\n",
    "            exit()\n",
    "\n",
    "        body = body.group()\n",
    "        #print(body)\n",
    "\n",
    "        # 추출된 정보 클린징\n",
    "        korean = re.compile('[\\u3131-\\u3163\\uac00-\\ud7a3]+')#한글제거\n",
    "\n",
    "        item_extract = str(body).replace('\\n', ' ')\n",
    "        item_extract = re.sub('<span.*?>.*?</span>', ' ', item_extract)\n",
    "        item_extract = re.sub('<b>.*?</b>', ' ', item_extract)    \n",
    "        item_extract = re.sub('<.*?>', ' ', item_extract)        \n",
    "        item_extract = item_extract.replace('\\t', ' ')\n",
    "        item_extract = re.sub(korean, '', item_extract)\n",
    "        item_extract = re.sub('[-=.#/?:$}]', ' ', item_extract)\n",
    "        item_extract = re.sub(\"[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』;{}()'\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]\", ' ', item_extract)\n",
    "        #print (item_extract)\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "    except:\n",
    "        item_extract = [\"not enough infomation\"]\n",
    "\n",
    "    return item_extract\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "tropical-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동으로 데이터 추출코드, 1)파일명 생성 2)링크접속 3)데이터크롤링 4)키워드로 저장\n",
    "# file_name : 추출해야할 csv 파일 경로 및 파일명 지정\n",
    "#     file_name = './college_datasets/Yale/Yale 교수 전공 및 general 모든 정보 link.csv'\n",
    "\n",
    "# college_name : 'Yale'\n",
    "# file_name =: './college_datasets/Yale/Yale 교수 전공 및 general 모든 정보 link.csv'\n",
    "def input_name_of_college(college_name, file_name):\n",
    "    # import file to dataframe\n",
    "    df = pd.read_csv(file_name)\n",
    "    \n",
    "    # get major title data\n",
    "    mjr_title = df.iloc[0]['major_title']\n",
    "    \n",
    "    # get info_type data\n",
    "    info_type = df.iloc[0]['info_type']\n",
    "    \n",
    "    #get link data\n",
    "    linked_page = df.iloc[0]['Link']\n",
    "    \n",
    "    # make file with full name\n",
    "    mk_file_full_name = college_name + \"_\" +  mjr_title + \"_\" + info_type + \"_info\" + \".txt\"\n",
    "    \n",
    "    # dataframe에서 데이터 추출하고 파일명으로 저장하기\n",
    "    md_file = open(mk_file_full_name, 'w')\n",
    "    # Yale_African American Studies_general_keywords_info.txt\n",
    "    \n",
    "    # 링크데이터로 크롤링하기\n",
    "    get_linked_data = linked_search_result(linked_page)\n",
    "    \n",
    "    # 데이터 클린징 후 리스트로 만들기\n",
    "    cleaned_data = cleaning_data(str(get_linked_data))\n",
    "    \n",
    "    # 리스트를 문자열로 변환\n",
    "    str_data = \" \".join(cleaned_data)\n",
    "    \n",
    "    # 생성된 파일에 결과데이터 저장하기\n",
    "    md_file.write(str_data)\n",
    "    \n",
    "    # 파일 닫기\n",
    "    md_file.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "behind-thesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: use options instead of chrome_options\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "college_name = 'WashU_St_Louis'\n",
    "file_name = './college_datasets/WashU_St_Louis/WashU-St_Louis_major_detail_inof.csv'\n",
    "result = input_name_of_college(college_name, file_name)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-australian",
   "metadata": {},
   "source": [
    "## auto-run code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "developing-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동으로 데이터 추출코드, 1)파일명 생성 2)링크접속 3)데이터크롤링 4)키워드로 저장\n",
    "# file_name : 추출해야할 csv 파일 경로 및 파일명 지정\n",
    "#     file_name = './college_datasets/Yale/Yale 교수 전공 및 general 모든 정보 link.csv'\n",
    "\n",
    "# college_name : 'Yale'\n",
    "# file_name =: './college_datasets/Yale/Yale 교수 전공 및 general 모든 정보 link.csv'\n",
    "def input_name_of_college(college_name, file_name):\n",
    "    # import file to dataframe\n",
    "    df = pd.read_csv(file_name)\n",
    "    \n",
    "    i = 0\n",
    "    for i in range(len(df.index)):\n",
    "        # get major title data\n",
    "        mjr_title = df.iloc[i]['major_title']\n",
    "\n",
    "        # get info_type data\n",
    "        info_type = df.iloc[i]['info_type']\n",
    "\n",
    "        #get link data\n",
    "        linked_page = df.iloc[i]['Link']\n",
    "\n",
    "        # make file with full name\n",
    "        mk_file_full_name = college_name + \"_\" +  mjr_title + \"_\" + info_type + \"_info\" + \".txt\"\n",
    "\n",
    "        # dataframe에서 데이터 추출하고 파일명으로 저장하기\n",
    "        md_file = open(mk_file_full_name, 'w')\n",
    "        # Yale_African American Studies_general_keywords_info.txt\n",
    "\n",
    "        # 링크데이터로 크롤링하기\n",
    "        try:\n",
    "            get_linked_data = linked_search_result(linked_page)\n",
    "            \n",
    "            # 데이터 클린징 후 리스트로 만들기\n",
    "            cleaned_data = cleaning_data(str(get_linked_data))\n",
    "\n",
    "            # 리스트를 문자열로 변환\n",
    "            str_data = \" \".join(cleaned_data)\n",
    "\n",
    "            # 생성된 파일에 결과데이터 저장하기\n",
    "            md_file.write(str_data)\n",
    "\n",
    "            # 파일 닫기\n",
    "            md_file.close()\n",
    "\n",
    "            # add numb\n",
    "            i += 1\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            md_file.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "closed-combine",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkwangil/opt/anaconda3/envs/py37pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: use options instead of chrome_options\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'WashU_St_Louis_Development/Global Studies_general_keywords_info.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2417fd0b1a43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcollege_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'WashU_St_Louis'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./college_datasets/WashU_St_Louis/WashU-St_Louis_major_detail_inof.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name_of_college\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollege_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-7ba7a76daf2c>\u001b[0m in \u001b[0;36minput_name_of_college\u001b[0;34m(college_name, file_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# dataframe에서 데이터 추출하고 파일명으로 저장하기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mmd_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmk_file_full_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Yale_African American Studies_general_keywords_info.txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'WashU_St_Louis_Development/Global Studies_general_keywords_info.txt'"
     ]
    }
   ],
   "source": [
    "college_name = 'WashU_St_Louis'\n",
    "file_name = './college_datasets/WashU_St_Louis/WashU-St_Louis_major_detail_inof.csv'\n",
    "result = input_name_of_college(college_name, file_name)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-lodge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
